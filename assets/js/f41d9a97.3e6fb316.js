"use strict";(self.webpackChunkml_notebook=self.webpackChunkml_notebook||[]).push([[7872],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return m}});var o=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=o.createContext({}),s=function(e){var t=o.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=s(e.components);return o.createElement(c.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},f=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),f=s(n),m=r,b=f["".concat(c,".").concat(m)]||f[m]||u[m]||i;return n?o.createElement(b,a(a({ref:t},p),{},{components:n})):o.createElement(b,a({ref:t},p))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,a=new Array(i);a[0]=f;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:r,a[1]=l;for(var s=2;s<i;s++)a[s]=n[s];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}f.displayName="MDXCreateElement"},51172:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return f},contentTitle:function(){return m},metadata:function(){return b},assets:function(){return d},toc:function(){return g},default:function(){return y}});var o=n(3905),r=Object.defineProperty,i=Object.defineProperties,a=Object.getOwnPropertyDescriptors,l=Object.getOwnPropertySymbols,c=Object.prototype.hasOwnProperty,s=Object.prototype.propertyIsEnumerable,p=(e,t,n)=>t in e?r(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,u=(e,t)=>{for(var n in t||(t={}))c.call(t,n)&&p(e,n,t[n]);if(l)for(var n of l(t))s.call(t,n)&&p(e,n,t[n]);return e};const f={title:"MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications",authors:["pommespeter"],tags:["detection","backbone","light-weight"]},m=void 0,b={permalink:"/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications",editUrl:"https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.md",source:"@site/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.md",title:"MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications",description:"\u8fd9\u662f\u4e00\u7bc7\u8bb2\u89e3\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e3b\u5e72\u7f51\u7edc\u7684\u8bba\u6587\u3002\u539f\u8bba\u6587\uff08MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\uff09\u3002",date:"2022-11-05T07:47:19.660Z",formattedDate:"2022\u5e7411\u67085\u65e5",tags:[{label:"detection",permalink:"/blog/tags/detection"},{label:"backbone",permalink:"/blog/tags/backbone"},{label:"light-weight",permalink:"/blog/tags/light-weight"}],readingTime:8.6,truncated:!0,authors:[{name:"PommesPeter",title:"I want to be strong. But it seems so hard.",url:"https://blog.pommespeter.com/",email:"me@pommespeter.com",imageURL:"https://github.com/PommesPeter.png",key:"pommespeter"}],prevItem:{title:"Fast-SCNN - Fast Semantic Segmentation Network",permalink:"/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network"},nextItem:{title:"Gated Channel Transformation for Visual Recognition",permalink:"/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition"}},d={authorsImageUrls:[void 0]},g=[],h={toc:g};function y(e){var t,n=e,{components:r}=n,p=((e,t)=>{var n={};for(var o in e)c.call(e,o)&&t.indexOf(o)<0&&(n[o]=e[o]);if(null!=e&&l)for(var o of l(e))t.indexOf(o)<0&&s.call(e,o)&&(n[o]=e[o]);return n})(n,["components"]);return(0,o.kt)("wrapper",(t=u(u({},h),p),i(t,a({components:r,mdxType:"MDXLayout"}))),(0,o.kt)("p",null,"\u8fd9\u662f\u4e00\u7bc7\u8bb2\u89e3\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e3b\u5e72\u7f51\u7edc\u7684\u8bba\u6587\u3002",(0,o.kt)("a",u({parentName:"p"},{href:"https://arxiv.org/abs/1704.04861"}),"\u539f\u8bba\u6587\uff08MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\uff09"),"\u3002"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e94\u7528\u4e8e\u79fb\u52a8\u6216\u8005\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc"),(0,o.kt)("li",{parentName:"ul"},"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u64cd\u4f5c\u6570\u8f83\u5c0f\u7684\u5377\u79ef\u6a21\u5757\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef(Depthwise Separable Convolution\uff0c\u4ee5\u4e0b\u79f0DSC)")),(0,o.kt)("p",null,"\u6458\u8981:"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.")))}y.isMDXComponent=!0}}]);