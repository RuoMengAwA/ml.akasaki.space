"use strict";(self.webpackChunkml_notebook=self.webpackChunkml_notebook||[]).push([[9409],{3905:function(e,t,n){n.d(t,{Zo:function(){return l},kt:function(){return m}});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),u=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},l=function(e){var t=u(e.components);return r.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),h=u(n),m=a,f=h["".concat(s,".").concat(m)]||h[m]||p[m]||o;return n?r.createElement(f,i(i({ref:t},l),{},{components:n})):r.createElement(f,i({ref:t},l))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=h;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:a,i[1]=c;for(var u=2;u<o;u++)i[u]=n[u];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},78965:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return h},contentTitle:function(){return m},metadata:function(){return f},assets:function(){return d},toc:function(){return y},default:function(){return v}});var r=n(3905),a=Object.defineProperty,o=Object.defineProperties,i=Object.getOwnPropertyDescriptors,c=Object.getOwnPropertySymbols,s=Object.prototype.hasOwnProperty,u=Object.prototype.propertyIsEnumerable,l=(e,t,n)=>t in e?a(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,p=(e,t)=>{for(var n in t||(t={}))s.call(t,n)&&l(e,n,t[n]);if(c)for(var n of c(t))u.call(t,n)&&l(e,n,t[n]);return e};const h={title:"Learning in the Frequency Domain",authors:["visualdust"],tags:["frequency-domain","attention-mechanism","non-convolution"]},m=void 0,f={permalink:"/blog/[40]Learning-in-the-Frequency-Domain",editUrl:"https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[40]Learning-in-the-Frequency-Domain.md",source:"@site/blog/[40]Learning-in-the-Frequency-Domain.md",title:"Learning in the Frequency Domain",description:"Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, Fengbo Ren",date:"2022-11-05T07:47:19.660Z",formattedDate:"2022\u5e7411\u67085\u65e5",tags:[{label:"frequency-domain",permalink:"/blog/tags/frequency-domain"},{label:"attention-mechanism",permalink:"/blog/tags/attention-mechanism"},{label:"non-convolution",permalink:"/blog/tags/non-convolution"}],readingTime:20.965,truncated:!0,authors:[{name:"Gavin Gong",title:"Rubbish CVer | Poor LaTex speaker | Half stack developer | \u952e\u5708\u8eba\u5c38\u7816\u5bb6",url:"https://gong.host",email:"gavin@gong.host",imageURL:"https://github.yuuza.net/visualDust.png",key:"visualdust"}],prevItem:{title:"Instance-sensitive Fully Convolutional Networks",permalink:"/blog/[39]Instance-sensitive-Fully-Convolutional-Networks"}},d={authorsImageUrls:[void 0]},y=[],g={toc:y};function v(e){var t,n=e,{components:a}=n,l=((e,t)=>{var n={};for(var r in e)s.call(e,r)&&t.indexOf(r)<0&&(n[r]=e[r]);if(null!=e&&c)for(var r of c(e))t.indexOf(r)<0&&u.call(e,r)&&(n[r]=e[r]);return n})(n,["components"]);return(0,r.kt)("wrapper",(t=p(p({},g),l),o(t,i({components:a,mdxType:"MDXLayout"}))),(0,r.kt)("p",null,(0,r.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K"}),"Kai Xu"),", ",(0,r.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+M"}),"Minghai Qin"),", ",(0,r.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F"}),"Fei Sun"),", ",(0,r.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y"}),"Yuhao Wang"),", ",(0,r.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y"}),"Yen-Kuang Chen"),", ",(0,r.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+F"}),"Fengbo Ren")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"  Deep neural networks have achieved remarkable success in computer vision tasks. Existing neural networks mainly operate in the spatial domain with fixed input sizes. For practical applications, images are usually large and have to be downsampled to the predetermined input size of neural networks. Even though the downsampling operations reduce computation and the required communication bandwidth, it removes both redundant and salient information obliviously, which results in accuracy degradation. Inspired by digital signal processing theories, we analyze the spectral bias from the frequency perspective and propose a learning-based frequency selection method to identify the trivial frequency components which can be removed without accuracy loss. The proposed method of learning in the frequency domain leverages identical structures of the well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN, while accepting the frequency-domain information as the input. Experiment results show that learning in the frequency domain with static channel selection can achieve higher accuracy than the conventional spatial downsampling approach and meanwhile further reduce the input data size. Specifically for ImageNet classification with the same inpu t size, the proposed method achieves 1.41% and 0.66% top-1 accuracy improvements on ResNet-50 and MobileNetV2, respectively. Even with half input size, the proposed method still improves the top-1 accuracy on ResNet-50 by 1%. In addition, we observe a 0.8% average precision improvement on Mask R-CNN for instance segmentation on the COCO dataset.")),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Comments"),": Accepted to CVPR 2020"))}v.isMDXComponent=!0}}]);