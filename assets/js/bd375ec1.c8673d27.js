"use strict";(self.webpackChunkml_notebook=self.webpackChunkml_notebook||[]).push([[2164],{3905:function(e,t,r){r.d(t,{Zo:function(){return u},kt:function(){return h}});var n=r(67294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var c=n.createContext({}),s=function(e){var t=n.useContext(c),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},u=function(e){var t=s(e.components);return n.createElement(c.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),f=s(r),h=a,m=f["".concat(c,".").concat(h)]||f[h]||p[h]||o;return r?n.createElement(m,i(i({ref:t},u),{},{components:r})):n.createElement(m,i({ref:t},u))}));function h(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,i=new Array(o);i[0]=f;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var s=2;s<o;s++)i[s]=r[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}f.displayName="MDXCreateElement"},82063:function(e,t,r){r.r(t),r.d(t,{frontMatter:function(){return f},contentTitle:function(){return h},metadata:function(){return m},assets:function(){return g},toc:function(){return d},default:function(){return N}});var n=r(3905),a=Object.defineProperty,o=Object.defineProperties,i=Object.getOwnPropertyDescriptors,l=Object.getOwnPropertySymbols,c=Object.prototype.hasOwnProperty,s=Object.prototype.propertyIsEnumerable,u=(e,t,r)=>t in e?a(e,t,{enumerable:!0,configurable:!0,writable:!0,value:r}):e[t]=r,p=(e,t)=>{for(var r in t||(t={}))c.call(t,r)&&u(e,r,t[r]);if(l)for(var r of l(t))s.call(t,r)&&u(e,r,t[r]);return e};const f={title:"Non-local Networks Meet Squeeze-Excitation Networks and Beyond",authors:["visualdust"],tags:["attention-mechanism"]},h=void 0,m={permalink:"/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond",editUrl:"https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.md",source:"@site/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.md",title:"Non-local Networks Meet Squeeze-Excitation Networks and Beyond",description:"Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu",date:"2022-11-05T07:47:19.660Z",formattedDate:"2022\u5e7411\u67085\u65e5",tags:[{label:"attention-mechanism",permalink:"/blog/tags/attention-mechanism"}],readingTime:9.21,truncated:!0,authors:[{name:"Gavin Gong",title:"Rubbish CVer | Poor LaTex speaker | Half stack developer | \u952e\u5708\u8eba\u5c38\u7816\u5bb6",url:"https://gong.host",email:"gavin@gong.host",imageURL:"https://github.yuuza.net/visualDust.png",key:"visualdust"}],prevItem:{title:"Non-local Neural Networks",permalink:"/blog/[27]Non-local-Neural-Networks"},nextItem:{title:"Disentangled Non-Local Neural Networks",permalink:"/blog/[29]Disentangled-Non-Local-Neural-Networks"}},g={authorsImageUrls:[void 0]},d=[],b={toc:d};function N(e){var t,a=e,{components:u}=a,f=((e,t)=>{var r={};for(var n in e)c.call(e,n)&&t.indexOf(n)<0&&(r[n]=e[n]);if(null!=e&&l)for(var n of l(e))t.indexOf(n)<0&&s.call(e,n)&&(r[n]=e[n]);return r})(a,["components"]);return(0,n.kt)("wrapper",(t=p(p({},b),f),o(t,i({components:u,mdxType:"MDXLayout"}))),(0,n.kt)("p",null,(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y"}),"Yue Cao"),", ",(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J"}),"Jiarui Xu"),", ",(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+S"}),"Stephen Lin"),", ",(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F"}),"Fangyun Wei"),", ",(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H"}),"Han Hu")),(0,n.kt)("p",null,"GCNet\uff08\u539f\u8bba\u6587\uff1a",(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/abs/1904.11492"}),"GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond"),"\u6216",(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/abs/2012.13375"}),"Global Context Networks"),"\uff09\u8fd9\u7bc7\u8bba\u6587\u7684\u7814\u7a76\u601d\u8def\u7c7b\u4f3c\u4e8eDPN\uff0c\u6df1\u5165\u63a2\u8ba8\u4e86Non-local\u548cSENet\u7684\u4f18\u7f3a\u70b9\uff0c\u7136\u540e\u7ed3\u5408Non-local\u548cSENet\u7684\u4f18\u70b9\u63d0\u51fa\u4e86GCNet\u3002"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at ",(0,n.kt)("a",p({parentName:"p"},{href:"https://github.com/xvjiarui/GCNet"}),"this https URL"),".")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"image-20210713150550619",src:r(63257).Z})),(0,n.kt)("p",null,"GCNet\u63d0\u51fa\u4e00\u79cd\u6a21\u5757\u6846\u67b6\u79f0\u4e3aGlobal context modeling framework\uff08\u4e0a\u56fe\u4e2d(a)\uff09\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u4e09\u6b65\uff1aContext modeling\u3001Transform\u3001Fusion\u3002"),(0,n.kt)("p",null,"\u8fd9\u7bc7\u8bba\u6587\u9009\u7528",(0,n.kt)("a",p({parentName:"p"},{href:"./%5B27%5DNon-local-Neural-Networks"}),"Non-Local Neural Networks"),"\uff08\u4e0a\u56fe\u4e2d(b)\u662f\u5176\u7b80\u5316\u7248\uff09\u7684Context modeling \u548c ",(0,n.kt)("a",p({parentName:"p"},{href:"./%5B23%5DSqueeze-and-Excitation-Networks"}),"Squeeze and Excitation Networks "),"\uff08\u4e0a\u56fe\u4e2d(c)\u662f\u5176\u4e00\u79cd\u5f62\u5f0f\uff09\u7684 Transform\u8fc7\u7a0b\u7ec4\u6210\u65b0\u7684\u6a21\u5757Global Context (GC) block\uff0c\u540c\u65f6\u8bad\u7ec3spacial\u548cchannel-wise\u4e0a\u7684\u6ce8\u610f\u529b\u3002\u8fd9\u662f\u4e00\u7bc7\u5f88\u597d\u7684\u8bba\u6587\uff0c\u6709\u5174\u8da3\u8bf7\u9605\u8bfb",(0,n.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/abs/1904.11492"}),"\u539f\u6587"),"\u3002"))}N.isMDXComponent=!0},63257:function(e,t,r){t.Z=r.p+"assets/images/image-20210713150550619-77ccbe29a84029ee8123e066defbd049.png"}}]);