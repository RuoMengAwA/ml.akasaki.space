<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">PP-LCNet - A Lightweight CPU Convolutional Neural Network | 工具箱的深度学习记事簿</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="PP-LCNet - A Lightweight CPU Convolutional Neural Network | 工具箱的深度学习记事簿"><meta data-react-helmet="true" name="description" content="轻量级Trick的优化组合。"><meta data-react-helmet="true" property="og:description" content="轻量级Trick的优化组合。"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2022-11-05T07:47:19.664Z"><meta data-react-helmet="true" property="article:author" content="https://github.com/AsTheStarsFalll"><meta data-react-helmet="true" property="article:tag" content="light-weight,tricks"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a aria-current="page" class="sidebarItemLink_zyXk sidebarItemLinkActive_wcJs" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_d4p0" itemprop="headline">PP-LCNet - A Lightweight CPU Convolutional Neural Network</h1><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022年11月5日</time> · <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>轻量级Trick的优化组合。</p><blockquote><p>论文名称：<a href="https://arxiv.org/pdf/2109.15099.pdf" target="_blank" rel="noopener noreferrer">PP-LCNet: A Lightweight CPU Convolutional Neural Network</a></p><p>作者：Cheng Cui, Tingquan Gao, Shengyu Wei,Yuning Du...</p><p>Code：<a href="https://github.com/PaddlePaddle/PaddleClas" target="_blank" rel="noopener noreferrer">https://github.com/PaddlePaddle/PaddleClas</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="摘要">摘要<a aria-hidden="true" class="hash-link" href="#摘要" title="Direct link to heading">​</a></h2><ol><li>总结了一些在延迟（latency）几乎不变的情况下精度提高的技术；</li><li>提出了一种基于MKLDNN加速策略的轻量级CPU网络，即PP-LCNet。</li></ol><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007133525281.png" alt="image-20211007133525281"><h2 class="anchor anchorWithStickyNavbar_y2LR" id="介绍">介绍<a aria-hidden="true" class="hash-link" href="#介绍" title="Direct link to heading">​</a></h2><p>目前的轻量级网络在启用<a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">MKLDNN</a>的Intel CPU上速度并不理想，考虑了一下三个基本问题：</p><ol><li>如何促使网络学习到更强的特征，但不增加延迟？</li><li>在CPU上提高轻量级模型精度的要素是什么？</li><li>如何有效地结合不同的策略来设计CPU上的轻量级模型？</li></ol><h2 class="anchor anchorWithStickyNavbar_y2LR" id="method">Method<a aria-hidden="true" class="hash-link" href="#method" title="Direct link to heading">​</a></h2><p>PP-LCNet使用深度可分离卷积作为基础结构，构建了一个类似MobileNetV1的BaseNet，并在其基础上结合现有的技术，从而得到了PP-LCNet</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007191050134.png" alt="image-20211007191050134"><p>参数配置：</p><img src="C:\Users\11864\AppData\Roaming\Typora\typora-user-images\image-20211008131450331.png" alt="image-20211008131450331"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="better-activation-function">Better activation function<a aria-hidden="true" class="hash-link" href="#better-activation-function" title="Direct link to heading">​</a></h3><p>激活函数是神经网络中非线性的来源，因此其质量一定程度上决定着网络的表达能力。</p><p>当激活函数由Sigmoid变为ReLU时，网络的性能得到了很大的提升，近来出现了很多超越ReLU的激活函数，如EfficientNet的Swish，MobileNetV3中将其升级为HSwish，避免了大量的指数运算；因此本网络中使用HSwish激活函数代替ReLU。</p><p>首先让我们看一下ReLU函数的近似推导：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left right" columnspacing="0em 1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></munderover><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>i</mi><mo>+</mo><mn>0.5</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo stretchy="false">(</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>d</mi><mtext> </mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>≈</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mi>x</mi></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo stretchy="false">(</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>p</mi><mi>l</mi><mi>u</mi><mi>s</mi><mtext> </mtext><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>≈</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo stretchy="false">(</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mtext> </mtext><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align} f(x)&amp;=\sum_{i=1}^{\infin}\sigma(x-i+0.5) &amp;(stepped\ sigmoid)\\ &amp;\approx log(1+e^x) &amp;(softplus\ function)\\ &amp;\approx max(0,N(0,1)) &amp;(ReLU\ function) \end{align}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6.2291em;vertical-align:-2.8645em"></span><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.3645em"><span style="top:-5.3645em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-2.9469em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"></span></span><span style="top:-1.4469em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.8645em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.3645em"><span style="top:-5.3645em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∞</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">0.5</span><span class="mclose">)</span></span></span><span style="top:-2.9469em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-1.4469em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.8645em"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.3645em"><span style="top:-5.3645em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">pp</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mspace"> </span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span><span style="top:-2.9469em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"><span class="mopen">(</span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal" style="margin-right:0.01968em">tpl</span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span><span style="top:-1.4469em"><span class="pstrut" style="height:3.6514em"></span><span class="mord"><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em">LU</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.8645em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.3645em"><span style="top:-5.3645em"><span class="pstrut" style="height:3.6514em"></span><span class="eqn-num"></span></span><span style="top:-2.9469em"><span class="pstrut" style="height:3.6514em"></span><span class="eqn-num"></span></span><span style="top:-1.4469em"><span class="pstrut" style="height:3.6514em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.8645em"><span></span></span></span></span></span></span></span></span></div><img src="https://gitee.com/Thedeadleaf/images/raw/master/606386-20180502160705206-923153087.png" alt="softplus"><p>出于计算量的考虑和实验验证选择了ReLU</p><p>ReLU6：</p><p>增加了上界</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007184950617.png" alt="image-20211007184950617"><p><a href="https://www.cnblogs.com/makefile/p/activation-function.html" target="_blank" rel="noopener noreferrer">Swish</a>：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>⋅</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>β</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)=x\cdot sigmoid(\beta x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></div><img src="https://gitee.com/Thedeadleaf/images/raw/master/606386-20171102101521763-698600913.png" alt="swish"><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>是个常数或可训练的参数。Swish 具备无上界有下界、平滑、非单调的特性。
Swish 在深层模型上的效果略优于 ReLU。仅仅使用 Swish 单元替换 ReLU 就能把 Mobile NASNetA 在 ImageNet 上的 top-1 分类准确率提高 0.9%，Inception-ResNet-v 的分类准确率提高 0.6%。</p><p>导数：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/606386-20171102101538013-1397340773.png" alt="swish-derivation"><p>当<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">β = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span>时,Swish变为线性函数<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>x</mi><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">f(x)=\frac x 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0404em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
当<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">β → ∞</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x)=\frac{1}{1+e^{−x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2484em;vertical-align:-0.4033em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7027em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>为0或1，这时Swish变为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ReLU(x)=max(0,x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em">LU</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>
因此Swish函数可以看做是介于线性函数与ReLU函数之间的平滑函数.</p><p>HSwish：</p><p>Swish函数的计算量是很大的，因此提出了HSwish，H表示Hard，意味着超过某个范围，激活值为常数</p><p>对ReLU6除以6再向左平移三个单位可以得到HSigmoid：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007185106870.png" alt="image-20211007185106870"><p>HSwish的近似公式为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>⋅</mo><mi>h</mi><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>r</mi><mi>e</mi><mi>l</mi><mi>u</mi><mn>6</mn><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><mn>6</mn></mfrac></mrow><annotation encoding="application/x-tex">x\cdot h\sigma(x)=\frac{relu6(x+3)}{6}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">hσ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">6</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">u</span><span class="mord mtight">6</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mbin mtight">+</span><span class="mord mtight">3</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，图像如下：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007185403938.png"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="se-modules-at-appropriate-positions">SE modules at appropriate positions<a aria-hidden="true" class="hash-link" href="#se-modules-at-appropriate-positions" title="Direct link to heading">​</a></h3><p>注意力模块无疑是轻量级网络完美的选择，本文探究了SE模块放置的位置，发现在网络深层的效果较好。</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007190945690.png" alt="image-20211007190945690"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="larger-convolution-kernels">Larger convolution kernels<a aria-hidden="true" class="hash-link" href="#larger-convolution-kernels" title="Direct link to heading">​</a></h3><p>使用更大的卷积核尺寸，发现在网络深层效果较好</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007191200660.png" alt="image-20211007191200660"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="larger-dimensional11conv-layer-after-gap">Larger dimensional1×1conv layer after GAP<a aria-hidden="true" class="hash-link" href="#larger-dimensional11conv-layer-after-gap" title="Direct link to heading">​</a></h3><p>在网络最后的GAP之后使用Pointwise卷积进行升维，以此提高网络的性能</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="drop-out">Drop out<a aria-hidden="true" class="hash-link" href="#drop-out" title="Direct link to heading">​</a></h3><p>实验发现drop out可以提高性能</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007191715602.png" alt="image-20211007191715602"><h2 class="anchor anchorWithStickyNavbar_y2LR" id="实验结果">实验结果<a aria-hidden="true" class="hash-link" href="#实验结果" title="Direct link to heading">​</a></h2><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007192557037.png" alt="image-20211007192557037"><p>与其他网络进行对比</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007192635596.png" alt="image-20211007192635596"></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_xD8n"><div class="col"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/tricks">tricks</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« <!-- -->RepVGG - Making VGG-style ConvNets Great Again</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows<!-- --> »</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#摘要" class="table-of-contents__link toc-highlight">摘要</a></li><li><a href="#介绍" class="table-of-contents__link toc-highlight">介绍</a></li><li><a href="#method" class="table-of-contents__link toc-highlight">Method</a><ul><li><a href="#better-activation-function" class="table-of-contents__link toc-highlight">Better activation function</a></li><li><a href="#se-modules-at-appropriate-positions" class="table-of-contents__link toc-highlight">SE modules at appropriate positions</a></li><li><a href="#larger-convolution-kernels" class="table-of-contents__link toc-highlight">Larger convolution kernels</a></li><li><a href="#larger-dimensional11conv-layer-after-gap" class="table-of-contents__link toc-highlight">Larger dimensional1×1conv layer after GAP</a></li><li><a href="#drop-out" class="table-of-contents__link toc-highlight">Drop out</a></li></ul></li><li><a href="#实验结果" class="table-of-contents__link toc-highlight">实验结果</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>