<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ml.akasaki.space/blog</id>
    <title>工具箱的深度学习记事簿 Blog</title>
    <updated>2022-11-05T07:47:19.664Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://ml.akasaki.space/blog"/>
    <subtitle>工具箱的深度学习记事簿 Blog</subtitle>
    <icon>https://ml.akasaki.space/img/logo.svg</icon>
    <entry>
        <title type="html"><![CDATA[CCNet - Criss-Cross Attention for Semantic Segmentation]]></title>
        <id>CCNet - Criss-Cross Attention for Semantic Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[提出十字交叉注意力模块，使用循环稀疏连接代替密集连接，实现性能SOTA]]></summary>
        <content type="html"><![CDATA[<p>提出十字交叉注意力模块，使用循环稀疏连接代替密集连接，实现性能SOTA</p><blockquote><p>论文名称：<a href="https://arxiv.org/pdf/1811.11721.pdf">CCNet: Criss-Cross Attention for Semantic Segmentation</a></p><p>作者：Zilong Huang，Xinggang Wang Yun，chao Wei，Lichao Huang，Wenyu Liu，Thomas S. Huang</p><p>Code：<a href="https://github.com/speedinghzl/CCNet">https://github.com/speedinghzl/CCNet</a></p></blockquote><h2>摘要</h2><p>上下文信息在视觉理解问题中至关重要，譬如语义分割和目标检测；</p><p>本文提出了一种十字交叉的网络（Criss-Cross Net）以非常高效的方式获取完整的图像上下文信息：</p><ol><li>对每个像素使用一个十字注意力模块聚集其路径上所有像素的上下文信息；</li><li>通过循环操作，每个像素最终都可以捕获完整的图像相关性；</li><li>提出了一种类别一致性损失来增强模块的表现。</li></ol><p>CCNet具有一下优势：</p><ol><li>显存友好：相较于Non-Local减少显存占用11倍</li><li>计算高效：循环十字注意力减少Non-Local约85%的计算量</li><li>SOTA</li><li>Achieve the <strong>mIoU</strong> scores of 81.9%, 45.76% and 55.47% on the <strong>Cityscapes test set</strong>, the <strong>ADE20K validation set</strong> and the <strong>LIP validation set</strong> respectively</li></ol><h2>介绍</h2><ul><li>当前FCN在语义分割任务取得了显著进展，但是由于固定的几何结构，分割精度局限于FCN<strong>局部感受野</strong>所能提供的<strong>短程感受野</strong>，目前已有相当多的工作致力于弥补FCN的不足，相关工作看论文。</li><li>密集预测任务实际上需要高分辨率的特征映射，因此Non-Local的方法往往计算复杂度高，并且占用大量显存，因此设想使用几个连续的稀疏连通图（sparsely-connected graphs）来替换常见的单个密集连通图（ densely-connected graph），提出CCNet使用稀疏连接来代替Non-Local的密集连接。</li><li>为了推动循环十字注意力学习更多的特征，引入了类别一致损失（category consistent loss）来增强CCNet，其强制网络将每个像素映射到特征空间的n维向量，使属于同一类别的像素的特征向量靠得更近。</li></ul><h2>方法</h2><p>CCNet可能是受到之前将卷积运算分解为水平和垂直的GCN以及建模全局依赖性的Non-Local，CCNet使用的十字注意力相较于分解更具优势，拥有比Non-Local小的多得计算量。</p><p>文中认为CCNet是一种图神经网络，特征图中的每个像素都可以被视作一个节点，利用节点间的关系（上下文信息）来生成更好的节点特征。</p><p>最后，提出了同时利用时间和空间上下文信息的3D十字注意模块。</p><h3>网络结构</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816133242063.png" alt="image-20210816133242063"/></p><p>整体流程如下：</p><ol><li>对于给定的$X$，使用卷积层获得降维的特征映射$H$；</li><li>$H$会输入十字注意力模块以生成新的特征映射$H&#x27;$​，其中每个像素都聚集了垂直和水平方向的信息；</li><li>进行一次循环，将$H&#x27;$输入十字注意力，得到$H&#x27;&#x27;$，其中每个像素实际上都聚集了所有像素的信息；</li><li>将$H&#x27;&#x27;$与局部特征表示$X$进行$Concatenation$​​；</li><li>由后续的网络进行分割。</li></ol><h3>Criss-Cross Attention</h3><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816135006656.png" alt="image-20210816135006656"/><p>主要流程如下：</p><ol><li><p>使用$1\times 1$​卷积进行降维得到$Q，K\in\mathbb{R}^{C&#x27;\times W\times H}$​；</p></li><li><p>通过<strong>Affinity</strong>操作生成注意力图$A\in\mathbb{R}^{(H+W-1)\times (H\times W)}$​，其中：</p><ol><li><p>对于$Q$​空间维度上的的每一个位置$u$​，我们可以得到一个向量$Q_u\in\mathbb{R}^{C&#x27;}$​；​​​</p></li><li><p>同时，我们在$K$​上得到一个集合$\Omega_u\in \mathbb{R}^{(H+W-1)\times C&#x27;}$​​，其代表着位置$u$​​​的同一行或同一列；</p></li><li><p>令$\Omega<em>{i,u}$​表示$\Omega</em>{u}$​的第$i$个元素，<strong>Affinity</strong>操作可以表示为：
$$
d<em>{i,u}=Q_u\Omega</em>{i,u}^T\qquad i\in <!-- -->[0,1,\cdots,H+W-1]<!-- -->,u\in<!-- -->[0,1,\cdots,H\times W]<!-- -->
$$
其用来表示两者之间的相关性，最终我们可以得到$D\in\mathbb{R}^{(H+W-1)\times (H\times W)}$​​</p></li><li><p>最终在通道维度上对$D$​使用$Softmax$​，即可得到注意力图$A$​，需要注意的是，这里的<strong>通道维度</strong>代表的是$H+W-1$​​​​这个维度，其表示某个位置像素与其垂直水平方向上像素的相关性。</p></li></ol></li><li><p>另一方面，依旧使用$1\times 1$卷积生成$V\in\mathbb{R}^{C\times W\times H}$，我们可以获得一个向量$V_u\in \mathbb{R}^C$和一个集合$\Phi_u\in \mathbb{R}^{(H+W-1)\times C}$​</p></li><li><p>最后使用<strong>Aggregation</strong>操作得到最终的特征图，其定义为：
$$
H&#x27;<em>u=\sum</em>{i=0}^{H+W-1}A<em>{i,u}\Phi</em>{i,u}+H_u
$$
其中$H&#x27;_u\in\mathbb{R}^{C}$​​​​​是某个位置的特征向量。</p></li></ol><p>至此，我们已经能够捕获某个位置像素水平和垂直方向上的信息，然而，该像素与周围的其他像素仍然不存在关系，为了解决这个问题，提出了循环机制。</p><h3>Recurrent Criss-Cross Attention (RCCA)</h3><p>通过多次使用CCA来达到对上下文进行建模，当循环次数R=2时，特征图中任意两个空间位置的关系可以定义为：
$$
\exist i\in\mathbb{R}^{H+W+1},s.t.A<em>{i,u}=f(A,u</em>{x}^{CC},u^{CC}_y,u_x,u_y)
$$
方便起见，对于特征图上的两个位置$(u_x,u_y)$和$(\theta_x,\theta_y)$，其信息传递示意图如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816164356027.png" alt="image-20210816164356027"/></p><p>可以看到，经过两次循环，原本不相关的位置也能够建立联系了。</p><h3>Learning Category Consistent Features</h3><p>对于语义分割任务，属于同一类别的像素应该具有相似的特征，而来自不同类别的像素应该具有相距很远的特征。</p><p>然而，聚集的特征可能存在过度平滑的问题，这是图神经网络中的一个常见问题，为此，提出了类别一致损失。
$$
l<em>{var}=\frac{1}{|C|}\sum</em>{c\in C}\frac{1}{N<em>c}\sum</em>{i=1}^{N<em>c}\varphi</em>{var}(h_i,\mu_i)
$$</p><p>$$
l<em>{dis}=\frac{1}{|C|(|C|-1)}\sum</em>{c<em>a\in C}\sum</em>{c<em>b\in C}\varphi</em>{dis}(\mu<em>{c_a},\mu</em>{c_b})
$$</p><p>$$
l<em>{reg}=\frac{1}{|C|}\sum</em>{c\in C}||\mu_c||
$$</p><p>其中的距离函数$\varphi$设计为分段形式，公式如下：
$$
\varphi_{var}=\left<!-- -->{<!-- --> \begin{array}{l}
||\mu_c-h_i||-\delta{_d}+(\delta{_d}-\delta{_v})^2,&amp;||\mu_c-h_i||&gt;\delta{_d}<!-- -->\<!-- -->
(||\mu_c-h_i||-\delta{_v})^2,&amp;\delta{_d}&gt;||\mu_c-h_i||\geqslant\delta{_v}<!-- -->\<!-- -->
0 &amp;||\mu_c-h_i||\leqslant\delta{_d}
\end{array}\right.
$$</p><p>$$
\varphi<em>{dis}=\left<!-- -->{<!-- -->\begin{array}
{l}
(2\delta{_d}-||\mu</em>{c<em>a}-\mu</em>{c<em>b}||)^2,&amp;||\mu</em>{c<em>a}-\mu</em>{c<em>b}||\leqslant2\delta{_d}<!-- -->\<!-- -->
0,&amp;||\mu</em>{c<em>a}-\mu</em>{c_b}||&gt;2\delta{_d}
\end{array}\right.
$$</p><p>本文中，距离阈值的设置为$\delta{_v}=0.5,\delta{_d}=1.5$​</p><p>为了加速计算，对RCCA的输入进行降维，其比率设置为16</p><p>总的损失函数定义如下：
$$
l=l<em>{seg}+\alpha l</em>{var}+\beta l<em>{dis}+\gamma l</em>{reg}
$$
本文中，$\alpha,\beta,\gamma$​​的值分别为1,1,0.001，</p><h3>3D Criss-Cross Attention</h3><p>在2D注意力的基础上进行推广，提出3DCCA，其可以在时间维度上收集额外的上下文信息</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816184130138.png" alt="image-20210816184130138"/><p>其流程与2DCCA大致相同，具体细节差异看论文。</p><h2>代码复现</h2><h3>Criss-Cross Attention</h3><pre><code class="language-python">def INF(B,H,W):
    # tensor -&gt; torch.size([H]) -&gt; 对角矩阵[H,H] -&gt; [B*W,H,H] 
    # 消除重复计算自身的影响
    return -torch.diag(torch.tensor(float(&quot;inf&quot;)).cuda().repeat(H),0).unsqueeze(0).repeat(B*W,1,1)
</code></pre><pre><code class="language-python">class CrissCrossAttention(nn.Module):
    &quot;&quot;&quot; Criss-Cross Attention Module&quot;&quot;&quot;
    def __init__(self, in_ch,ratio=8):
        super(CrissCrossAttention,self).__init__()
        self.q = nn.Conv2d(in_ch, in_ch//ratio, 1)
        self.k = nn.Conv2d(in_ch, in_ch//ratio, 1)
        self.v = nn.Conv2d(in_ch, in_ch, 1)
        self.softmax = nn.Softmax(3)
        self.INF = INF
        self.gamma = nn.Parameter(torch.zeros(1)) # 初始化为0


    def forward(self, x):
        bs, _, h, w = x.size()
        # Q
        x_q = self.q(x)
        # b,c&#x27;,h,w -&gt; b,w,c&#x27;,h -&gt; b*w,c&#x27;,h -&gt; b*w,h,c&#x27;
        # 后两维相当于论文中的Q_u
        x_q_H = x_q.permute(0,3,1,2).contiguous().view(bs*w,-1,h).permute(0, 2, 1)
        # b,c&#x27;,h,w -&gt; b,h,c&#x27;,w -&gt; b*h,c&#x27;,w -&gt; b*h,w,c&#x27;
        x_q_W = x_q.permute(0,2,1,3).contiguous().view(bs*h,-1,w).permute(0, 2, 1)
        # K
        x_k = self.k(x) # b,c&#x27;,h,w
        # b,c&#x27;,h,w -&gt; b,w,c&#x27;,h -&gt; b*w,c&#x27;,h
        x_k_H = x_k.permute(0,3,1,2).contiguous().view(bs*w,-1,h)
        # b,c&#x27;,h,w -&gt; b,h,c&#x27;,w -&gt; b*h,c&#x27;,w
        x_k_W = x_k.permute(0,2,1,3).contiguous().view(bs*h,-1,w)
        # V
        x_v = self.v(x)
        # b,c,h,w -&gt; b,w,c,h -&gt; b*w,c,h
        x_v_H = x_v.permute(0,3,1,2).contiguous().view(bs*w,-1,h) 
        # b,c,h,w -&gt; b,h,c,w -&gt; b*h,c,w
        x_v_W = x_v.permute(0,2,1,3).contiguous().view(bs*h,-1,w)
        # torch.bmm计算三维的矩阵乘法，如[bs,a,b][bs,b,c]
        # 先计算所有Q_u和K上与位置u同一列的
        energy_H = (torch.bmm(x_q_H, x_k_H)+self.INF(bs, h, w)).view(bs,w,h,h).permute(0,2,1,3) # b,h,w,h
        # 再计算行
        energy_W = torch.bmm(x_q_W, x_k_W).view(bs,h,w,w)
        # 得到注意力图
        concate = self.softmax(torch.cat([energy_H, energy_W], 3)) # b,h,w,h+w

        # 后面开始合成一张图
        att_H = concate[:,:,:,0:h].permute(0,2,1,3).contiguous().view(bs*w,h,h)
        #print(concate)
        #print(att_H) 
        att_W = concate[:,:,:,h:h+w].contiguous().view(bs*h,w,w)
        # 同样的计算方法
        out_H = torch.bmm(x_v_H, att_H.permute(0, 2, 1)).view(bs,w,-1,h).permute(0,2,3,1) # b,c,h,w
        out_W = torch.bmm(x_v_W, att_W.permute(0, 2, 1)).view(bs,h,-1,w).permute(0,2,1,3) # b,c,h,w
        #print(out_H.size(),out_W.size())
        return self.gamma*(out_H + out_W) + x # 乘积使得整体可训练
</code></pre><h3>Category Consistent Loss</h3><h2>实验</h2><p>在Cityscapes、ADE20K、COCO、LIP和CamVid数据集上进行了实验，在一些数据集上实现了SOTA，并且在Cityscapes数据集上进行了消融实验。</p><h3>实验结果</h3><p>在Cityscapes上的结果：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816184546419.png" alt="image-20210816184546419"/><h3>消融实验</h3><h4>RCCA模块</h4><p>通过改变循环次数进行了如下实验：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816185515867.png" alt="image-20210816185515867"/></p><p>可以看到，RCCA模块可以有效的聚集全局上下文信息，同时保持较低的计算量。</p><p>为了进一步验证CCA的有效性，进行了定性比较：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816184752238.png" alt="image-20210816184752238"/></p><p>随着循环次数的增加，这些白色圈圈区域的预测逐渐得到纠正，这证明了密集上下文信息在语义分割中的有效性。</p><h4>类别一致损失</h4><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816190815707.png" alt="image-20210816190815707"/></p><p>上图中的CCL即表示使用了类别一致损失</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816190830125.png" alt="image-20210816190830125"/></p><p>上述结果表明了分段距离和类别一致损失的有效性。</p><h4>对比其他聚集上下文信息方法</h4><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816190815707.png" alt="image-20210816190815707"/></p><p>同时，对Non Local使用了循环操作，可以看到，循环操作带来了超过一点的增益，然而其巨量的计算量和显存需求限制性能</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816214333065.png" alt="image-20210816214333065"/><h4>可视化注意力图</h4><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816215023367.png" alt="image-20210816215023367"/></p><p>上图中可以看到循环操作的有效性。</p><h3>更多实验</h3><p>在ADE20K上的实验验证了类别一致损失（CCL）的有效性：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816215139417.png" alt="image-20210816215139417"/></p><p>在LIP数据集的实验结果：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816215228469.png" alt="image-20210816215228469"/></p><p>在COCO数据集的实验结果：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816215624406.png" alt="image-20210816215624406"/></p><p>在CamVid数据上的实验结果：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210816215702601.png" alt="image-20210816215702601"/></p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RepVGG - Making VGG-style ConvNets Great Again]]></title>
        <id>RepVGG - Making VGG-style ConvNets Great Again</id>
        <link href="https://ml.akasaki.space/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[这篇笔记的写作者是AsTheStarsFall。]]></summary>
        <content type="html"><![CDATA[<p>这篇笔记的写作者是<a href="https://github.com/asthestarsfalll">AsTheStarsFall</a>。</p><blockquote><p>论文名称：<a href="https://arxiv.org/abs/2101.03697">RepVGG: Making VGG-style ConvNets Great Again</a></p><p>作者：Xiaohan Ding，Xiangyu Zhang，Ningning Ma，Jungong Han，Guiguang Ding，Jian Sun</p><p>Code：<a href="https://github.com/DingXiaoH/RepVGG">https://github.com/DingXiaoH/RepVGG</a></p></blockquote><h2>摘要</h2><ol><li>提出了一种简单强大的CNN，推理时其拥有VGG类似的plain结构，仅由卷积和ReLU组成；训练时拥有多分支的拓扑结构</li><li>得益于结构重参化（re-parameterization）技术，RepVGG运行速度比ResNet-50快83%，比ResNet-101快101%，并且具有更高的精度。</li></ol><img src="https://i.loli.net/2021/09/14/H4NT17LA635BQgK.png"/><h2>介绍</h2><p>随着CNN在计算机视觉领域的成功，网络结构越来越复杂。这些复杂的结构相较于plain的网络结构虽然能够提供更高的精度，然是其缺点也是显而易见的：</p><ol><li>复杂的多分支结构，各种各样的残差连接和concatenation增加了网络实现和修改难度，降低了推理速度和内存利用效率；</li><li>某些模块如深度卷积、channel shuffle等增加了内存访问的成本；</li></ol><p>推理时间受到各种因素的影响，浮点运算（FLOPs）并不能准确反映实际速度，实际上VGG和NesNet在工业界还在大量使用着。而本文提出的RepVGG有以下优势：</p><ol><li>与VGG相同的plain结构，没有任何分支；</li><li>只使用$3\times 3$卷积；</li><li>具体的架构没有使用自动搜索、手动优化、复合缩放等繁重的操作，仅仅使用了重参化。</li></ol><h2>相关工作</h2><h3>From Single-path to Multi-branch</h3><p>主要是介绍各种网络结构的演变和进化，愈来愈多的复杂结构和结构搜索方法虽然一定程度上提高了模型性能，但是代价是巨大的计算资源。</p><h3>Effective Training of Single-path Models</h3><p>已有一些工作尝试训练没有分支的网络，这些网络往往非常深，不能做到精确拟合，有工作提出了一种初始化方法用来训练10000层的plain卷积网络，但是这些网络既不方便也不实用。</p><h3>Model Re-parameterization</h3><p>DiracNet将卷积层编码为$\hat W=diag(a)I+diag(b)W<em>{norm}$， $\hat W$表示最终使用的权重矩阵，$a,b$是可学习的向量，$W</em>{norm}$是可学习的归一化矩阵。与具有同等参数量的ResNet相比，精度有所下降。</p><p>实际上，DiracNet是将卷积核以另一种数学形式表达，使其更容易优化。</p><p>也有其他的工作使用不同的重参化方式，但是RepVGG的方法对于实现plain结构更为重要。</p><h3>Winograd Convolution</h3><p>Winogard 是一种加速$3\times 3$卷积（stride=1）的经典算法，其乘法量减少到原来的4/9，因此RepVGG重参化之后仅仅使用$3\times3$卷积来加速推理。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210915133349387.png" alt="image-20210915133349387"/></p><h2>Building RepVGG via Structural Re-param</h2><p>选择plain网络结构的原因</p><h3>Simple is Fast, Memory-economical, Flexible</h3><p>使用简单的卷积神经网络（比如VGG）是因为其拥有至少以下三个优点：</p><ol><li><p><strong>快速</strong></p><p>很多网络拥有用比VGG更小的理论浮点计算量（FLOPs），但是其实际推理速度并没有VGG快速，FlOPs不能代表网络的计算速度。</p><p>计算速度主要与两个重要因素有关：</p><ul><li><strong>内存访问成本（MAC）</strong>：虽然残差连接和concatenation几乎可以忽略不计，但是其提高了内存访问成本（残差连接需要提高一倍的内存占用），此外，组卷积也会提高时间；</li><li><strong>并行度</strong>：并行度是另一个重要因素，Inseption和一些自动搜索架构使用了很多小操作（small
operators），这大大降低了网络的并行度。</li></ul></li><li><p><strong>内存经济</strong></p><p>对于多分支的网络拓扑结构，每个分支的输出都必须保留，直到addition或concatenation操作完成，这会大大提高内存占用，如下图：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210915135344621.png" alt="image-20210915135344621"/><p>而plain的网络结构能够及时释放内存，并且设计专门的硬件时可以将更多的计算单元集成到芯片上。</p></li><li><p><strong>灵活</strong></p></li></ol><p>多分支的网络结构限制了本身的灵活性，很难进行修改，牵一发而动全身，并且剪枝技术也受到很多限制。相比之下，plain结构允许我们根据需求自由配置歌层，并进行修剪以获得更好的性能效率权衡。</p><h3>Training-time Multi-branch Architecture</h3><p>plain的网络结构有一个致命的缺点——性能差，使用BN层的情况下，VGG-16仅仅能达到72% top-1准确率；</p><p>受到ResNet的启发，使用一个$y=x+f(x)$，当$x、f(x)$不匹配时，使用$1\times 1$的卷积层，则$y=g(x)+f(x)$；</p><p>ResNet成功的一个解释是，这种多分支的结构使得网络成为各个浅层模型的隐式集成，具体来说，有n个Block时，模型可以被解释为$2^n$个模型的集合，因为每个块将流分成两条路径。</p><p>虽然多分支结构在推理方面存在缺陷，但是其十分有利于训练；于是构建了一个$y=x+g(x)+f(x)$的结构，堆叠了n层，从上述可知，这意味着$3^n$个模型的集合。</p><h3>Re-param for Plain Inference-time Model</h3><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210915185030692.png" alt="image-20210915185030692"/><p>推理之前会进行重参数化，RepVGG的模块结构如上图所示，由$3\times 3,1\times 1,identity$组成，将这些全部变为$3\times3$卷积相加即可实现ReP。</p><p>$1\times1$：赋值给$3\times3$矩阵的中心，其余为0即可，具体实现可以使用zero-padding</p><p><strong>identity</strong>：将$3\times3$矩阵中心赋值为1，其余为0</p><p>padding的代码为</p><pre><code class="language-python">    def _pad_1x1_to_3x3_tensor(self, kernel1x1):
        if kernel1x1 is None:
            return 0
        else:
            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])
</code></pre><p><strong>BN</strong>：</p><p>设卷积$Conv(x)=W(x)+b$，BN为$BN(x)=\gamma\frac{(x-mean)}{\sqrt{var}}+\beta$</p><p> 带入可得:
$$
\begin{align}
BN(Conv(x))&amp;=\gamma\frac{(W(x)+b-mean)}{\sqrt{var}}+\beta<!-- -->\<!-- -->&amp;=\frac{\gamma<em>W(x)}{\sqrt{var}}+(\frac{\gamma</em>(b-mean)}{\sqrt{var}}+\beta)
\end{align}
$$
注意后面为卷积的偏置项</p><pre><code class="language-python">    def _fuse_bn_tensor(self, branch):
        if branch is None:
            return 0, 0
        if isinstance(branch, nn.Sequential):
            kernel = branch.conv.weight
            running_mean = branch.bn.running_mean
            running_var = branch.bn.running_var
            gamma = branch.bn.weight
            beta = branch.bn.bias
            eps = branch.bn.eps
        else:
            assert isinstance(branch, nn.BatchNorm2d)
            if not hasattr(self, &#x27;id_tensor&#x27;):
                input_dim = self.in_channels // self.groups
                kernel_value = np.zeros(
                    (self.in_channels, input_dim, 3, 3), dtype=np.float32)
                for i in range(self.in_channels):
                    kernel_value[i, i % input_dim, 1, 1] = 1
                self.id_tensor = torch.from_numpy(
                    kernel_value).to(branch.weight.device)
            kernel = self.id_tensor
            running_mean = branch.running_mean
            running_var = branch.running_var
            gamma = branch.weight
            beta = branch.bias
            eps = branch.eps
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)
        return kernel * t, beta - running_mean * gamma / std
</code></pre><p>这里BN的<code>running_mean</code>和<code>running_var</code>是从开始训练就一直记录的，并且其计算场景是<code>online</code>的，因为事先不知道会有多少数据，所以只能每次增量计算。计算公式为：
$$
mean<em>{t}=\frac{\sum</em>{i=1}^{t}x<em>i}{N_t}=\frac{mean</em>{t-1}\cdot(N<em>{t-1}+x_t)}{N_t}=mean</em>{t-1}+\frac{1}{N<em>t}(x_i-mean</em>{t-1})
$$
代码如下</p><pre><code class="language-python">class RunningStats:

    def __init__(self):
        self.n = 0
        self.old_m = 0
        self.new_m = 0
        self.old_s = 0
        self.new_s = 0
    
    def clear(self):
        self.n = 0
        
    def push(self, x):
        self.n += 1
        
        if self.n == 1:
            self.old_m = self.new_m = x
            self.old_s = 0
        else:
            self.new_m = self.old_m + (x - self.old_m) / self.n # 更新
            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)
            
            self.old_m = self.new_m
            self.old_s = self.new_s

    def mean(self):
        return self.new_m if self.n else 0.0
    
    def variance(self):
        return self.new_s / (self.n - 1) if self.n &gt; 1 else 0.0
</code></pre><h3>Architectural Specification</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210915210000222.png" alt="image-20210915210000222"/></p><p>上图表示RepVGG各个阶段的输出，第一个阶段使用Stride=2的conv来进行下采样</p><p>第一阶段输入分辨率高，仅使用一个卷积层，在最后一个阶段（$14\times14$）使用更多的卷积层</p><p>使用Efficient neural networks中经典的multiplier来控制网络大小和性能的平衡，具体看论文</p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PP-LCNet - A Lightweight CPU Convolutional Neural Network]]></title>
        <id>PP-LCNet - A Lightweight CPU Convolutional Neural Network</id>
        <link href="https://ml.akasaki.space/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[轻量级Trick的优化组合。]]></summary>
        <content type="html"><![CDATA[<p>轻量级Trick的优化组合。</p><blockquote><p>论文名称：<a href="https://arxiv.org/pdf/2109.15099.pdf">PP-LCNet: A Lightweight CPU Convolutional Neural Network</a></p><p>作者：Cheng Cui, Tingquan Gao, Shengyu Wei,Yuning Du...</p><p>Code：<a href="https://github.com/PaddlePaddle/PaddleClas">https://github.com/PaddlePaddle/PaddleClas</a></p></blockquote><h2>摘要</h2><ol><li>总结了一些在延迟（latency）几乎不变的情况下精度提高的技术；</li><li>提出了一种基于MKLDNN加速策略的轻量级CPU网络，即PP-LCNet。</li></ol><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007133525281.png" alt="image-20211007133525281"/><h2>介绍</h2><p>目前的轻量级网络在启用<a href="https://github.com/oneapi-src/oneDNN">MKLDNN</a>的Intel CPU上速度并不理想，考虑了一下三个基本问题：</p><ol><li>如何促使网络学习到更强的特征，但不增加延迟？</li><li>在CPU上提高轻量级模型精度的要素是什么？</li><li>如何有效地结合不同的策略来设计CPU上的轻量级模型？</li></ol><h2>Method</h2><p>PP-LCNet使用深度可分离卷积作为基础结构，构建了一个类似MobileNetV1的BaseNet，并在其基础上结合现有的技术，从而得到了PP-LCNet</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007191050134.png" alt="image-20211007191050134"/><p>参数配置：</p><img src="C:Users	864AppDataRoamingTypora	ypora-user-imagesimage-20211008131450331.png" alt="image-20211008131450331"/><h3>Better activation function</h3><p>激活函数是神经网络中非线性的来源，因此其质量一定程度上决定着网络的表达能力。</p><p>当激活函数由Sigmoid变为ReLU时，网络的性能得到了很大的提升，近来出现了很多超越ReLU的激活函数，如EfficientNet的Swish，MobileNetV3中将其升级为HSwish，避免了大量的指数运算；因此本网络中使用HSwish激活函数代替ReLU。</p><p>首先让我们看一下ReLU函数的近似推导：
$$
\begin{align}
f(x)&amp;=\sum_{i=1}^{\infin}\sigma(x-i+0.5) &amp;(stepped\ sigmoid)<!-- -->\<!-- -->
&amp;\approx log(1+e^x)  &amp;(softplus\ function)<!-- -->\<!-- -->
&amp;\approx max(0,N(0,1)) &amp;(ReLU\ function)
\end{align}
$$</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/606386-20180502160705206-923153087.png" alt="softplus"/><p>出于计算量的考虑和实验验证选择了ReLU</p><p>ReLU6：</p><p>增加了上界</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007184950617.png" alt="image-20211007184950617"/><p><a href="https://www.cnblogs.com/makefile/p/activation-function.html">Swish</a>：
$$
f(x)=x\cdot sigmoid(\beta x)
$$</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/606386-20171102101521763-698600913.png" alt="swish"/><p>$\beta$是个常数或可训练的参数。Swish 具备无上界有下界、平滑、非单调的特性。
Swish 在深层模型上的效果略优于 ReLU。仅仅使用 Swish 单元替换 ReLU 就能把 Mobile NASNetA 在 ImageNet 上的 top-1 分类准确率提高 0.9%，Inception-ResNet-v 的分类准确率提高 0.6%。</p><p>导数：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/606386-20171102101538013-1397340773.png" alt="swish-derivation"/><p>当$β = 0$时,Swish变为线性函数$f(x)=\frac x 2$
当$β → ∞$, $\sigma(x)=\frac{1}{1+e^{−x}}$为0或1，这时Swish变为$ReLU(x)=max(0,x)$
因此Swish函数可以看做是介于线性函数与ReLU函数之间的平滑函数.</p><p>HSwish：</p><p>Swish函数的计算量是很大的，因此提出了HSwish，H表示Hard，意味着超过某个范围，激活值为常数</p><p>对ReLU6除以6再向左平移三个单位可以得到HSigmoid：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007185106870.png" alt="image-20211007185106870"/><p>HSwish的近似公式为$x\cdot h\sigma(x)=\frac{relu6(x+3)}{6}$，图像如下：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007185403938.png"/><h3>SE modules at appropriate positions</h3><p>注意力模块无疑是轻量级网络完美的选择，本文探究了SE模块放置的位置，发现在网络深层的效果较好。</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007190945690.png" alt="image-20211007190945690"/><h3>Larger convolution kernels</h3><p>使用更大的卷积核尺寸，发现在网络深层效果较好</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007191200660.png" alt="image-20211007191200660"/><h3>Larger dimensional1×1conv layer after GAP</h3><p>在网络最后的GAP之后使用Pointwise卷积进行升维，以此提高网络的性能</p><h3>Drop out</h3><p>实验发现drop out可以提高性能</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007191715602.png" alt="image-20211007191715602"/><h2>实验结果</h2><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007192557037.png" alt="image-20211007192557037"/><p>与其他网络进行对比</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007192635596.png" alt="image-20211007192635596"/>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swin Transformer - Hierarchical Vision Transformer using Shifted Windows]]></title>
        <id>Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</id>
        <link href="https://ml.akasaki.space/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[分层Local Vision Transformer，通用主干网络，各类下游任务实现SOTA。Best Paper Award!]]></summary>
        <content type="html"><![CDATA[<p>分层Local Vision Transformer，通用主干网络，各类下游任务实现SOTA。Best Paper Award!</p><blockquote><p>论文名称：<a href="https://arxiv.org/abs/2103.14030">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p><p>作者：Ze Liu ，Yutong Lin，Yue Cao，Han Hu，Yixuan Wei，Zheng Zhang，Stephen Lin，Baining Guo</p><p>Code：<a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a></p></blockquote><h2>介绍</h2><p>自AlexNet以来，CNN作为骨干（backbone）在计算机视觉中得到了广泛应用；另一方面，自然语言处理中的网络结构的演变则走了一条不同的道路，现在的主流结构是Transformer。</p><p>Transformer是为序列建模和转换任务而设计的，它以关注数据中的长期依赖关系而著称。其在NLP领域的巨大成功吸引了人们研究它对CV的适应性，最近的实验显示其在图像分类和联合视觉语言建模方面有所成效。</p><p>本文的主要贡献有：</p><ol><li>提出了一种分层Transformer，其可以作为计算机视觉的通用主干网络，并且在各类下游任务上取得SOTA；</li><li>通过Shift Windows实现了对输入图像尺寸的线性时间复杂度。</li></ol><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211020204110669.png" alt="image-20211020204110669"/><h2>Method</h2><h3>整体结构</h3><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211018125415232.png" alt="image-20211018125415232"/><p>上图是Swin Transformer中最小版本的可视化结构图，其主要流程如下：</p><ol><li>通过Patch Partition将输入的RGB图像分割成不重叠的Patch，堆叠进B维度；</li><li>使用Linear Embedding将通道映射至C；</li><li>紧接着使用两个连续的Swin Transformer Block，将上述组合称为Stage 1；</li><li>为了获得分层表示，通过Patch Merging对Stage 1的输出继续进行分块，并且同样会使用Linear Layer进行通道降维，再使用几个连续的Swin Transformer Block；如此，便能构成更多的Stage.</li></ol><h3>Shifted Window based Self-Attention</h3><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211018165411438.png" alt="image-20211018165411438"/><p>连续的Swin Transformer如上图所示，其主要流程如下：</p><ol><li><p>对于第一个Swin Transfromer Block，会先对输入$Z^{l-1}$使用LayerNorm，然后使用W-MSA（Window based Self-Attention），并且使用残差连接得到$\hat z^l$，可以写成如下形式：
$$
\hat Z^l = W-MSA(LN(Z^{l-1}))+Z^{l-1}
$$</p></li><li><p>接下来使用LN、MLP（两层、GELU激活函数）和残差连接的得到最终输出$Z^l$，可以写成如下形式：
$$
Z^l=MLP(LN(\hat Z^l))+\hat Z^l
$$</p></li><li><p>对于接下来的Swin Transformer Block，会将其W-MSA替换成SW-MSA（Shifted Window based Self-Attention），可写成如下形式：
$$
\hat Z^{l+1} = SW-MSA(LN(Z^{l}))+Z^{l}<!-- -->\<!-- -->
Z^{l+1}=MLP(LN(\hat Z^{l+1}))+\hat Z^{l+1}
$$</p></li></ol><p>至此便完成了连续的Swin Transformer Block的构建，由于需要将这两种组合起来达到信息交换的目的，因此层数的设置应为偶数。</p><h4>Self-attention in non-overlapped windows</h4><p>为了实现线性的时间复杂度，提出在Window（窗口）中进行建模，窗口以非重叠的方式均匀地划分图像，这种方式在局部窗口中进行Patch的关系建模，计算注意力时，会将Patch展品与标准多头自注意力的时间复杂度对比如下：
$$
\Omega(MSA)=4hwC^2+2(hw)^2C<!-- -->\<!-- -->
\Omega(W-MSA)=4hwC^2+2M^2hwC
$$
其中输入包含$M\times M$个Patch。</p><p>由于M是固定的，所有W-MSA对输入图像尺寸的复杂度呈线性。</p><h4>Shifted window partitioning in successive blocks</h4><p>虽然W-MSA解决了MSA时间复杂度随输入二次增长的问题，但是不同窗口间没有信息交流，这显然会限制模型的建模能力。</p><p>为了保持高效的同时进行有效建模，提出了Shifted Window：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211019210948129.png" alt="image-20211019210948129"/><p>通过控制不同框的大小，实现上一层不同Window之间的信息交流，但是这样较难实现，并且Window的数量会从$<!-- -->[\frac hM]<!-- -->\times<!-- -->[\frac wM]<!-- -->$增加到$(<!-- -->[\frac hM]<!-- -->+1)\times(<!-- -->[\frac wM]<!-- -->+1)$，并且某些Window的大小会小于$M\times M$，因此提出了一种更简单的方法来实现这个功能：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211019230354019.png" alt="image-20211019230354019"/><p>将原有的窗口以M/2的大小进行偏移，将多出的部分移动到相对的位置，这样就实现了不同Window之前的信息交流，不过需要注意的一点是，实际计算的过程中会使用Mask，将上图右侧移动过来的位置给盖住，原因是这部分的注意力没有意义。</p><h4>Relative position bias</h4><p>添加了相对位置偏置$B\in \mathbb R^{M^2\times M^2}$，其描述每个Window相对于其它Window的相对位置，注意力公式可以写成：
$$
Atten(Q,K,V)=SoftMax(QK^T/\sqrt d +B)V
$$
该相对位置偏置相较于绝对位置嵌入拥有更好的性能。</p><p>由于每个轴上的相对位置的取值范围都是$<!-- -->[-M+1,M-1]<!-- -->$，于是生成一个小的偏置矩阵$\hat B\in \mathbb{R}^{(2M-1)\times(2M-1)}$，相对位置偏置$B$从$\hat B$中采样而来。</p><h3>Patch merging</h3><p>Patch merging起到一个“下采样”的作用，具体实现方式是CNN中空间到深度的变换，将空间信息堆叠进通道中，这就相当于变相扩大了Window的大小</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211020114206242.png" alt="image-20211020114206242"/><h2>代码分析</h2><h3>Window operation</h3><p>window_partition：</p><p>将输入图像分割成$window<!-- -->_<!-- -->size\times window<!-- -->_<!-- -->size$大小的patch，并堆叠进Batch维度。</p><pre><code class="language-python">def window_partition(x, window_size):
    &quot;&quot;&quot;
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    &quot;&quot;&quot;
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size,
               W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous(
    ).view(-1, window_size, window_size, C)
    return windows
</code></pre><p>window_reverse：</p><p>恢复，用于残差连接之前。</p><pre><code class="language-python">def window_reverse(windows, window_size, H, W):
    &quot;&quot;&quot;
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    &quot;&quot;&quot;
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size,
                     window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x
</code></pre><h3>WindowAttention</h3><p>该部分代码为W-MSA和SW-MSA，具体切换依赖于输入数据和mask，该模块只计算负责Window内的自注意力。</p><pre><code class="language-python">class WindowAttention(nn.Module):
    r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    &quot;&quot;&quot;

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - \
            coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(
            1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - \
            1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer(&quot;relative_position_index&quot;,
                             relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)#使用一个线性层生成QKV，使用切片分开
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)#对输出进行映射
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        &quot;&quot;&quot;
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        &quot;&quot;&quot;
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C //
                                  self.num_heads).permute(2, 0, 3, 1, 4)
        # make torchscript happy (cannot use tensor as tuple)
        q, k, v = qkv[0], qkv[1], qkv[2]

        q = q * self.scale #意义不明
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(
            2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None: #当使用SW-MSA时，会使用mask
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N,
                             N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -&gt; str:
        return f&#x27;dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}&#x27;

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops

</code></pre><h3>SwinTransformer</h3><p>SwinTransformerBlock：</p><pre><code class="language-python">class SwinTransformerBlock(nn.Module):
    r&quot;&quot;&quot; Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    &quot;&quot;&quot;

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) &lt;= self.window_size:
            # if window size is larger than input resolution, we don&#x27;t partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 &lt;= self.shift_size &lt; self.window_size, &quot;shift_size must in 0-window_size&quot;

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(
            drop_path) if drop_path &gt; 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)

        if self.shift_size &gt; 0:  # shift_size表示使用SW-MSA
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            # nW, window_size, window_size, 1
            mask_windows = window_partition(img_mask, self.window_size)
            mask_windows = mask_windows.view(-1,
                                             self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(
                attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer(&quot;attn_mask&quot;, attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, &quot;input feature has wrong size&quot;

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size &gt; 0:
            shifted_x = torch.roll(
                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows
        # nW*B, window_size, window_size, C
        x_windows = window_partition(shifted_x, self.window_size)
        # nW*B, window_size*window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        # W-MSA/SW-MSA
        # nW*B, window_size*window_size, C
        attn_windows = self.attn(x_windows, mask=self.attn_mask)

        # merge windows
        attn_windows = attn_windows.view(-1,
                                         self.window_size, self.window_size, C)
        shifted_x = window_reverse(
            attn_windows, self.window_size, H, W)  # B H&#x27; W&#x27; C ，还原

        # reverse cyclic shift
        if self.shift_size &gt; 0:
            x = torch.roll(shifted_x, shifts=(
                self.shift_size, self.shift_size), dims=(1, 2))  # 使用torch.roll实现shift
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x
</code></pre><p>stage：</p><p>下面的代码用来实现一个stage，每个stage中的MSA部分包含偶数个Swin Transformer Block</p><pre><code class="language-python">class BasicLayer(nn.Module):
    &quot;&quot;&quot; A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    &quot;&quot;&quot;

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        # 偶数层使用Shift，奇数层不用
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (
                                     i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(
                                     drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(
                input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x
</code></pre><p>Swin Transformer：</p><p>主干以及head，将多个stage组合起来，但是对分割似乎不太友好，因为Swin Transformer只有下采样，上采样过程需要使用CNN的方法自行实现。</p><pre><code class="language-python">class SwinTransformer(nn.Module):
    r&quot;&quot;&quot; Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    &quot;&quot;&quot;

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=4, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(
                torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate,
                                                sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(
                                   depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (
                                   i_layer &lt; self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(
            self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {&#x27;absolute_pos_embed&#x27;}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {&#x27;relative_position_bias_table&#x27;}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

</code></pre><h3>downsample</h3><p>这里的下采样采用的是空间到深度的转换：</p><pre><code class="language-python">class PatchMerging(nn.Module):
    r&quot;&quot;&quot; Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    &quot;&quot;&quot;

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        &quot;&quot;&quot;
        x: B, H*W, C
        &quot;&quot;&quot;
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, &quot;input feature has wrong size&quot;
        assert H % 2 == 0 and W % 2 == 0, f&quot;x size ({H}*{W}) are not even.&quot;

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x
</code></pre><h3>其他</h3><p>PatchEmbed：</p><p>对输入图像使用PatchEmbed生成token表示：</p><pre><code class="language-python">class PatchEmbed(nn.Module):
    r&quot;&quot;&quot; Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    &quot;&quot;&quot;

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] //
                              patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim,
                              kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f&quot;Input image size ({H}*{W}) doesn&#x27;t match model ({self.img_size[0]}*{self.img_size[1]}).&quot;
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x
</code></pre><h2>通用型的主干网络需要什么？</h2><p>本文旨在使用Transformer构建一个通用的主干网络，那么一个通用的主干网络需要什么呢？</p><ol><li>轻量</li><li>强大的特征提取能力</li><li>多尺度</li></ol><h2>local vision transformer</h2><p>众所周知，Transformer是一种自注意力，而自注意力的关键就是计算全局中所有token之间的关系，这似乎与local有很大的矛盾。</p><p>最近越来越多的工作对local vision transformer进行研究，其实际上是一种local attention，比如之前的VOLO outlooker attention，其优点主要在于计算复杂度低，相较于Transformer的全局粗略建模能够更精细地在局部进行建模（VOLO的观点），但是其局部的关注与Transformer是相悖的，因此提出了各种Cross Window的信息交流方式：</p><p>比如本文的Shift Windows，美团Twins的local attention和global attention结合，华为MSG-Transformer使用的信使token，交大GG-Transformer使用的AdaptivelyDilatedSplitting使用Dilate的思想来从全局采集Window（类似于shuffle加上从深度到空间的转换），腾讯的Shuffle Transformer（与GG-Transformer类似）等，以及之前的Recurrent Criss-Cross Attention，其利用横纵轴上信息计算全局注意力，或是类似于RCCA模块的CSWin Transformer。</p><p>这些都是local attention，但是通过不同的方法增强了其全局建模的能力，具体原因可能是因为local attention的<strong>稀疏连接性</strong>，这也是VOLO的思想所在，并且除了上述网络，也在很多网络中得以体现，比如ECANet针对SENet的改进，其使用一维卷积获得注意力权重，但是取得了更好的效果。</p><p>关于这点将在<a href="file:///D:/UserData/Desktop/%E8%AE%BA%E6%96%87/Attention%20or%20Conv/2106.04263.pdf">Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight</a>进行讨论——Local vision transformer work的原因究竟是什么？</p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demystifying Local Vision Transformer]]></title>
        <id>Demystifying Local Vision Transformer</id>
        <link href="https://ml.akasaki.space/blog/[46]Demystifying-Local-Vision-Transformer"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[从稀疏连接性、权重共享、动态权重进一步探究Local Attention。]]></summary>
        <content type="html"><![CDATA[<p>从稀疏连接性、权重共享、动态权重进一步探究Local Attention。</p><blockquote><p>论文名称：Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight</p><p>作者：Qi Han1，Zejia Fan，Qi Dai，Lei Sun，Ming-Ming Cheng，Jiaying Liu，Jingdong Wang</p><p>Code：<a href="https://github.com/Atten4Vis/DemystifyLocalViT/">https://github.com/Atten4Vis/DemystifyLocalViT/</a></p></blockquote><h2>介绍</h2><p>本文的主要成果发现（finding）如下：</p><ol><li><p>Local Transformer采用的Local Attention利用了现有的正则化方案（regularization schemes）、稀疏连接（sparse connectivity ）、权重共享（weight sharing）以及动态权重预测（dynamic weight prediction），在不需要额外增加模型复杂度和训练数据的情况下增加性能；</p></li><li><p>局部注意力（Local Attention）与（动态）深度卷积（(dynamic )depth-wise convolution）在稀疏连接性上<strong>相似</strong>，在权重共享和动态权重预测上不同。</p><p>实验结果表明，局部注意力和（动态）深度卷积所采用的正则化形式和动态权重预测方案具有<strong>相似</strong>的性能。</p></li><li><p>此外，提出了一个关系图来联系卷积和注意力，同时开发了基于MLP的方法。</p><p>关系图表明，这些方法本质上利用了不同的稀疏连接和权重共享模式，可以选择使用动态权重预测进行模型正则化。</p></li></ol><h2>Understanding Local Attention</h2><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110241956165.png" alt="image-20211024150706152"/><p>上图分别表示了(a)convolution,(b) global attention and spatial mixing MLP,(c) local attention and depth-wise convolution, (d) point-wise MLP or $1×1$ convolution, (e) MLP (fully-connected layer)的连接模式。</p><p>接下来将对其进行介绍——</p><h3>Sparse Connectivity, Weight Sharing, and Dynamic Weight</h3><p>本节将简要介绍两种正则化形式：稀疏连接和权重共享，以及动态权重，和它们的优点，并使用这三种形式来分析局部注意力，将其与深度卷积联系起来。</p><p><strong>稀疏连接</strong>：稀疏连接意味着某些输出和某些输入之间并没有联系，它在不减少神经元的情况下，降低了模型的复杂度。</p><p><strong>权重共享</strong>：权重共享表示某些连接权重相等，它减少了模型的参数量，增加了网络的大小，并且不需要增加相应的训练数据。</p><p><strong>动态权重</strong>：动态权重指的是为每个实例学习特定的连接权重，它旨在增加模型表达能力，如果将学习的连接权重视为隐藏变量，则可以将动态权重视为引入二阶操作（该观点在Involution和VOLO中都有所表现，将在后面进行讨论），从而提高网络的能力。</p><h3>Local Attention</h3><p>Vision Transformer通过重复注意力层和后续的前馈层形成一个网络，而Local Vision Transformer，采用局部注意力层，将空间划分为一组小窗口，在每个窗口内同时计算自注意力，以提高内存和计算效率。</p><p>多头局部自注意力最终可写作如下形式：
$$
y<em>i =[y</em>{i1}^Ty<em>{i2}^Ty</em>{i3}^T\cdots y<em>{iM}^T]^T
<!-- -->\<!-- -->y</em>{im}=\sum<em>{j=1}^{N_k}a</em>{ijm}X_{ijm}
$$
其中i表示当前位置，j表示整个窗口中的所有位置，$N_k$即表示一个窗口中像素的总数（下同）。</p><h3>Properties</h3><p>Local Attention是一个具有动态权重计算的channel-wise和spatially-locally的连接层。</p><p>其聚集信息的过程可以等价地写成：
$$
y<em>i=\sum</em>{j=1}^{N<em>k}W</em>{ij}\odot X<em>{ij}
$$
$\odot$表示逐元素相乘，$W</em>{ij}$表示权重向量，从$a_{ij}$中获得。</p><p><strong>稀疏连接</strong>：</p><p>Local Attention在空间上是稀疏的，每个位置只与一个小窗口中的其他位置连接，<strong>并且通道间不存在连接</strong>。</p><p>上式中的$\odot$表示给定注意力权重，每个输出的元素，比如$y_{id}$（第d个通道中的第i个位置），是依赖于同个通道同一窗口中的其他输入元素，而与其他通道上的元素无关。</p><p><strong>权重共享</strong>：</p><p>权重在每个通道间共享。</p><p>对于单头注意力，所有$W<em>{ijk}\in W</em>{ij}$都是相同的，$W<em>{ijk}=a</em>{ij},1\leqslant k \leqslant D$；</p><p>对于多头注意力，$W<em>{ij}$被划分为M个子向量，有$M</em>{ijm}=a_{ijm}$.</p><p><strong>动态权重</strong>：</p><p>权重$<!-- -->{<!-- -->W<em>{i1},W</em>{i2},\cdots,W<em>{iN_k}<!-- -->}<!-- -->$是动态地从查询向量$q_i$和窗口中的键向量$<!-- -->{<!-- -->k</em>{i1},k<em>{i2},\cdots,k</em>{iN<em>k}<!-- -->}<!-- -->$生成的（点积和Softmax），可以写成如下形式：
$$
<!-- -->{<!-- -->W</em>{i1},W<em>{i2},\cdots,W</em>{iN<em>k}<!-- -->}<!-- -->=f(q_i;{k</em>{i1},k<em>{i2},\cdots,k</em>{iN_k}})
$$
<strong>每个权重都可能包含所有通道的信息，并且充当跨通道信息交流的桥梁</strong>，因为其在通道间共享权重，所以每个权重都可能学习到这些通道的信息，这一定程度上起到了跨通道交流的作用</p><p><strong>集合表示（Set representation）</strong>：</p><p>每个query对应的key和value被表示为一个集合，这就导致其存在的位置关系没有被利用，但是这可以被<strong>位置嵌入</strong>，或是<strong>学习</strong>一个相对位置嵌入所弥补。</p><h3>Connection to Depth-Wise Convolution</h3><p>深度卷积对每个通道都使用一个单独的卷积核，输出时并不会求和，是分组卷积的极致表示，但是这意味着通道之间没有任何信息交流，可以写成如下形式：
$$
y<em>i=\sum</em>{j=1}^{N<em>k}W</em>{offset(i,j)}\odot X_{ij}
$$
将Local Attention和深度卷积进行对比：</p><p><strong>相似性</strong>：二者在稀疏连接上具有相似性：No Connection Across Channels，每个位置仅与同通道上的窗口其他位置相连接。</p><p><strong>差异性</strong>：</p><ol><li><p>权重共享：深度卷积只在空间上共享权重，而Local Attention跨通道共享权重；</p></li><li><p>连接权重的性质：对于深度卷积来说，连接权重是<strong>静态</strong>的，被学习表示为模型的参数，而对于Local Attention，其连接权重是<strong>动态</strong>的，其为每一个实例（像素）单独生成；</p><p>深度卷积也可以从动态权重中受益——主要分为两种，一是学习一致的动态权重，二是为每个实例动态生成权重，将在后面进行介绍；</p></li><li><p>窗口表示：深度卷积天然地保留了位置信息，而Local Attention则需要使用位置嵌入来弥补位置信息的丢失。</p></li></ol><h3>关系图</h3><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110241956844.png" alt="image-20211024150634894"/><p>提出了一个关系图，其包含了三种正则化方法——稀疏连接、动态权重和低秩。</p><ol><li>MLP在空间上稀疏连接便得到了标准卷积：<ol><li>在空间上低秩化便能得到金字塔、多尺度卷积；</li><li>在通道上低秩化便得到了Bottleneck；</li><li>在通道上稀疏连接又能得到深度可分离卷积；</li></ol></li><li>MLP在维度上稀疏连接便得到了separable MLP：<ol><li>在空间上稀疏连接便能得到深度可分离卷积；</li><li>在空间上实现动态权重得到ViT：<ol><li>在空间上稀疏连接便得到Local ViT；</li><li>在空间上低秩化便得到PVT；</li></ol></li></ol></li></ol><h2>Experimental Study</h2><p>与Swin Transformer进行对比，在ImageNet图像分类，COCO目标检测和ADE语义分割上进行了实验。</p><h3>Architectures</h3><p>将Swin-T和Swin-B中的Local Attention替换为了深度卷积，所有的线性映射层都替换为$1\times 1$卷积层，同时建立了动态权重版本的深度卷积，该版本使用类似于SENet的技术生成一致动态权重。</p><h3>Datasets and Implementation Details</h3><p>看论文</p><h3>Main Results</h3><p><strong>ImageNet classification</strong>：深度卷积版本的参数量和计算量都下降了约15%，动态版本的参数量大量上升，但复杂度几乎相同，性能几乎持平，同时比较了其他方法的性能</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110241957911.png" alt="image-20211024160713613"/><p><strong>COCO object detection</strong>、<strong>ADE Semantic Segmentation</strong>：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110241957276.png" alt="image-20211024161120185"/><h3>Additional Studies</h3><p><strong>Weight sharing</strong>：研究了在通道间共享权重对深度卷积和Swin Transformer的影响</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110241957956.png" alt="image-20211024162356897"/><p><strong>Dynamic weight</strong>：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110241957433.png" alt="image-20211024162523601"/><p><strong>Cooperating with SE</strong>：SE是一个参数和计算效率较高的动态模块，DW可以从中受益，但是本身已是动态模块的Swin出现了掉点的情况。</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110241957683.png" alt="image-20211024162949608"/><p>更多看论文</p><h2>Dynamic Weight</h2><p>动态权重主要有两种类型——学习一致的权重（homogeneous connection weight），为每个位置或者区域生成不同的权重。</p><p>第一种学习一致的权重，比如<a href="https://asthestarsfalll.icu/2021/08/01/Dynamic-conv/">Dynamic Convolution</a>，或者最经典的SENet，实际上也是一种动态权重。</p><p>其主要特点是对于每一个输入（特征图）给出唯一的权重，该权重在空间上共享。</p><p>第二种为每个实例（位置、区域）生成不同的权重，比如GENet、<a href="https://asthestarsfalll.icu/2021/05/07/Involuiton/">Involution</a>、<a href="https://asthestarsfalll.icu/2021/07/23/VOLO/">VOLO</a>以及Vision Transformer。</p><p>其主要特点是对于每一个输入（某个位置或者区域）生成唯一的权重，该权重仅在该位置或者区域生效。</p><p>本文采用了第一种权重生成方法，</p><pre><code class="language-python">class DynamicDWConv(nn.Module):
    def __init__(self, dim, kernel_size, bias=True, stride=1, padding=1, groups=1, reduction=4):
        super().__init__()
        self.dim = dim
        self.kernel_size = kernel_size
        self.stride = stride 
        self.padding = padding 
        self.groups = groups 

        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.conv1 = nn.Conv2d(dim, dim // reduction, 1, bias=False)
        self.bn = build_norm_layer(norm_cfg_global, dim // reduction)[1]
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(dim // reduction, dim * kernel_size * kernel_size, 1)
        if bias:
            self.bias = nn.Parameter(torch.zeros(dim))
        else:
            self.bias = None

    def forward(self, x):
        b, c, h, w = x.shape
        weight = self.conv2(self.relu(self.bn(self.conv1(self.pool(x)))))
        weight = weight.view(b * self.dim, 1, self.kernel_size, self.kernel_size)
        x = F.conv2d(x.reshape(1, -1, h, w), weight, self.bias.repeat(b), stride=self.stride, padding=self.padding, groups=b * self.groups)
        x = x.view(b, c, x.shape[-2], x.shape[-1])
        return x
</code></pre><h2>VOLO：OutLook Attention</h2><p>VOLO也是一种Local Attention，并且性能表现优异，将其中的Local Attention的部分分别替换为self attention和深度卷积，可得到如下结果：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/202110242034232.png" alt="image-20211024203433183"/><p>同时替换了了SVT中的Local Attention部分，结果都是有所增加的。</p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation]]></title>
        <id>DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[论文名称：DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_DCT-Mask_Discrete_Cosine_Transform_Mask_Representation_for_Instance_Segmentation_CVPR_2021_paper.pdf">DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation</a></p><p>作者：Xing Shen, Jirui Yang, Chunbo Wei, Bing Deng, Jianqiang Huang, Xiansheng Hua, Xiaoliang Cheng, Kewei Liang</p><p>仓库地址：<a href="https://github.com/calmevtime/DCTNet">https://github.com/calmevtime/DCTNet</a></p></blockquote><h2>摘要</h2><p>$Binary\; grid\; mask$ 广泛用于实例分割。就例如 $Mask\ R-CNN$<a href="#references"><sup>1</sup></a>，如下图所示，网络在 $28\times 28$ 的网格中预测 $Mask$ 。</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/mask.jpg"/></p><p>但是一般来说，低分辨率的网格不足以捕捉细节，而高分辨率会大大增加训练的复杂性，为解决此问题，这篇论文提出一种新的 $Mask$ 表达方式，利用离散余弦变换（$DCT$）将高分辨率的$Binary\; grid\; mask$编码成一个紧凑的向量，这种方法称为 $DCT-Mask$。</p><p>该方法可以非常容易集成到大多数基于像素的实例分割上。它不需要任何预处理或预训练，而且几乎对速度没有损害。</p><h2>介绍</h2><p>就如上图所示，$Mask\ R-CNN$ 将 $GT$ 采样到 $28\times 28$ ，然后上采样重构它，如下图所示，低分辨率的 $Binary\; grid\; mask$ 不足以捕获细节特征，并在上采样过程中产生偏差。</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/vs.jpg"/></p><p>如上图为使用 $DCT$ 和未使用 $DCT$ 方法的比较，左边为 $GT$ ；之后是 $Resize$ 后的 $GT$ ；再是基于 $Resize$ 后的重建图；最后是重建图与原来的$GT$图的误差值。</p><p>所以就算预测 $Mask$ 是正确的，重建的 $Mask$ 也有一定的系统误差。解决方式之一是提高 $Binary\; grid\; mask$ 的分辨率，但是实验显示提高分辨率后平均精度（$AP$）比 $28\times 28$ 要差，具体见下图。</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/mask_size.jpg"/></p><h2>Method</h2><p>作者给出的方法是 $DCT\ mask$ ，如下图是该 $DCT\ mask$ 的 $pipline$。</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/pipline.jpg"/></p><p>该处理方式是受 $JPEG$ 标准的启发，$pipline$ 将二进制掩码转化为紧凑的向量。首先将 $GT\ Resize$到 $K\times K$ 大小，然后对其进行二维 $DCT-II$ (假装是罗马 2)变换，在重构时利用二维逆 $DCT$ 变换，最后利用双线性插值 $Resize$ 到 $H\times W$。数学表达如下（先看<a href="#%E7%A6%BB%E6%95%A3%E4%BD%99%E5%BC%A6%E5%8F%98%E6%8D%A2dct">离散余弦变换</a>）：</p><p>设 $Binary\; grid\; mask\; M<em>{gt}\in\ R^{H\times W}$。$Resize$ 到$M</em>{K\times K}\in\ R^{K\times K}$。文中$K=128$。二维$DCT$变换$M_{DCT}\in\ R^{K\times K}$ 频率信号由如下公式得到：</p><p>$$
M<em>{DCT}(u, v)=\frac{2}{K}C(u)C(v)\sum</em>{x=0}^{K-1} \sum<em>{y=0}^{K-1} M</em>{K \times K}(x, y) \cos \frac{(2 x+1) u \pi}{2 K} \cos \frac{(2 y+1) v \pi}{2 K}
$$</p><p>这里 $C(\omega)=1/\sqrt{2}$ 当 $\omega=0$ 时当 $\omega$ 等于其他值时 $C(\omega)=1$</p><p>因为 $DCT$ 具有很强的能量聚集性，所以可以从 $M_{DCT}$ 经过 $zig-zag$ 编码后得到向量选择第一个 $N$ 维度的向量 $V\in\ R^{N}$ (为什么是$select\; the\; first\; N-dimensional\; vector?$)</p><p>之后对该向量补零重构得到 $\bar{M}_{DCT}\in\ R^{K\times K}$，下一步利用二维逆 $DCT$ 变换</p><p>$$
\bar{M}<em>{K \times K}(x, y)=\frac{2}{K} \sum</em>{u=0}^{K-1} \sum<em>{v=0}^{K-1} C(u) C(v) \bar{M}</em>{D C T}(u, v) \cos \frac{(2 x+1) u \pi}{2 K} \cos \frac{(2 y+1) v \pi}{2 K}
$$</p><h2>DCT-Mask in Mask R-CNN</h2><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/net.jpg"/></p><p>如上图 $DCT-Mask$ 在$Mask\ R-CNN$ 的应用，在$Mask\; head$ 中使用 4 个卷积层，提取$Mask$ 特征，然后用三个线性归回层回归$DCT$向量</p><p>则实际上变为回归问题，损失函数可构建为</p><p>$$
\mathcal{L}<em>{mask}=1^{obj}\sum</em>{i}^{N}D(\hat{V}<em>{i},V</em>{i})
$$</p><p>这里 $V<em>{i},\hat{V}</em>{i}$ 分别表示为第$i$个元素的$GT$与预测值。$1^{obj}$ 是样本中正样本指示函数，$D$ 是第一范数距离矩阵。</p><p>如下图为对$N$的取值的探究</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/exc.jpg"/></p><p>其中$None$表示为使用的二进制掩码。</p><h2>效果</h2><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/than.jpg"/></p><h2>Zig-Zag 编码</h2><p>下图为$Zig-Zag$ 编码方式</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/zig-zag.jpg"/></p><h2>离散余弦变换 DCT</h2><p>$DCT$ 变换的全称是离散余弦变换（$Discrete\; Cosine\; Transform$），主要用于将数据或图像的压缩，能够将空域的信号转换到频域上，具有良好的去相关性的性能。</p><p>在详细说明 $DCT$ 公式之前需要对 $DFT$ 有所了解。</p><p>$$
X<!-- -->[k]<!-- --> = \sum_{n=0}^{N-1}x<!-- -->[n]<!-- --> \left(cos \left( \frac{2\pi k n}{N}\right)-jsin \left( \frac{2\pi k n}{N}\right)\right)
$$</p><p>将上面式子拆开来</p><p>$$
X<!-- -->[k]<!-- -->=\sum<em>{n=0}^{N-1} x<!-- -->[n]<!-- -->\left(\cos \frac{2 \pi \mathrm{kn}}{N}\right)-j \sum</em>{n=0}^{N-1} x<!-- -->[n]<!-- --> \sin \left(\frac{2 \pi k n}{N}\right)
$$</p><p>可以看到 $DFT$ 变化结果，实数部分由$\displaystyle\sum<em>{n=0}^{N-1} x<!-- -->[n]<!-- -->\left(\cos \frac{2 \pi \mathrm{kn}}{N}\right)$ 组成，而虚数部分由$\displaystyle j\sum</em>{n=0}^{N-1} x<!-- -->[n]<!-- --> \sin \left(\frac{2 \pi k n}{N}\right)$组成，设$\displaystyle\cos \left(\frac{2 \pi \mathrm{kn}}{N}\right)=\cos(kt)$，那 $DFT$ 公式可以写为：</p><p>实数部分:</p><p>$$
Re<!-- -->[k]<!-- -->=\sum_{n=0}^{N-1} x<!-- -->[n]<!-- -->\cos(kt)
$$</p><p>虚数部分:</p><p>$$
Im<!-- -->[k]<!-- -->=\sum_{n=0}^{N-1} x<!-- -->[n]<!-- -->\sin(kt)
$$</p><p>显然，$\cos$ 是一个偶函数，$\sin$ 是一个奇函数，因此</p><p>$$
Re<!-- -->[k]<!-- -->=Re<!-- -->[-k]<!-- -->,Im<!-- -->[k]<!-- -->=-Im<!-- -->[k]<!-- -->
$$</p><p>所以当 $x<!-- -->[n]<!-- -->$ 是一个实数函数时，其频率的实部是偶函数，虚部是一个奇函数。</p><p>那当原信号 $x<!-- -->[n]<!-- -->$ 是一个全是实数的偶函数信号，$x<!-- -->[n]<!-- -->\sin{kt}$ 就变成一个奇函数，奇函数那么自然</p><p>$$
Im<!-- -->[k]<!-- -->=\sum_{n=0}^{N-1} x<!-- -->[n]<!-- -->\sin(kt)=0
$$</p><p>因此，当原时域信号是一个实偶信号时，我们就可以把 $DFT$ 写成</p><p>$$
X<!-- -->[k]<!-- -->=\sum_{n=0}^{N-1} x<!-- -->[n]<!-- -->\left(\cos \frac{2 \pi \mathrm{kn}}{N}\right)
$$</p><p>以上就是 $DCT$ 变换的核心思想，当然这与实际的 $DCT$ 公式还是有差距的。</p><p>先来看最常用的 $DCT$ 变换公式</p><p>$$
F(u)=c(u) \sum_{x=0}^{N-1} f(x) \cos \left<!-- -->[\frac{(x+0.5) \pi}{N} u\right]<!-- -->
$$</p><p>其中当 $u=0$ 时 $c(0)=\displaystyle\sqrt{\frac{1}{N}}$ 否则 $c(u)=\displaystyle\sqrt{\frac{2}{N}}$</p><p>可以看到与我们上面推导的内容还是有很大不一样的，这是因为在实际应用中没有刚刚好的实偶函数信号给我们，既然没有，我们就构造一个实信号。</p><p>设一长度为 $N$ 的实数离散信号 ${x<!-- -->[0]<!-- -->,x<!-- -->[1]<!-- -->,\cdots,x<!-- -->[N-1]<!-- -->}$ 。首先，我们先将这个信号长度扩大成原来的两倍，并变成 $2N$ ，定义新信号 $x&#x27;<!-- -->[m]<!-- -->$ 为</p><p>$$
\begin{aligned}
x&#x27;<!-- -->[m]<!-- -->=x<!-- -->[m]<!-- -->(0\le m \le N-1)<!-- -->\<!-- -->
x&#x27;<!-- -->[m]<!-- -->=x<!-- -->[-m-1]<!-- -->(-N\le m\le -1)
\end{aligned}
$$</p><p>可视化一下：</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/th.jpg"/></p><p>其中红色为原始信号，红色为延拓后的信号，这样我们就将一个实信号变成了一个实偶信号，显然信号的区间已经变化为 $<!-- -->[-N,N-1]<!-- -->$</p><p>但是这样插值之后也随之带来问题，这个信号并不关于 $m=0$ 偶对称，所以为了让信号关于原点对称，把整个延拓信号向右平移 $\frac{1}{2}$ 个单位</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/0.5.png"/></p><p>因此上面 $DFT$ 公式变化为</p><p>$$
X<!-- -->[k]<!-- -->=\sum_{m=-N+\frac{1}{2}}^{N-\frac{1}{2}} x^{\prime}\left<!-- -->[m-\frac{1}{2}\right]<!-- --> e^{\frac{-j 2 \pi m k}{2 N}}
$$</p><p>根据欧拉公式对上式展开，展开时我们只要实数部分就行了</p><p>$$
X<!-- -->[k]<!-- -->=\sum<em>{m=-N+\frac{1}{2}}^{N-\frac{1}{2}} x^{\prime}\left<!-- -->[m-\frac{1}{2}\right]<!-- --> e^{\frac{-j 2 \pi m k}{2 N}}=\sum</em>{m=-N+\frac{1}{2}}^{N-\frac{1}{2}} x^{\prime}\left<!-- -->[m-\frac{1}{2}\right]<!-- --> \cos \left(\frac{2 \pi m k}{2 N}\right)
$$</p><p>但是这样是不科学的，因为$m$是带小数甚至负数的，因为在离散信号中找不到这样的信号。因此我们需要变形，我们知道这个序列是偶对称序列，因此</p><p>$$
\sum<em>{m=-N+\frac{1}{2}}^{N-\frac{1}{2}} x^{\prime}\left<!-- -->[m-\frac{1}{2}\right]<!-- --> \cos \left(\frac{2 \pi m k}{2 N}\right)=2 \times  \sum</em>{m=\frac{1}{2}}^{N-\frac{1}{2}} x^{\prime}\left<!-- -->[m-\frac{1}{2}\right]<!-- --> \cos \left(\frac{2 \pi m k}{2 N}\right)
$$</p><p>于是设$\displaystyle n=m-\frac{1}{2}$，代入上式</p><p>$$
2 \times  \sum<em>{n=0}^{N-1} x^{\prime}<!-- -->[n]<!-- --> \cos \left(\frac{2 \pi\left(n+\frac{1}{2}\right) k}{2 N}\right)=2 \times  \sum</em>{n=0}^{N-1} x^{\prime}<!-- -->[n]<!-- --> \cos \left(\frac{\left(n+\frac{1}{2}\right) \pi k}{N}\right)
$$</p><p>关于 $DCT$ 中 $c(u)$ 是怎么来的，$c(u)$ 在函数计算中，加不加都无所谓，但实际上，这个值因为一些工程上的意义，在 $DFT$ 中也常常出现$\frac{1}{N}$ 这主要是为了在 $DFT$ 变换变成矩阵运算的形式时，将该矩阵正交化，所以这里的$c(u)$也同样。$c(u)=\displaystyle \sqrt{\frac{1}{2N}}$ 将该系数乘入上面式子</p><p>$$
\sqrt{\frac{1}{2 N}} \times  2 \times  \sum<em>{n=0}^{N-1} x^{\prime}<!-- -->[n]<!-- --> \cos \left(\frac{\left(n+\frac{1}{2}\right) \pi k}{N}\right)=\sqrt{\frac{2}{N}} \times  \sum</em>{n=0}^{N-1} x^{\prime}<!-- -->[n]<!-- --> \cos \left(\frac{\left(n+\frac{1}{2}\right) \pi k}{N}\right)
$$</p><p>于是我们便得到 $DCT$ 式子</p><h2>分析能量聚集性</h2><p>上面推导了 $DCT$ 公式，这里尝试对其能量聚集性进行解释。</p><p><img src="./src/Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation/dct.jpg"/></p><p>回想我们如何得到傅里叶变换公式，我们先对原信号进行<strong>周期</strong>延拓，而在$DCT$中我们先对信号进行<strong>镜像</strong>延拓，如上面的图可以看出$DFT$直接进行周期变换会造成跳变，对应与频率里的高频。而$DCT$对信号进行镜像，其过度更加平滑，同时会弱化高频信号（高频决定细节，低频决定轮廓）。而根本原因是对一般的周期函数展开成 fourier 级数的性质问题，这里不在深入探究。</p><h2>References</h2><ul><li>[1][mask r-cnn]<!-- -->(<a href="https://arxiv.org/pdf/1703.06870.pdf">https://arxiv.org/pdf/1703.06870.pdf</a>)</li></ul>]]></content>
        <author>
            <name>PuQing</name>
            <uri>https://github.com/AndPuQing</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Retinex Decomposition for Low-Light Enhancement]]></title>
        <id>Deep Retinex Decomposition for Low-Light Enhancement</id>
        <link href="https://ml.akasaki.space/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[该论文提出时间2018.7.20]]></summary>
        <content type="html"><![CDATA[<blockquote><p>该论文提出时间2018.7.20</p></blockquote><h2>大纲</h2><blockquote><p>对于Retinex方法，其为一种有效的低照度增强方法，它将观察到的图像分解为反射率和照度</p></blockquote><p>大多数现有的Retinex方法都需要花费大量精力去设置分解的参数，以达到较好的效果，但是这样在实际场景中效果较差，而在这篇论文中，作者收集了一个低照度与正常光对比的低光数据集并基于该数据集的学习提出了一个Deep Retinex-net </p><blockquote><p>Deep Retinex-net其中包括了一个 Decom-Net 用于分解 以及一个 Enhance-Net用于照度调节</p></blockquote><blockquote><p>Decom-Net：（分解）在训练过程中<strong>不考虑</strong>分解后反射率和光照的基本事实，而是只学习两个关键的约束条件，低照度到正常图像共享的<strong>一致反射率</strong>以及照明的<strong>平滑度</strong></p></blockquote><blockquote><p>Enhance-Net：（增强）基于分解的基础，进行亮度增强</p></blockquote><p>对于联合去噪，存在对于反射率的去噪操作，而在Retinex-net中是端到端可训练的，因此，对于分解的学习过程有助于亮度调整。</p><p>经过大量实验表明，作者的方法在视觉上的弱光增强获得了令人满意的效果，并且拥有图像分解的良好表现</p><h2>正文</h2><h3>1.低照度图像的影响</h3><p>​    低照度图片下，会显著降低图像的可见性，丢失的细节和以及低对比度首先会对我们造成不愉快的视觉影响，而对于正常的计算机视觉系统的，照明不足的图片本身就不适用于正常的视觉系统，最终导致大量的性能损失。</p><h3>2.引入低照度图像增强</h3><p>​    为了使隐藏的细节可见，出现了大量的图像增强方法，以提高低照度图片的主观及客观质量。如直方图均衡化（HE）以及它的变体约束输出图像的直方图以满足某些约束，还有利用低照度图片和朦胧环境中的图像之间逆连接的De-hazing方法</p><blockquote><p>另外一部分的低照度增强方法是基于Retinex的理论基础而提出的，其假设观察到的彩色图像可以被分解为反射率和照度，并以此扩展提出之后的各类经典方法。（1）SSR 作为一种早期的尝试，使用了高斯滤波器将光照图限制为平滑的。（2）MSRCR 通过多尺度高斯滤波器和颜色恢复扩展了SSR，并提出了一种利用亮度级误差测量来保持照明自然性的方法。（3）SRIE 使用加权变分模型同时估计反射率和照度。手动改变照度后，可以恢复目标结果。（4）LIME 只在结构先验下估计光照，并使用反射作为最终的增强结果。也有基于retinx的联合微光增强和噪声去除这两种方法。</p><p>随着深度神经网络的快速发展，CNN在低级别图像处理中得到了广泛的应用，包括super-resolution，rain removal等。Lore等人的使用堆叠稀疏去噪自动编码器来同时进行微光增强和降噪(LLNet)。</p></blockquote><h3>3.Retinex-Net的预计网络框架</h3><p><img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210720205844.png" alt="image-20210720205844362"/></p><blockquote><p>增强过程分为三步:分解、调整和重构。在分解步骤中，子网Decom-Net将输入图像分解为反射率和照度。在接下</p><p>来的调整步骤中，一个基于编码器-解码器的增强网络将照明变亮。引入多尺度拼接，从多尺度角度调整光照。在这个步骤中，反射率上的噪声也被去除。最后，我们重建调整后的照明和反射率，以获得增强的结果</p></blockquote><h3>4. Retinex-Net for Low-Light Enhancement</h3><p>$$
S = R\circ I
$$</p><p>S是源图像，R为反射率，I为照度，$ \circ$为逐元素相乘，其中R（反射率）描述了被捕获物体的内在属性，它被认为在任何亮度条件下都是一致的。照度表示物体上的各种亮度。在弱光图像上，它通常受到黑暗和不平衡照明分布的影响。（所以可以在结构图中发现R的normal和low的图片是一样的）</p><blockquote><p>网络由三个步骤组成:分解、调整和重建。在分解步骤中，视网膜神经网络通过分解神经网络将输入图像分解成图像。它在训练阶段接收成对的弱光/正常光图像，而在测试阶段只接收弱光图像作为输入。在弱光/正常光图像具有相同的反射率和光照平滑度的约束下，Decom-Net学习以数据驱动的方式提取不同光照图像之间的一致R。在调整步骤中，使用增强网来照亮光照图。增强网络采用一个整体的编码器-解码器框架。使用多尺度拼接来保持大区域中照明与上下文信息的全局一致性，同时集中注意力调整局部分布。此外，如果需要，通常在弱光条件下出现的放大噪声将从反射率中去除。然后，我们在重建阶段通过逐元素乘法来组合调整后的照度和反射率。</p></blockquote><h3>5.Data-Driven Image Decomposition</h3><p>对于分解一张图片，有一种方法是使用自己人工设置的限制条件直接在弱光输入图像上估计反射率和照度，但是！因为场景的不确定性，所以不容易设计适合各种场景的约束函数，所以最终作者选择使用了data-driven 的方式来解决分解问题。</p><p>于是，Decom-Net 出现了，<strong>其每次接收成对的弱光/正常光图像，并且在低照度与正常图片共享相同的反射率前提下学习如何分解两者</strong>，且虽然分解是用成对数据训练的，但它可以在测试阶段单独分解低照度输入。在训练过程中，不需要提供反射率和照度的常理条件。只有必要的知识，包括反射率的一致性和光照图的平滑度作为损失函数嵌入到网络中。因此，我们的网络的分解是从成对的低/正常光图像中自动学习的，并且本质上适合于描述不同光条件下图像之间的光变化。</p><p>值得注意的一点：虽然它和分解内在图像很相似，但是本质是不同的，所以在该任务中，我们的主要目的不应该是精确的获得实际的内在图像，而是一个很好的光线调节表示，我们需要的是让网络学习弱光图像与其增强图像之间的一致性成分</p><p>在实际使用中，作者先使用一个3 x 3卷积提取图像特征，而后再使用多个3 x 3卷积加上ReLU激活函数将RGB图像映射为反射和照明，而3 x 3卷积都是从特征空间投影R和I，并且使用sigmoid函数将 R 和 I 都投影在【0 , 1】的范围中</p><p>损失 $L$ 被分为三个部分: Reconstruction loss(重建损失) $L<em>{recon}$ ，Invariable reflectance loss（恒定反射损失）$L</em>{ir}$ , illumination smoothness loss(照明平滑度损失) $L<em>{is}$​ :
$$
L = L</em>{recon} + \lambda<em>{ir}L</em>{ir} +\lambda<em>{is}L</em>{is}
$$</p><blockquote><p>其中$\lambda<em>{is}和$$\lambda</em>{it}$​​是平衡反射率一致性和照度平滑度的系数</p></blockquote><p>基于Reflectance 图像都可以用相应照度映射来重建图像的假设，Reconstruction loss（重建损失）$L<em>{recon}$​被表述为：
$$
L</em>{recon}=\sum<em>{i=low,normal}\sum</em>{j=low,normal}\lambda_{i,j}||R_i\circ I_j-S_j||_1
$$
<img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210722153835.png" alt="image-20210722153835257"/></p><p>Invariable reflectance loss(恒定反射损失) $L<em>{ir}$ 被引入来限制反射率的一致性:
$$
L</em>{ir}=||R<em>{low}-R</em>{normal}||_1
$$</p><h3>6.Structure-Aware Smoothness Loss</h3><p>对于照度映射有一个基本假设是局部一致性和结构感知，换句话说，一个好的照度映射的解决方案应该是纹理细节平滑，同时仍然可以保留整体结构边界。</p><p>Total variation minimization (TV) 变差化最小化，其常常被用于最小化整个图像的梯度，经常被用作各种图像恢复任务的平滑度先验，然而，直接使用 TV 作为损失函数在图像具有强结构或亮度剧烈变化的区域会失败。这是由于无论该区域是文本细节区域还是强边界区域，光照图的梯度都是均匀减少的。换句话说，电视损失就是结构盲区化。照度模糊化，且会将顽固的黑色边缘留在反射图中</p><p>为了使得损失了解到图像结构，原始的TV函数应该被反射图映射加权，且最终的$L_{is}$应该被表述为</p><p>$$
L<em>{is}=\sum</em>{i=low,normal}||\nabla I_i\circ exp(-\lambda_g\nabla R_i)||
$$
限制由初始的照度映射加权，初始光照图是R，G，B通道中每个像素的最大值，在训练阶段我们可以同时更新照度和权重（反射率）</p><h3>7.Multi-Scale Illumination Adjustment</h3><blockquote><p>照明增强网络采用编码器-解码器架构的整体框架。为了从分层的角度调整照明，我们引入了多尺度连接，编码器-解码器体系结构在大区域中获得上下文信息。输入图像被连续下采样到小尺度，在该尺度下网络可以获得大尺度照明分布的视角。这给网络带来了自适应调整的能力，而后，而后利用大尺度下的光照学习，上采样重建局部光照特征。通过元素求和，从下采样块到其对应的镜像上采样块引入跳跃连接，强制网络学习残差。</p></blockquote><p>为了分层调整光照，即保持全局光照的一致性，同时适应不同的局部光照分布，引入的多尺度拼接，如果有 M 个递增的上采样块，每个上采样块提取一个拥有 C 个通道的特征图，我们通过最近邻插值调整这些不同比例的特征到最终比例，并将它们连接到一个拥有 C×M 通道的特征图。然后，通过1×1卷积层，级联特征被简化为 C 信道。遵循3×3卷积层来重建照度图</p><p>下采样块由步长为2的卷积层和ReLU组成。在上采样块中，使用了大小调整卷积层。它可以避免形成棋盘模式化的人工产物。调整大小卷积层由最近邻插值操作、步长为1的卷积层和ReLU组成。</p><p>增强函数中的损失 $L$ 由重建损失 $L<em>{recon}$ 和照明平滑度损失 $L</em>{is}$ 组成， $L<em>{recon}$​ 目的是制造正常光 $\hat{S}$ 其公式为
$$
L</em>{recon}=||R<em>{low}\circ\hat{I}-S</em>{normal}||_1
$$</p><h3>8.Denoising on Reflectance</h3><p>在分解过程中，作者对网络施加了若干约束，其中之一就是照度图的结构感知平滑性，当估计的照度图平滑时，图像细节将保留在反射图中，包括增强后的噪声，所以在网络中，我们还需要对反射图进行去噪操作，鉴于暗处噪声在分解过程中会根据亮度强度被放大，所以我们应该使用与光照相关的去噪方法。</p>]]></content>
        <author>
            <name>RuoMengAwA</name>
            <uri>https://github.com/RuoMengAwA</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GhostNet - More Features from Cheap Operations]]></title>
        <id>GhostNet - More Features from Cheap Operations</id>
        <link href="https://ml.akasaki.space/blog/[49]GhostNet-More-Features-from-Cheap-Operations"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[鬼网！]]></summary>
        <content type="html"><![CDATA[<blockquote><p><em>鬼网！</em></p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212350.png" alt="image-20210510203042875"/></p></blockquote><hr/><h1>GhostNet产生原因</h1><blockquote><p><em>mobileNet或者是shuffleNet提出了使用depthwise或者是shuffle等操作，但是引入的1x1卷积依然会产生一定的计算量</em></p><p>为什么1x1卷积依然会产生较大的计算量?看卷积计算量的计算公式$n ∗ h ∗ w ∗ c ∗ k ∗ k $,可以发现，由于c和n都是比较大的，所以会导致这个计算量也是比较大的（后文具体结构复现时还会解释）</p><p>所以，我们如何在这个基础上再减少参数，优化网络速度呢，作者从一个独特的角度，观察了ResNet50第一个残差块输出的特征图，发现有许多输出特征很相似，基本只要进行简单的线性变换就能得到，而不需要进行复杂的非线性变换得到。</p><p>如图：</p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212347.png" alt="image-20210510184255116"/></p><p>以上图片中同色图片可以使用cheap operations进行生成</p><p>所以可以先通过一个非线性变化得到其中一个特征图，针对这个特征图做线性变化，得到原特征图的幽灵特征图。</p><p><em>ps:这里说的非线性卷积操作是卷积-批归一化-非线性激活全套组合，而所谓的线性变换或者廉价操作均指普通卷积，不含批归一化和非线性激活</em></p></blockquote><p>​		 所以，总结其<strong>核心思想</strong>就是：设计一种分阶段的卷积计算模块，在少量的非线性的卷积得到的特征图基础上，再进行一次线性卷积，从而获取更多的特征图，而新得到的特征图，就被叫做之前特征图的‘ghost’，以此来实现消除冗余特征（也可以说是不避免冗余的特征映射，而是使用一种更低成本效益的方式接受它），使得在保持相似的识别性能的同时，降低通用卷积层的计算代价，以获取更加轻量的模型（非线性的操作是 <em>昂贵的</em>，线性操作是 <em>廉价的</em>）（这操作鬼想得到。。。）</p><hr/><h1>Ghost module</h1><blockquote><p><em>鬼模块！</em></p><p><img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210511143636.png" alt="image-20210511143636786"/></p></blockquote><hr/><h2>功能实现</h2><p>图例：（解释Ghost module的大致功能）</p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212343.png" alt="image-20210510210934894"/></p><blockquote><p><strong>如图所示，相较于传统卷积，直接对input进行卷积操作（昂贵的非线性运算），Ghostnet则先进行部分卷积，得到channel较少的特征图，之后再使用这些特征图cheap operation进行廉价的线性运算，得到更多的特征图，最后将不同的特征图concat到一起，组合形成新的output</strong></p></blockquote><hr/><h2>计算</h2><blockquote><p>首先，假设我们输入特征图的尺寸是 $h <em> w </em> c$ ,输出特征图的尺寸是 $h’ <em> w’ </em> n$ ,卷积核大小为 $k * k$</p><p>在cheap operation变换中，我们假设特征图的channel是 <strong>m</strong>，变换的数量 <strong>s</strong> 代表这<strong>m</strong>个通道分别被线性映射几次，最终得到的新的特征图的数量是<strong>n</strong>,那么我们可以得到等式：</p><p>$$
n = m \times s = output_{total}
$$
由于Ghost的变换过程中最后存在一个恒等变换（<strong>Identity——直接将第一步conv生成的特征图作为output的一部分，该过程也属于线性映射</strong>），所以实际有效，生成新特征图的变换数量是<strong>s-1</strong>,所以上式可以得到如下公式：</p><p>$$
m \times ( s − 1 ) = \frac{n}{s\times(s-1)}= output_{new}
$$</p><p>可以看出，ghostnet对m个通道进行分别的卷积，所以使得Ghost模块中的线性映射具有很大的多样性</p></blockquote><hr/><h2>以论文中的公式复现ghostnet module实现过程</h2><p>$$
Y = X*f+b
$$</p><p>对于输入数据	$Y\in R^{c \times h\times w}$  ，卷积层操作如公式1，	$\cal{Y\in R^{h’\times w’\times n}}$ ,为输出的n维特征图，	$\cal{f \in R^{c\times k\times k \times n}}$为该层的卷积核 ，所以该层的计算量为$n \cdot h&#x27; \cdot w&#x27; \cdot c\cdot k \cdot k$ 这个数据量通常成千上万，因为c（输入图片数）和n（输出特征图数）通常会很大 。公式1的参数量与输入和输出的特征图数息息相关，而从图1可以看出中间特征图存在大量冗余，且存在相似的特征(Ghost)，所以完全没必要占用大量计算量来计算这些Ghost</p><hr/><p>$$
Y^{\prime}=X*f
$$</p><p>假设原输出的特征为某些内在特征进行简单的变换得到Ghost，通常这些内在特征数量都很少，并且能通过原始卷积操作公式2获得，	$\cal{Y&#x27;\in R^{h’\times w’\times m}}$为原始卷积输出，$\cal{Y\in R^{c\times k\times k \times m}}$ 为使用的卷积核，$m\leq n$  ,baise直接简化掉</p><p>$$
y<em>{i j}=\Phi</em>{i, j}\left(y<em>{i}^{\prime}\right), \quad \forall i=1, \ldots, m, \quad j=1, \ldots, s
$$
为了获得原来的$n$维特征，对$Y ′$ 的内在特征分别使用一系列简单线性操作来产生$s$维 ghost 特征，$Φ</em>{i,j}$为生成$Y’_i$的$j-th$ ghost特征图的线性变化函数，对每一个卷积核（特征图）进行s-1次的线性变换，最后生成s-1 + 1个特征图（原特征图加ghost特征图）</p><hr/><h2>与主流卷积操作对比</h2><blockquote><ol><li>对比<strong>Mobilenet、Squeezenet和Shufflenet</strong>中大量使用<strong>1 × 1 pointwise</strong>卷积，<strong>Ghost</strong>模块的原始卷积可以自定义卷积核数量</li><li>目前大多数方法都是先做<strong>pointwise</strong>卷积降维，再用<strong>depthwise</strong>卷积进行特征提取，而<strong>Ghost</strong>则是先做原始卷积，再用简单的线性变换来获取更多特征</li><li>目前的方法中处理每个特征图大都使用<strong>depthwise</strong>卷积或shift操作，而Ghost模块使用线性变换，可以有很大的多样性</li><li><strong>Ghost</strong>模块同时使用<strong>identity mapping</strong>来保持原有特征</li></ol></blockquote><hr/><h2>复杂度分析</h2><blockquote><p>在理想情况下 $n\cdot (s-1)$ 次线性运算之间可以拥有不同的形状和参数，但是在线推理会受阻，特别是考虑到cpu和gpu的使用，所以，在论文中，作者建议我们在一个ghostnet模块中使用相同大小线性操作（如3x3，5x5）以便更加有效的实现</p><p>假设Ghost模块包含1个<strong>identity mapping</strong>和 $m\cdot(s-1)$ 个线性变换，每个线性操作核的大小为 $d\times d$ </p><p>以下是ghostnet模块计算量的压缩比例和参数量的压缩比例：（在此处，$d\cdot d$ 与 $k\cdot k$ 有着相似的大小，$n/s$是第一次变化时的输出通道数目）</p><p>$$
\begin{aligned}
r<em>{s} &amp;=\frac{n \cdot h^{\prime} \cdot w^{\prime} \cdot c \cdot k \cdot k}{\frac{n}{s} \cdot h^{\prime} \cdot w^{\prime} \cdot c \cdot k \cdot k+(s-1) \cdot \frac{n}{s} \cdot h^{\prime} \cdot w^{\prime} \cdot d \cdot d} <!-- -->\<!-- -->
&amp;=\frac{c \cdot k \cdot k}{\frac{1}{s} \cdot c \cdot k \cdot k+\frac{s-1}{s} \cdot d \cdot d} \approx \frac{s \cdot c}{s+c-1} \approx s
\end{aligned}
$$
<strong>where $d \times d$ has the similar magnitude as that of $k \times k$, and $s \ll c$. Similarly, the compression ratio can be calculated as</strong>
$$
r</em>{c}=\frac{n \cdot c \cdot k \cdot k}{\frac{n}{s} \cdot c \cdot k \cdot k+(s-1) \cdot \frac{n}{s} \cdot d \cdot d} \approx \frac{s \cdot c}{s+c-1} \approx s,
$$
式子4为理论的加速比公式 ，可以看出相较于普通的卷积，ghostnet的压缩比例都是s，且$s&lt;&lt; c $  所以，可以看出ghostnet的计算量缩减基于自己定义的s参数，且自定义的<strong>s一般远小于c</strong></p><p>式子5为理论的压缩比公式，该式子是利用刚刚提出的Ghost module加速比而生成的</p></blockquote><hr/><h1>Ghost Bottlenecks</h1><blockquote><p><em>鬼脖子！</em></p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212338.png" alt="image-20210511144002644"/></p></blockquote><hr/><h2>功能实现</h2><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212317.png" alt="image-20210511144840545"/></p><blockquote><p>Ghost Bottleneck(G-bneck)与residual block类似，主要是两个Ghost模块堆叠二次形成，<strong>先升维后降维</strong>而不是常见的先降维后升维</p><p>第一个模块作为一个扩展层，增加了通道的数量(增加特征维度)，我们将输出通道数与输入通道数的比值(增大的比例)称为<strong>扩展比 / expansion ration</strong>，</p><p>而第二个模块减少了与快捷路径匹配的通道数量(减少特征维度 / 通道数)，使其与shortcut一致，然后与输入相加，</p><p>之后在这两个Ghost模块的输入和输出之间连接快捷方式。每一层后应用批处理标准化(BN)和ReLU非线性处理，除了没有在第二个Ghost模块后使用ReLU其他都使用了Relu处理，</p><p>G-bneck包含stride=1和stride=2版本，对于stride=2，shortcut路径使用下采样层，并在Ghost模块中间插入stride=2的深度（depthwise）卷积</p><p>在实际使用中，为了加速提高效率，两个Ghost模块之间插入的是点（pointwise）卷积</p></blockquote><hr/><h2>GhostNet 组成</h2><blockquote><p>G-bneck表示Ghost Bottleneck。#exp表示扩展大小。#out表示输出通道的数量。SE表示是否使用SE模块</p></blockquote><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212315.png" alt="image-20210511151351577"/></p><ul><li><p>分析：</p><p>基于MobileNetV3的架构优势，使用Ghost Bottleneck取代了MobileNetV3中的Bottleneck。GhostNet主要由Ghost Bottleneck组成，以Ghost module作为构建块。第一层是标准的卷积层，有16个滤波器，然后是一系列Ghost瓶颈，通道逐渐增加。根据输入特性映射的大小，这些Ghost Bottleneck被分为不同的阶段。所有Ghost Bottleneck都是在stride=1的情况下应用的，除了每个阶段的最后一个Bottleneck是stride=2。最后利用全局平均池化和卷积层将特征映射转换为1280维的特征向量进行最终分类。挤压和激励(SE)模块也应用于一些重影瓶颈中的残余层，如图，相对于MobileNetV3，我们不使用hard-swish 函数（因为其高延迟）。该体系结构提供了一个基本的设计参考，但进一步的超参数调整或基于Ghost module的自动体系结构搜索，以进一步提高性能。	</p></li></ul><hr/><ul><li><p>Width Multiplier</p><p>即使上图中的GhostNet结构已经很高效，但在某些特定场景下仍然需要需要对模型进行调整，可以简单地使用α对每层的维度进行扩缩（在每一层的通道上都乘一个因子$α$），$α$被称为<strong>width multiplier</strong>，使用α的GhostNet表示为GhostNet-α×。模型大小与计算量大约为$a^2$倍，通常，较小的α导致较低的延迟和较低的性能，反之亦然。</p></li><li><p>我的想法：</p><p>GhostNet减少了参数量，会生成的相似特征图，（或许）可以达到更强的泛化性，但是精度还是会有所下降</p><p>既然在ghostnet中精度会下降，那如果使用自带的α（宽度因子）且（α&gt;1）是否可以做到保持甚至提高性能的同时降低延迟</p><p>本身GhostNet是会降低延迟的，而放弃一点降低效果，之后调大宽度因子，或许可以达到这个效果呢</p></li></ul><hr/><h2>GhostNet——Experiments</h2><p><strong>超参数分析</strong></p><p>  Ghost模块有两个超参数，分别是上面说的ｓ，产生ｍ＝ｎ／ｓ固有特征图以及参数d，线性映射中的卷积核ｄｘｄ的大小。</p><p>  这两个参数的影响，是在VGG１６的基础上进行实验的。</p><p>  首先固定ｓ＝２，然后测试ｄ＝｛１，３，５，７｝，图6表3的结果是网络在CIFAR-１０上的表现，可以看出ｄ＝３的表现最好，这是因为１ｘ１的卷积核无法在特征图上引入空间信息，而ｄ为５或者是７导致了过拟合核更大的计算量，因此，作者采取ｄ＝３来进行接下来的有效性和效率的实验。</p><p>  接下来是固定ｄ＝３，然后测试ｓ＝｛２，３，４，５｝。实际上，ｓ直接关系到计算量和网络的表现，因为大的ｓ导致了更大的压缩和加速比例，结果显示，当增加ｓ时，无论是速度还是准确率都出现下降，当ｓ为２时，代表着VGG１６被压缩了两倍，作者提出的方法表现比VGG１６还要好一点，体现出Ghost模块的优越性。</p><hr/><p><strong>与SOTA对比</strong></p><p>  <img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212312.png" alt="image-20210511155841426"/></p><p>  作者将VGG16和ResNet56与SOTA的压缩模型在CIFAR10数据集上进行对比，作者的模型可以在达到更高的准确率的同时，减少网络的推理速度</p><hr/><p><strong>将Ghost生成的特征图可视化</strong></p><p>  <img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210511160945.png" alt="image-20210511160945341"/></p><p>  尽管生成的特征图都是基于同一个特征图产生的，但是它们之间确实有很大的不同，如图。这意味着生成的特征更加的灵活多变，可以满足特定任务的需求。（<strong>这里可以看出Ghost其实使得同一个特征图中不同通道包含了不同的特征信息，增强了模型的表现力</strong>）。</p><hr/><h2>ImageNet上的分类表现</h2><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212256.png" alt="image-20210511163851455"/></p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212404.png" alt="image-20210511163941831"/></p><p><img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210511162523.png" alt="image-20210511162523436"/></p><blockquote><p>使用k = 1，s = 2 ，d = 3 的GhostNet，不同的模型大小使用不同的<em>α</em>值进行调整</p></blockquote><p>作者按照FLOPs的数量级将图7中的表格分为了四类，例如~50，~150，200-300等。从结果中我们可以看到，通常较大的FLOP在这些小型网络中会导致更高的准确性，这表明了它们的有效性。整体而言，GhostNet最轻量且准确率最高，在各种计算复杂度级别上始终优于其他的对比网络，因为GhostNet在利用计算资源生成特征图方面更加有效。</p><hr/><h2>目标检测</h2><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212407.png" alt="image-20210511163620621"/></p><blockquote><p> 在one-stage和two-stage检测算法上，GhostNet能降低大部分计算量，而mAP与其它主干网络差不多		</p></blockquote><p>​	为了进一步评估GhostNet的泛化能力，作者在MS COCO数据集上进行了目标检测实验。 作者将拆分出来的trainval35k作为训练数据，以mAP作为评价的指标。作者采用了 具有特征金字塔网络（FPN）的两阶段Faster R-CNN 和一阶段的RetinaNet 来作为实验的框架，而GhostNet被用来作为特征提取器。 作者使用的预训练模型是在ImageNet上使用SGD训练12个epochs的模型，将输入图像的大小调整为800的短边和不超过1333的长边。 上图展示了检测结果，其中FLOP是使用224×224尺寸的输入图像计算出来的。实验结果表明GhostNet大大降低了计算成本，无论在RetinaNet还是在Faster R-CNN框架上，都可以达到与MobileNetV2和MobileNetV3相同水准的mAP</p><hr/><h1>Conclusion</h1><p>为了减少神经网络的计算消耗，论文提出Ghost模块来构建高效的网络结果。该模块将原始的卷积层分成两部分，先使用更少的卷积核来生成少量内在特征图，然后通过简单的线性变化操作来进一步高效地生成ghost特征图。从实验来看，对比其它模型，GhostNet的压缩效果最好，且准确率保持也很不错，值得学习。</p><hr/><h1>附言</h1><blockquote><p>还有一个很强大的轻量级主干网络，其效果极强，比ghostnet强，只是没有开源，名为MicroNet</p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212409.png" alt="image-20210511164928585"/></p></blockquote><h1>鬼鬼！</h1><blockquote><p><em>鬼鬼！</em></p><p><img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210511145048.png" alt="image-20210511145048076"/></p></blockquote>]]></content>
        <author>
            <name>RuoMengAwA</name>
            <uri>https://github.com/RuoMengAwA</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kindling the Darkness - A Practical Low-light Image Enhancer]]></title>
        <id>Kindling the Darkness - A Practical Low-light Image Enhancer</id>
        <link href="https://ml.akasaki.space/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[研究背景：]]></summary>
        <content type="html"><![CDATA[<h2>研究背景：</h2><p>对于一张低光图像，不仅是暗，而且也会伴随着噪声和颜色失真等多方面的图像功能退化，所以仅仅提高亮度将无可避免的提高人工产生的影响，必然会放大隐藏的伪影</p><hr/><h2>特点：</h2><p>还是从retinex理论中得到的启发，继而将弱光图像分解为光照（<strong>illumination</strong>）和 反射率（<strong>reflectance</strong>）；前者负责亮度调整，后者用于去除降质（噪声，颜色失真）。这样图像分解的好处是让每一个模块可以更好地被正规化/学习</p><p>而对于输入图像，该网络只需要使用两张不同曝光条件下的图像（即使他们是两张弱光图像也可以），而不是弱光图像和真实图像（这样的好处是，很难定义多亮的图像算是真实图像）</p><p>对于严重的视觉缺陷图片也依旧拥有很强的鲁棒性</p><hr/><h2>效果：</h2><p>模型在2080Ti下的训练速度为，处理一张VGA分辨率图片花费的时间不到50ms</p><p>用户可以自由的调节光照水平（暂时没看到在哪体现）</p><p>具体效果展示（实机测试）：</p><p>不同噪度：</p><blockquote><p>高光图像和低光图像对照（不同的）</p></blockquote><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212643.png" alt="image-20210802180913986"/></p><p>可以得出，KinD在多条件下，效果暂时都优于其他低照度优化算法（最主要的是效果真实，相较于其余算法，失真的情况会大大减少（不过现在还有一个KinD++））</p><hr/><h2>解决问题</h2><ul><li>How to effectively estimate the illumination component
from a single image, and flexibly adjust light levels?</li></ul><blockquote><p>如何从单个图像中有效地估计照明分量，并灵活地调整亮度？</p></blockquote><ul><li>How to remove the degradations like noise and color dis-
tortion previously hidden in the darkness after lightening
up dark regions?</li></ul><blockquote><p>如何消除黑暗和因为黑暗而隐藏的色彩失真？</p></blockquote><ul><li>How to train a model without well-defined ground-truth
light conditions for low-light image enhancement by only
looking at two/several different examples?</li></ul><blockquote><p>如何仅通过查看两个/多个不同的示例，在没有明确的ground-truth光条件的情况下训练模型以进行微光图像增强？ </p></blockquote><hr/><h2>差异</h2><p>相较于之前我们所见到的Retinex-net，其在分解图上的观点有所不同，Retinex-net认为在不同照度下，共享一致的反射率，而KinD的理论认为，在不同照度条件下，反射图是不同的，因为弱光图像会在黑暗中退化（或者说越是黑暗退化越严重）这种退化被转移到了反射图中，就造成了信息的丢失，而在（相对）明亮的图中的反射率可以作为退化弱光图像的参考（ground-truth），所以我认为，该网络还学习了反射图的退化（优化点）</p><hr/><h2>网络结构</h2><p> <img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212649.png" alt="image-20210730100054384"/></p><hr/><h2>Illumination Guided Reflectance Restoration（光照引导反射恢复）</h2><p>从数学上来讲，对于一张图片应该被表现为：</p><p>$$
\mathbf{I}=\mathbf{R} \circ \mathbf{L}+\mathbf{E}
$$</p><blockquote><p>其中 E 被表现为退化的部分</p></blockquote><p>经过一些数学的代数方法，可以表示为：</p><p>$$
\mathbf{I}=\mathbf{R} \circ \mathbf{L}+\mathbf{E}=\tilde{\mathbf{R}} \circ \mathbf{L}=(\mathbf{R}+\tilde{\mathbf{E}}) \circ \mathbf{L}=\mathbf{R} \circ \mathbf{L}+\tilde{\mathbf{E}} \circ \mathbf{L}
$$
其中$ \tilde{\mathbf{R}}$表示为被污染（退化）的反射率，$ \tilde{\mathbf{E}}$表示光照解耦的退化</p><blockquote><p><strong>PS</strong>：反射率 <strong>reflectance</strong> 的恢复不能在整个图像上进行均匀处理，而光照图 <strong>illumination</strong> 可以作为一个很好的向导起到指引作用</p><p>不从弱光图像$I$中直接去除噪声$E$的原因：1.失衡问题仍然存在，图像中的细节和噪声混在一起  2.去噪没有合适的参考图像</p></blockquote><hr/><h2>Arbitrary Illumination Manipulation（获得任意光照条件下的图）</h2><p>不同的情况可能需要不同的图，实际系统需要为任意照明操作提供接口，在各个文献中，增强光照条件的三种主要方法是<strong>聚变</strong>、<strong>光级指定</strong>和<strong>伽马校正</strong>。由于固定的融合模式，基于融合的方法缺乏光线调节功能。如果采用<strong>光级指定</strong>，训练数据集必须包含具有目标级别的图像，这限制了其灵活性。对于<strong>伽马校正</strong>，虽然可以通过设置不同的伽马值来实现目标，但它可能无法反映不同光线（曝光）水平之间的关系。本文提倡从<strong>真实数据中学习灵活的映射函数</strong>，该函数允许用户指定任意级别的光照/曝光。</p><hr/><h2>网络内部</h2><h3>Layer Decomposition Net</h3><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212656.png" alt="image-20210802194606413"/></p><p>损失函数的定义（<a href="https://blog.csdn.net/u014546828/article/details/106833284/">借鉴博客</a>）：</p><ul><li><strong>reflectance similarity</strong> ：对于强光图像和弱光图像，二者的反射率是近似相同的（如果不考虑退化的话），因此将反射率正则化的损失函数定义为：</li></ul><p>$$
\mathcal{L}<em>{r s}^{L D}:=\left<!-- -->|<!-- -->\mathbf{R}</em>{l}-\mathbf{R}<em>{h}\right<!-- -->|</em>{2}^{2}
$$</p><ul><li><strong>illumination smoothness</strong> ：前面分析过，光照图像可以用输入图像进行引导，在输入图像强边缘区，光照发生较大变化；在弱边缘区，光照可以认为也是平滑的，因此损失函数定义为:</li></ul><p>$$
\mathcal{L}<em>{i s}^{L D}:=\left<!-- -->|<!-- -->\frac{\nabla \mathbf{L}</em>{l}}{\max \left(\left|\nabla \mathbf{I}<em>{l}\right|, \epsilon\right)}\right<!-- -->|</em>{1}+\left<!-- -->|<!-- -->\frac{\nabla \mathbf{L}<em>{h}}{\max \left(\left|\nabla \mathbf{I}</em>{h}\right|, \epsilon\right)}\right<!-- -->|<!-- -->_{1}
$$</p><ul><li><p>注意到，当$\nabla I$大时（边缘），使得损失函数值很小，此时对$\nabla L$ 的约束较轻；当$\nabla I$较小时（平滑)，使得损失函数值增大，此时要求$\nabla L$必须很小，才能减小损失函数值。这样，光照图像$L$就和输入图像$I$  有一个相关的结构</p></li><li><p><strong>mutual consistency</strong> ：定义为</p></li></ul><p>$$
\mathcal{L}<em>{m c}^{L D}:=<!-- -->|<!-- -->\mathbf{M} \circ \exp (-c \cdot \mathbf{M})<!-- -->|</em>{1}
$$</p><p>$$
\mathbf{M}:=\left|\nabla \mathbf{L}<em>{l}\right|+\left|\nabla \mathbf{L}</em>{h}\right|
$$</p><p>  （这里有点不理解原理，真的不太明白它的意思,大概就是迫使迫使$<!-- -->[L_l,L_h]<!-- -->$其中一个和另一个相近)</p><ul><li><p><strong>reconstruction error</strong> ：即生成的$<!-- -->[R_l,R_h]<!-- -->和<!-- -->[L_l,L_h]<!-- -->$反过来合成的两个新图，应分别 与$<!-- -->[I_l,I_h]<!-- -->$​相似，即 </p></li><li><p>$$
\mathcal{L}<em>{\text {rec }}^{L D}:=\left<!-- -->|<!-- -->\mathbf{I}</em>{l}-\mathbf{R}<em>{l} \circ \mathbf{L}</em>{l}\right<!-- -->|<em>{1}+\left<!-- -->|<!-- -->\mathbf{I}</em>{h}-\mathbf{R}<em>{h} \circ \mathbf{L}</em>{h}\right<!-- -->|<!-- -->_{1}
$$</p></li></ul><p>图层分解网络的结构：</p><ol><li><p><strong>reflectance branch</strong> ：5-layer U-Net + a conv layer + Sigmoid ；</p></li><li><p><strong>illumination branch</strong> ：two （conv+ReLU layers） + a conv layer（级联从 reflectance branch 来的特征图，目的是为了从光照中排除纹理）+ Sigmoid；</p></li></ol><hr/><h3>Reflectance Restoration Net</h3><p><img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210802205648.png" alt="image-20210802205648721"/></p><ol><li><p><strong>网络的原则是</strong>：采用较清晰的反射率作为较杂乱的反射率的参考。</p></li><li><p><strong>损失函数</strong>：</p></li></ol><p>$$
\mathcal{L}^{R R}:=\left<!-- -->|<!-- -->\hat{\mathbf{R}}-\mathbf{R}<em>{h}\right<!-- -->|</em>{2}^{2}-\operatorname{SSIM}\left(\hat{\mathbf{R}}, \mathbf{R}<em>{h}\right)+\left<!-- -->|<!-- -->\nabla \hat{\mathbf{R}}-\nabla \mathbf{R}</em>{h}\right<!-- -->|<!-- -->_{2}^{2}
$$</p><ol start="3"><li><p><strong>网络结构</strong>：U-Net （更多层）</p></li><li><p>需要注意的是，为什么反射率恢复网络还要引入亮度图像，这是因为，前面说过，噪声和颜色失真最主要出现在弱光照的区域，即衰减的分布依赖于照明分布。因此，将光照信息与反射系数降低一起带入恢复网中</p></li><li><p>传统的 BM3D 会使图像出现模糊现象。而本文中恢复的方法，可保持图像的清晰和锐化。</p></li></ol><hr/><h3>Illumination Adjustment Net</h3><p><img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210802210223.png" alt="image-20210802210223832"/></p><ol><li><p><strong>参数$\alpha$​</strong>：由于给定的两个图像是相对强弱的。那么，输出的图像，是以强光图像为目标呢，还是以弱光图像为目标呢？如果用户是想将弱光图像强化，就设置强光图像为目标，反之，以弱光图像为目标。这个操作可以根据用户需求而自己设置。怎么设置呢？就是通过参数$\alpha = mean(L_t/L_s)$​来实现。其中，$L_t$表示目标图像,$L_s$表示原图像（例如，若对弱光图像强化,则$L_t = L_h,L_s=L_t$）。</p></li><li><p><strong>亮度调剂网络结构</strong>： <strong>two （conv+ReLu ）+ one conv + Sigmoid</strong> 。注意到  被扩展为一个特征图，作为网络输入的一部分。</p></li><li><p><strong>亮度调剂损失函数</strong>：</p></li></ol><p>$$
\mathcal{L}^{I A}:=\left<!-- -->|<!-- -->\hat{\mathbf{L}}-\mathbf{L}<em>{t}\right<!-- -->|</em>{2}^{2}+\left<!-- -->|<!-- -->|\nabla \hat{\mathbf{L}}|-\left|\nabla \mathbf{L}<em>{t}\right|\right<!-- -->|</em>{2}^{2}
$$</p><p>​	即输出图像$\hat{L}$应和目标图像相似，且边缘也相似。</p><ol start="4"><li><strong>与$\gamma$​变换的对比</strong>：对比实验包括亮度降低（以弱光图像为目标）和亮度提升（以强光图像为目标）两个方面。为了更清晰说明情况，(f)-(k) 的曲线图给出了各个图像中这三列像素的曲线对比。</li></ol><p><img src="https://gitee.com/ruomengawa/pic-go/raw/master/img/20210802212402.png" alt="image-20210802212401964"/></p><p>从 (f)-(h) 可以看出，对于亮度降低情况中，在相对明亮的区域，KinD 学习的方式在强度上比  变换减少更多，而在黑暗的区域减少较小或与  变换差不多相同。</p><p>从 (i)-(k) 可以看出，对于亮度提升情况中，KinD 方法在相对暗的区域对光的增强小于  变换，而在明亮的区域的光强调整比  变换增加更多或差不多相同。</p><p>总之，KinD 的方法在亮度调节上，比$\gamma$变换得到的亮度对比度更高。</p><ol start="5"><li>作者最后指出，亮度调节可以通过调节$\alpha$实现,$\alpha$是参与网络训练的,$\alpha$被扩展为一个特征图，作为网络输入的一部分。例如，当$L_t=L_h,L_s=L_t$ 设置$\alpha = 2$ ，表示图像的亮度增加 2 倍。</li></ol>]]></content>
        <author>
            <name>RuoMengAwA</name>
            <uri>https://github.com/RuoMengAwA</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How much Position Information Do Convolutional Neural Networks Encode?]]></title>
        <id>How much Position Information Do Convolutional Neural Networks Encode?</id>
        <link href="https://ml.akasaki.space/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[Md Amirul Islam, Sen Jia, Neil D. B. Bruce]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Islam%2C+M+A">Md Amirul Islam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jia%2C+S">Sen Jia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bruce%2C+N+D+B">Neil D. B. Bruce</a></p><blockquote><p>In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.</p></blockquote><p><code>Comments</code>: Accepted to ICLR 2020</p><h2>引言</h2><p>经典CNN模型被认为是<code>spatially-agnostic</code>的，因此胶囊网络或循环网络已被用于建模学习特征层内的相对空间关系。目前尚不清楚CNN是否捕获了在位置相关任务中重要的绝对空间信息（例如语义分割和显著对象检测）。如下图所示，被确定为最显著的区域倾向于靠近图像中心。在裁剪过图像上做显著性检测时，即使视觉特征没有改变，最显著的区域也会移动。</p><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220215235439.png"/></p><p>在这篇文中，研究了绝对位置的作用通过执行一系列随机化测试，假设CNN确实可以学习到编码位置信息作为决策线索，从而获得位置信息。实验表明，位置信息是通过常用的填充操作（零填充）隐式学习的。</p><h2>CNN中的位置信息</h2><p>CNN会在早期卷积阶段尝试提取精细的高空间频率细节（例如边缘、纹理、线条），而在编码的最深层，网络产生最丰富的类别特定特征表示。在本文中，提出了一个假设，即位置信息隐式编码在提取的特征映射中，并在从视觉场景中分类、检测或分割对象时起着重要作用。下面给出研究问题的数学定义。</p><p><strong>问题表述：</strong> 给定一个输入图像$I<em>m\in R^{h\times w\times 3}$，目标是预测一个类似梯度的位置信息掩模${\hat{f}}_p\in R^{h\times w}$，其中每个像素值定义了绝对坐标，像素从左到右，从上到下。我们生成类似梯度的掩模$G</em>{pos}\in R^{h\times w}$用于实验中的监督，基本CNN原型网络的权重是固定的。</p><h3>位置编码网络</h3><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216001140.png"/></p><p>位置编码网络（<code>PosENet</code>）由两个关键组件组成：一个前馈卷积编码器网络$f<em>{enc}$和一个简单的位置编码模块$f</em>{pem}$。编码器网络从较浅层到较深层提取不同抽象级别的特征。位置编码模块将来自编码器网络的多尺度特征作为输入，并在最后预测绝对位置信息。</p><p><strong>编码器：</strong> 使用基于<code>ResNet</code>和<code>VGG</code>的体系结构，通过<strong>删除平均池层</strong>和分配类别的层来构建编码器网络$f_{enc}$，即上面灰色的部分，编码器模块由五个特征提取块组成，用（$f^1\vartheta$、$f^2\vartheta$、$f^3\vartheta$、$f^4\vartheta$、$f^5\vartheta$）表示。从标准网络的底层到顶层提取的多尺度特征用（$f^1pos$、$f^2pos$、$f^3pos$、$f^4pos$、$f^5pos$）表示。总结如下：</p><p>$$
f^ipos=f^i\vartheta(\mathbf{W}<em>{a}*\mathcal{I}</em>{m})
$$</p><p>其中$\mathbf{W}<em>{a}$表示被冻结的权重，$*$表示卷积运算。注意，在编码网络时，只有位置编码模块$f</em>{pem}$被训练成专注于提取位置信息，而编码器网络被强制保持其现有权重。</p><p><strong>位置编码模块：</strong> 位置编码模块将$f<em>{enc}$的多尺度特征（$f^1pos$、$f^2pos$、$f^3pos$、$f^4pos$、$f^5pos$）作为输入，并通过转换函数$\mathcal{T}</em>{pos}$生成所需的位置映射${\hat{f}}<em>p$。转换函数$\mathcal{T}</em>{pos}$首先在特征图上应用双线性插值操作，使其具有相同的空间维度，从而生成特征图$f^c<em>{pos}$。然后进行一系列$k\times k$卷积操作。在实验中，我们在$<!-- -->{<!-- -->1,3,5,7<!-- -->}<!-- -->$之间改变$k$的值，并且大多数实验是在位置编码模块$f</em>{pem}$中使用单个卷积层进行的。关键操作可概括如下：</p><p>$$
f^cpos=(f^1pos\oplus \cdots \oplus f^5pos ) \qquad \hat{f}_{p}=(\mathbf{W}^cpos*f^cpos)
$$</p><p>式中，$W^c<em>{pos}$是与转换函数$\mathcal{T}</em>{pos}$相关的可训练权重。</p><p>编码模块的主要目标是验证在分类标签上训练时是否隐式学习位置信息。此外，位置编码模块对隐含的位置信息和梯度状真值掩模之间的关系进行建模。<strong>如果特征图中没有编码位置信息，则输出预计为随机输出，反之亦然（忽略图像内容的任何指导）。</strong></p><h3>合成数据和真值生成</h3><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216003321.png"/></p><p>为了验证网络中位置信息的存在性，我们通过指定一个标准化梯度位置图作为真值来执行随机化测试（<strong>梯度一词来表示像素强度，而不是反向传播中的梯度</strong>）。我们首先在水平（<strong>H</strong>）和垂直（<strong>V</strong>）方向生成类似梯度的遮罩。同样，我们应用高斯滤波器来设计另一种真值图，高斯分布（<strong>G</strong>）。生成这三种模式的关键动机是验证模型是否可以学习一个或两个轴上的绝对位置。此外，我们还创建了两种类型的重复模式，水平和垂直条纹（<strong>HS</strong>，<strong>VS</strong>）。无论其方向如何，多级特征中的位置信息都可能通过编码模块$f_{pem}$的转换进行建模。注意这里设计的梯度真值可以看作是一种随机标签，因为输入图像和真值之间没有位置相关性。由于位置信息的提取与图像内容无关，因此可以选择任何图像数据集。</p><h3>训练网络</h3><p>通过均方差损失，进行监督</p><p>$$
\Delta<em>{\hat{f}_p}=\frac{1}{2n}(x</em>{i}-y<em>{i})^2
$$
其中$x\in R^n$和$y\in R^n$（n表示空间分辨率）分别是矢量化的预测位置图和真值图。$x</em>{i}$和$y<em>{i}$分别是${\hat{f}}_p$和$G^h</em>{pos}$的一个像素。</p><h2>结论</h2><h3>位置信息存在</h3><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216003952.png"/></p><p>上表1所示，<code>PosENet（VGG和ResNet）</code>可以很容易地从预训练的CNN模型中提取位置信息，尤其是基于<code>ResNet</code>的<code>PosENet</code>模型。然而，单独训练<code>PosENet（PosENet）</code>在不同的模式和源图像中获得更低的分数。<strong>这一结果意味着很难单独从输入图像中提取位置信息。PosENet仅在与深度编码器网络耦合时才能提取与真值位置图一致的位置信息</strong>。
如前所述，鉴于与输入的相关性已被忽略，生成的真值图可被视为一种随机化测试。然而，测试集在不同的真值模式上的高性能表明，该模型不是盲目地过度拟合噪声，而是提取真实位置信息。然而，与其他模式相比，我们观察到重复模式（HS和VS）的性能较低，这是由于模型的复杂度，特别是真值和绝对位置之间缺乏相关性（表1的最后两行）。H模式可视为正弦波的四分之一，而条纹模式（HS和VS）可视为正弦波的重复周期，这需要更深入的讨论。</p><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216004257.png"/></p><p>上图显示了跨不同模式的几种体系结构的定性结果。我们可以看到与H、G和HS模式对应的预测和真值位置图之间的相关性，这进一步揭示了这些网络中位置信息的存在。定量和定性结果有力地验证了假设，即位置信息隐式编码在每个体系结构中，没有任何明确的监督。</p><p>此外这里也展示了不同架构的网络对于位置的编码能力不同或者语义内容的先验知识程度不同。</p><h3>分析POSENET</h3><p>在上述中，位置编码网络只用了一层不同大小的卷积层进行实验。什么能影响位置编码网络的性能呢？
（1）改变内核大小在位置编码模块中
（2）添加卷积层的堆栈长度，以从多级特征中提取位置信息。</p><p><strong>堆叠层的影响：</strong>    表1中的实验结果表明存在从对象分类任务中学习到的位置信息。在本实验中，改变了PosENet的设计，以检验是否有可能更准确地提取隐藏位置信息。先前实验（表1）中使用的PosENet只有一个卷积层，其内核大小为3×3。
在这里，我们将一组不同长度的卷积层应用于PosENet，并在表2（a）中报告实验结果。</p><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216005333.png"/></p><p>如表2所示，我们在堆叠多层时将内核大小固定在3×3。在PosENet中应用更多层可以改善所有网络位置信息的读出。一个原因可能是堆叠多个卷积滤波器允许网络具有更大的有效感受野，例如，两个3×3卷积层在空间上等于一个5×5卷积层。另一种可能性是，位置信息的表示方式可能需要一阶以上的推断（？）。</p><p><strong>不同核大小的影响：</strong> 我们仅使用一个具有不同内核大小的卷积层来进一步验证PosENet，并在表2（b）中报告了实验结果。从表2（b）中，可以看到，与较小的内核大小相比，较大的内核大小可能捕获更多的位置信息。这一发现意味着位置信息可能在空间上分布在层内和特征空间中，因为更大的感受野可以更好地解析位置信息。(This finding implies that the position information may be distributed spatially within layers and in feature space as a larger receptive field can better resolve position information.)</p><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216005525.png"/></p><h3>位置信息储存在哪里</h3><p>上述实验表明，位置信息是在预训练的CNN模型中编码的。观察位置信息在各层中的分布也同样有意思。
下面实验分别对提取的特征$f_1pos$、$f^2pos$、$f^3pos$、$f^4pos$、$f^5pos$进行PosENet训练，以检查哪一层编码了更多的位置信息。</p><p><img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216005951.png"/></p><p>如上表所示，与$f^1pos$相比，使用$f^5pos$取得最佳的性能。这可能部分是由于从较深的层相比更浅的层提取了更多的特征图（分别为512和64层）。然而，这很可能表明在网络的最深层中位置信息的编码更强，其中该信息由高级语义共享。同时进一步研究了VGG16的这种效应，其中最上面的两层（$f^4pos$和$f^5pos$）具有相同数量的特征。更有趣的是，$f^5pos$比$f^4pos$获得更好的结果。这一比较表明，更深层次的特征包含更多的位置信息，这验证了顶层视觉特征与全局特征相关联的普遍观点(top level visual featuresare associated with global features.)。</p><h2>位置信息来自哪里</h2><p>零填充广泛应用于卷积层，以保持输入和输出的相同空间尺寸，在水平和垂直两个轴的开始和结束处添加许多零。为了验证这一点，作者做了如下实验。
删除了VGG16中实现的所有填充机制，但仍然使用ImageNet预训练权重初始化模型。请注意，仅使用基于VGG的PosENet执行此实验，因为删除ResNet模型上的填充将导致跳连的大小不一致。
<img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216010613.png"/></p><p>在PosENet中不使用填充的效果。<strong>从表4可以看出，没有零填充的VGG16模型在自然图像上实现的性能远远低于默认设置（padding=1）</strong>。类似地，我们通过应用零填充向PosENet引入位置信息。padding=1的PosENet（在帧周围连接一个零）比原始（padding=0）实现更高的性能。当我们设置padding=2时，位置信息的作用更加明显。<strong>这也验证了之前的实验，该实验表明PosENet无法提取明显的位置信息，因为没有应用填充，并且信息是从预训练的CNN模型编码的</strong>。这就是为什么在之前的实验中没有在PosENet中应用零填充。此外，我们的目标是探索在预训练模型中编码了多少位置信息，而不是直接与PosENet结合。图6示出了，使用高斯模式，零填充编码位置信息的影响。(Fig. 6 illustrates the impact of zero-padding on encoding position information subject to padding using a Gaussian pattern.)</p><h3>one more thing</h3><p>回想一下，位置信息应该是独立于内容的一种信息，即不管输入的图片语义如何，其位置信息都应差不多。但是在表一中表明，图像中的语义会影响位置映射。为了可视化语义的影响，使用以下等式计算内容损失热图：</p><p>$$
\mathcal{L}=\frac{|\mathcal{G}^hpos-\hat{f}^h_p|+|\mathcal{G}^vpos-\hat{f}^v_p|+|\mathcal{G}^gpos-\hat{f}^g_p|}{3}
$$
其中$\hat{f}^h_p$，$\hat{f}^v_p$，$\hat{f}^g_p$，分别是水平、垂直和高斯模式的预测位置图。
<img src="./src/How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode/Pasted-image-20220216013335.png"/>
如图所示，PosENet的热图在各个角落都有较大的内容损失。而VGG和ResNet的损失图更多地与语义内容相关。特别是对于ResNet，对语义内容的深入理解会导致在生成平滑梯度时产生更强的干扰。损失最大的分别是脸、人、猫、飞机和花瓶（从左到右）。这种可视化可以作为显示模型关注哪些区域的替代方法，特别是在ResNet的情况下。</p><h2>总结</h2><p>本文探讨了卷积神经网络中绝对位置信息隐式编码的假设。实验表明，位置信息在很大程度上是可用的。而更大的感受野或位置信息的非线性读出进一步增强了绝对位置的读出。实验还表明，当不存在语义线索时，根据来自语义信息的干扰的实验表明对what（语义特征）和where（绝对位置）进行联合编码，恢复位置信息也是可能的（Experiments also reveal that this recovery is possible when no semantic cues are present and interference from semantic information suggests joint encoding of what (semantic features) and where (absolute position，<code>不太明白</code>）。<strong>结果表明，零填充和边界作为锚定，空间信息从中衍生出来，并随着空间特征提取的发生最终传播到整个图像</strong>(zero padding and borders as an anchor from which spatial information is derived and eventually propagated over the whole image as spatial abstraction occurs )。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Axiomatic Attribution for Deep Networks]]></title>
        <id>Axiomatic Attribution for Deep Networks</id>
        <link href="https://ml.akasaki.space/blog/[52]Axiomatic-Attribution-for-Deep-Networks"/>
        <updated>2022-11-05T07:47:19.664Z</updated>
        <summary type="html"><![CDATA[Mukund Sundararajan, Ankur Taly, Qiqi Yan]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sundararajan%2C+M">Mukund Sundararajan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Taly%2C+A">Ankur Taly</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan%2C+Q">Qiqi Yan</a></p><blockquote><p>We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.</p></blockquote><h2>简介</h2><p>本文介绍了一种神经网络的可视化方法：<code>积分梯度（Integrated Gradients）</code>，是一篇 2016-2017 年间的工作。</p><p>所谓可视化，简单来说就是对于给定的输入 $x$ 以及模型 $F(x)$，想办法指出 $x$ 的哪些分量对模型的预测有较大的影响，或者说 $x$ 各个分量的重要性做个排序，而专业的话术就是<code>归因</code>(Attribution)。一个朴素的思路是直接使用梯度 $\nabla _{x}F(x)$ 来作为 $x$各个分量的重要性指标，而积分梯度是对它的改进。</p><h2>朴素梯度</h2><p>考虑一个图片分类系统，将输入图 $x$ 分为$C$个类别中的一个，对于每个类$c\in C$，都有一个函数$S_c(x)$将输入$x$映射到类别空间的得分，分类结果则取决于哪个映射值最大，即</p><p>$$
class(x)=\argmax_{c\in C} S_c(x)
$$</p><p>如果$S_c(x)$本事就是可导的，一个简单的方法就是直接对$S_c(x)$求导，即</p><p>$$
M_c(x)=\nabla_x S_c(x)
$$</p><p>这里的$M_c(x)$表示$x$上的每个像素上的微笑扰动对类别$c$得分的影响。</p><p>这种方法在实际操作中确实能显示出与分类结果相关的区域，但求得的 saliency maps 通常在视觉上有很多的噪点(对这些噪点作用目前还不清楚，有可能这些噪点看似随机，实际上对网络的运作有很大的影响，也有可能这些噪点本身就不重要)，但正是因为这些噪点的存在，导致只能大致确定相关区域的位置，而不能给出符合人类理解的结果，如下图：</p><p><img src="./src/Axiomatic-Attribution-for-Deep-Networks/grad.png" alt="image"/></p><p>产生噪点的可能原因在于$S_c$函数的导数在小范围内有很大的波动，毕竟没有任何理由要求该函数是平滑的，而且网络通常采用$ReLU$作为非线性激活函
数，所以$S_x$甚至不是连续可导的。</p><p>下面给一特定图片加一微小的扰动，观察其中一个像素的偏导数的变化情况：</p><p><img src="https://upload-images.jianshu.io/upload_images/13184001-16a9f43eefa626d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/610/format/webp" alt="image"/></p><p>从数学层面推导，它就是基于泰勒展开</p><p>$$
\begin{align}
F(x+\Delta x)-F(x)&amp;\approx \left \langle \nabla_xF(x),\Delta x \right \rangle =\sum_i<!-- -->[\nabla_xF(x)]<!-- -->_i\Delta x_i
\end{align}
$$</p><p>其中$\Delta x_i$是一个微小的扰动，$\Delta x_i=\epsilon_i$，$\epsilon_i$是一个微小的随机噪声。</p><p>$\nabla_xF(x)$是大小跟$x$一样的向量，这里$<!-- -->[\nabla_xF(x)]<!-- -->_i$为它的第$i$个分量，那么对于同样大小的$\Delta x_i$，$<!-- -->[\nabla_xF(x)]<!-- -->_i$的绝对值越大，那么$F(x+\Delta x_i)$相对$F(x)$的影响越大。也就是说：</p><blockquote><p>$<!-- -->[\nabla_xF(x)]<!-- -->_i$衡量了模型对输入的第$i$个分量的敏感程度，所以用$|<!-- -->[\nabla_xF(x)]<!-- -->_i|$作为第$i$个分量的重要性指标。</p></blockquote><p>这种思路比较简单，很多时候它确实可以成功解释一些模型，但是它也有明显的缺点。一旦进入到了饱和区(典型的就是 ReLU 的负半轴)，梯度就为 0 了，那就揭示不出什么有效信息了。</p><p>按照论文中的描述就是说违反了$Sensitivity$公理。</p><p>公理: $Sensitivity$</p><blockquote><p>定义：如果对于所有仅在一个特征上具有不同取值的输入 ($input$) 和基线 $(baseline)$ ，并且模型为两者给出了不同的预测。那么，那个不同取值的特征应该被赋予一个非 0 归因。若一个归因方法满足上面的要求，则称该归因方法满足 $Sensitivity$。</p></blockquote><p>举个例子，一个单变量$ReLU$网络</p><p>$$
f(x)=1-ReLU(1-x)=\left<!-- -->{<!-- -->\begin{matrix}
x,x&lt;1<!-- -->\<!-- -->
1,x\geq 1
\end{matrix}\right.
$$</p><p>假设基线$(baseline)$为$x=0$并且输入$x=2$，那么显然$f(0)=0,f(2)=1$。下面检测是否满$Sensitivity$公理：</p><p>首先，输入$x=2$和基线$x=0$不同；其次，输入$f(x=2)=1$与基线$f(x=0)=0$也不同；不违法$Sensitivity$公理的归因方法应该为输入$x$归因一个非 0 值，但是梯度在$x=2$时为 0，所以不满足$Sensitivity$公理。</p><h2>积分梯度</h2><h3>参照背景</h3><p>首先，我们需要换个角度来理解原始问题：我们的目的是找出比较重要的分量，但是这个重要性不应该是绝对的，而应该是相对的。比如，我们要找出近来比较热门的流行词，我们就不能单根据词频来找，不然找出来肯定是“的”、“了”之类的停用词，我们应当准备一个平衡语料统计出来的“参照”词频表，然后对比词频差异而不是绝对值。这就告诉我们，为了衡量 $x$ 各个分量的重要性，我们也需要有一个“参照背景”$\bar{x}$。</p><p>很多场景下可以简单地让$\bar{x}=0$，但这未必是最优的，比如还可以选择$\bar{x}$为所有训练样本的均值。我们期望 $F(\bar{x})$应当给一个比较平凡的预测结果，比如分类模型的话，$\bar{x}$的预测结果应该是每个类的概率都很均衡。于是我们去考虑 $F(\bar{x})−F(x)$，我们可以想象为这是从 $x$ 移动到 $\bar{x}$ 的成本。</p><p>如果还是用近似展开，那么我们可以得到</p><p>$$
\begin{align}
F(\bar{x})-F(x)\approx\sum_i<!-- -->[\nabla_x F(x)]<!-- -->_i<!-- -->[\bar{x}-x]<!-- -->_i
\end{align}
$$</p><p>对于上式，我们就可以有一种新的理解</p><blockquote><p>从 $x$ 移动到 $\bar{x}$ 的总成本为$F(\bar{x})-F(x)$，它是每个分量的成本之和，而每个分量的成本近似为$<!-- -->[\nabla_x F(x)]<!-- -->_i<!-- -->[\bar{x}-x]<!-- -->$，所以我们可以用$|<!-- -->[\nabla_xF(x)]<!-- -->_i<!-- -->[\bar{x}-x]<!-- -->_i|$作为每个$i$分量的重要性指标。</p></blockquote><p>当然，不管是$<!-- -->[\nabla_xF(x)]<!-- -->_i$还是$|<!-- -->[\nabla_xF(x)]<!-- -->_i<!-- -->[\bar{x}-x]<!-- -->_i|$，它们的缺陷在数学上都是一样的（梯度消失），但是对应的解释缺并不一样。</p><h3>积分恒等</h3><p>前面$|<!-- -->[\nabla_xF(x)]<!-- -->_i<!-- -->[\bar{x}-x]<!-- -->_i|$不够好是因为公式$(2)$不够精确，那如果能找到一个精度相等的类似表达式，那么就可以解决这个问题了。积分梯度正是找到了这样的一个表达式：</p><p>设$\gamma(a),a\in<!-- -->[0,1]<!-- -->$代表连接$x$和$\bar{x}$的一条参数曲线，其中$\gamma(0)=x,\gamma(1)=\bar{x}$，那么我们就有</p><p>$$
\begin{aligned}
F(\bar{x})-F(x) &amp;=F(\gamma(1))-F(\gamma(0)) <!-- -->\<!-- -->
&amp;=\int<em>{0}^{1} \frac{d F(\gamma(\alpha))}{d \alpha} d \alpha <!-- -->\<!-- -->
&amp;=\int</em>{0}^{1}\left\langle\nabla<em>{\gamma} F(\gamma(\alpha)), \gamma^{\prime}(\alpha)\right\rangle d \alpha <!-- -->\<!-- -->
&amp;=\sum</em>{i} \int<em>{0}^{1}\left[\nabla</em>{\gamma} F(\gamma(\alpha))\right]<em>{i}\left<!-- -->[\gamma^{\prime}(\alpha)\right]</em>{i} d \alpha
\end{aligned}
$$</p><p>可以看到，式子(3)具有跟式(2)相同的形式，只不过将$<!-- -->[\nabla_xF(x)]<em>i<!-- -->[\bar{x}-x]<!-- -->_i$换成了$\int</em>{0}^{1}\left<!-- -->[\nabla_{\gamma} F(\gamma(\alpha))\right]<em>{i}\left<!-- -->[\gamma^{\prime}(\alpha)\right]</em>{i} d \alpha$，但式子(3)的是精确的积分恒等式，所以积分梯度就提出使用</p><p>$$
\begin{align}
\left|\int<em>{0}^{1}\left[\nabla</em>{\gamma} F(\gamma(\alpha))\right]<em>{i}\left<!-- -->[\gamma^{\prime}(\alpha)\right]</em>{i} d \alpha\right|
\end{align}
$$</p><p>作为第$i$个分量的重要性度量。但是这两点间有无数路径，如图</p><p><img src="./src/Axiomatic-Attribution-for-Deep-Networks/Snipaste_2022-06-23_18-36-00.png" alt="image"/></p><p>作为最简单的方案，就是将$\gamma(a)$取为两点之间的直线，即</p><p>$$
\gamma(a)=(1-a)x+a\bar{x}
$$</p><p>这时候积分梯度具体化为</p><p>$$
\left|\left<!-- -->[\left.\int<em>{0}^{1} \nabla</em>{\gamma} F(\gamma(\alpha))\right|_{\gamma(\alpha)=(1-\alpha) x+\alpha \bar{x}} d \alpha\right]<em>{i}<!-- -->[\bar{x}-x]</em>{i}\right|
$$</p><p>所以相比于$|<!-- -->[\nabla_xF(x)]<em>i<!-- -->[\bar{x}-x]<!-- -->_i|$，用梯度的积分$\int</em>{0}^{1}\left<!-- -->[\nabla_{\gamma} F(\gamma(\alpha))\right]<em>{i}\left<!-- -->[\gamma^{\prime}(\alpha)\right]</em>{i} d \alpha$替换$\nabla_xF(x)$，也就是从$x$到$\bar{x}$的直线上每一点的梯度的平均结果。直观上来看，由于考虑了整条路径上的所有点的梯度，因此就不再某一点的梯度为 0 的限制了。</p><h3>离散近似</h3><p>最后这个积分着实有点恐怖。积分梯度可以通过求和来高效地做近似计算，只需要将基线$\bar{x}$至$x$直线上足够间隔点的梯度相近即可。</p><p>$$
\left|\left<!-- -->[\frac{1}{n} \sum<em>{k=1}^{n}\left(\left.\nabla</em>{\gamma} F(\gamma(\alpha))\right|_{\gamma(\alpha)=(1-\alpha) x+\alpha \bar{x}, \alpha=k / n}\right)\right]<em>{i}<!-- -->[\bar{x}-x]</em>{i}\right|
$$</p><h2>实验效果</h2><p>在分类问题中的效果</p><p><img src="./src/Axiomatic-Attribution-for-Deep-Networks//Snipaste_2022-06-23_18-37-52.png" alt="image"/></p><p>在自然语言中的效果</p><p><img src="./src/Axiomatic-Attribution-for-Deep-Networks/Snipaste_2022-06-23_18-40-31.png" alt="image"/></p><h2>附录</h2><p><a href="https://tensorflow.google.cn/tutorials/interpretability/integrated_gradients?hl=zh-cn">integrated_gradients</a></p><p><a href="https://github.com/wmn7/ML_Practice/blob/master/2019_07_08/Saliency%20Maps/Saliency%20Maps%20Picture.ipynb">Saliency Maps Picture</a></p>]]></content>
        <author>
            <name>PuQing</name>
            <uri>https://github.com/AndPuQing</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[欢迎来到魔法部日志]]></title>
        <id>欢迎来到魔法部日志</id>
        <link href="https://ml.akasaki.space/blog/[00]unlimited-paper-works"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[如果你看到了这里，说明你已经准备好开始探求这个领域事物的规律以及这些规律的本源了。作为一个本科生，最适合你入坑的就是开始习惯性阅读领域内论文。大约在我的大二的下半学期，我和我的朋友们开始共同阅读论文并写下笔记。这些笔记粗浅、幼稚，甚至会出现一些理解上的错误——万事开头难。但是我们还是想把这些笔记整理起来——这便是魔法部日志的开始。在我新建文件夹的时候，魔法部日志的文件夹名称是“unlimited paper works”，在成为理性的怀疑者之前，应该先掌握这个科研领域。我们做好了长期投入的准备，并希望把简单的事情做到出人意料得精彩。]]></summary>
        <content type="html"><![CDATA[<p>如果你看到了这里，说明你已经准备好开始探求这个领域事物的规律以及这些规律的本源了。作为一个本科生，最适合你入坑的就是开始习惯性阅读领域内论文。大约在我的大二的下半学期，我和我的朋友们开始共同阅读论文并写下笔记。这些笔记粗浅、幼稚，甚至会出现一些理解上的错误——万事开头难。但是我们还是想把这些笔记整理起来——这便是魔法部日志的开始。在我新建文件夹的时候，魔法部日志的文件夹名称是“unlimited paper works”，在成为理性的怀疑者之前，应该先掌握这个科研领域。我们做好了长期投入的准备，并希望把简单的事情做到出人意料得精彩。</p><p>加入魔法部日志也不是什么难事，你只需要热身一下，读完下面的一篇引导，就可以开始了(以下内容已通过语法检查工具<a href="https://github.com/PaperCube">PaperCube</a>的检查)。</p><h2>How to Read and Comprehend Scientific Research Articles</h2><p>Scientific articles are how scholars and researchers communicate with each other. Reading scientific articles helps you to participate in your comprehension by wondering how the researchers explain their ideas. Books, websites, papers, scientific magazines are general places to start with.</p><p>This tutorial will discuss:</p><ul><li>How to read a scientific article</li><li>How to find the main points of an article</li><li>How to take effective notes</li></ul><h3>How to read a scientific article</h3><p>The least effective way to read scientific articles is from the start to finish. Instead, expert researchers scan the article skimming for key findings. The structured scientific articles are defined by several distinct sections. Most articles like lab reports are divided into five sections:</p><ul><li>Abstract</li><li>Introduction</li><li>Methods </li><li>Results</li><li>Discussion</li></ul><p>And the most effective way to read a scientific article is to follow this order: Abstract, Discussion, Introduction, Results, Methods. The difference between the original structure and suggested reading order are listed below:</p><table><thead><tr><th>The original structure</th><th>Order suggested</th></tr></thead><tbody><tr><td>Abstract</td><td>Abstract</td></tr><tr><td>Introduction</td><td>Discussion</td></tr><tr><td>Methods</td><td>Introduction</td></tr><tr><td>Results</td><td>Results</td></tr><tr><td>Discussion</td><td>Methods</td></tr></tbody></table><p>By reading in the suggested order, you can quickly find the information you need to determine if the article is useful for you. After you read each section, asking yourself whether the article is interesting and relevant enough to your research assignment will help you to decide whether to continue reading it.</p><h4>The Abstract</h4><p>Abstract usually contains four kinds of information:</p><ul><li>Purpose of the study (why they did it)</li><li>Methodology (how they did it)</li><li>Results (what they have found)</li><li>Conclusion (what it means)</li></ul><p>After reading these sections, think about whether you should continue your reading.</p><h4>The Discussion</h4><p>This section usually contains things below:</p><ul><li>Clear answers about the question posed in the introduction</li><li>Explanations about how the results supports the conclusions</li></ul><p>After reading this sections ask yourself whether you understand and believe the author(s)&#x27; claims.</p><h4>The Introduction</h4><p>The introduction serves two purposes:</p><ul><li>Stimulating and interests the subject</li><li>Putting the article in the large context</li></ul><p>Generally introductions achieve these goals by leading the reader from the <code>General</code>(what is already known to the topic), to the <code>Specific</code>(what is not yet known), to the <code>Focused Question</code>(what the authors are asking). Thus, the authors describe previous works and how they are related to it.</p><p>Before we move on to the next section, ask yourself why the authors did this study, and, does the researched question match up with the conclusions in the discussion?</p><h4>The results</h4><p>The results&#x27; section states:</p><ul><li>What the author has found</li><li>Key data, often shown in figures or tables</li></ul><p>Ask yourself if the data collected are appropriate to answer the researched question before moving on to the next section.</p><h4>The methods</h4><p>The method sections tells the reader:</p><ul><li>What experiments were done to answer the question stated in the introduction</li></ul><p>This section can be difficult to read for students due to the technical language used and complex details listed. However, you can fully understand what happened by reading it carefully.</p><h3>How to find the main points of an article</h3><p>While you are reading the article, also distinguish the author&#x27;s main points. It can be difficult to distinguish between the main point and less relevant sub-points.</p><p>Key places to look for the main points include:</p><ul><li>The article title</li><li>The abstract</li><li>Keywords</li><li>Figure and Table titles</li><li>The first and the last sentences of the Introduction</li></ul><p>The keywords to look out for the main points include:</p><ul><li>&quot;We hypothesize that...&quot;</li><li>&quot;We propose...&quot;</li><li>&quot;We introduce...&quot;</li></ul><h3>How to take effective notes</h3><p>Another important part of reading and comprehending a scientific article is to take notes. Effective note-taking will save your time and help you clarify your thoughts.</p><p>Creating a standard template for taking note will help you organize your research and enable you to make quick comparisons and will save your time re-reading articles. </p><p>A possible template of reading articles can be:</p><table><thead><tr><th>Article title</th><th></th></tr></thead><tbody><tr><td>Author(s)</td><td></td></tr><tr><td>Journal</td><td></td></tr><tr><td>Date</td><td></td></tr><tr><td>Pages/Volume</td><td></td></tr><tr><td>Issues</td><td></td></tr><tr><td>URL</td><td></td></tr><tr><td>Main concepts</td><td></td></tr><tr><td>My critical response</td><td></td></tr></tbody></table><h3>Summary</h3><p>So we know that reading a paper in the proper order, making full understand of the author&#x27;s main points, and taking effective notes can be a effective way to do your research.</p><p>好了，你可以启程了。</p><hr/><h2>写作一篇科技类论文？</h2><p>注：这一部分是在听了陈关荣教授的分享后写下的。</p><h3>认真写作的重要性</h3><p>请认真写作。</p><p><img src="./src/unlimited-paper-works/image-20210607125125488.png" alt="image-20210607125125488"/></p><p>上图是不认真写作的结果。</p><h3>一篇科技论文典型的结构</h3><table><thead><tr><th>Content</th><th>描述</th></tr></thead><tbody><tr><td>Title（题目）</td><td>文献的题目。通常情况下，文献的题目应该简单、准确、精炼、引人注目，表达文章主要内容或思想。</td></tr><tr><td>Author（作者）</td><td>文献的作者。（千万不能未经同意和许可就随便地把别人例如导师的名字加到文章上单方面去投稿）</td></tr><tr><td>Abstract（摘要）</td><td>文献的摘要，要全面准确、简明握要。摘要是供出版社、图书馆、信息库检索用的，通常要单独刊登，因此，要自我完备，尽量不要使用数学公式、数学符号、方程序号、 引文序号、图表等等。</td></tr><tr><td>Keywords（关键词）</td><td>文章的关键字，文章所包含的研究领域或方向，并且应该用单数（例如：“attractor” 而不是 “attractors”）。SCI 系统利用关键词来分类文献，并且读者利用关键词来搜索文章。 关键词不全面可能导致检索遗漏和引用减少。</td></tr><tr><td>Introduction（引言）</td><td>引言应该全面、客观、准确地介绍问题的背景和历史发展， 他人以及自己的贡献，本文的动因和主要成果。注意，短文在引言部分并不一定需要说明文章的基本结构。</td></tr><tr><td>Sections（段落）</td><td>文章的内容分为很多段落。这些段落的标题、编号和格式应该尽量统一；在写作时，要尽量避免冗长的句子、避免不必要的符号和定义、避免太多太滥的缩写、避免太多太滥的方程号码。</td></tr><tr><td>Conclusion（结论）</td><td>文章的结论。结论中不要简单地改写甚至重复文章的摘要；和摘要部分一样，结论部分不要援引前文中出现过的方程号码、图表号码， 不要重新讨论数学公式、给出定理补充证明之类。</td></tr><tr><td>Acknowledgement（致谢）</td><td>在这部分可以感谢认真而又有实质性建议的匿名审稿人、认真而又有实质性建议的朋友、以及有关科研基金。</td></tr><tr><td>References（参考文献）</td><td>这部分列表呈现写作时参考的文献。请注意严格使用标准格式。关于格式，请使用统一格式，或使用你打算投稿的那个杂志的格式。文献的多少要恰当：不要漏掉重要和必要的文献，又不要罗列多余且毫不相关的文章。</td></tr><tr><td>Appendix（附录）</td><td>附录通常可以放一些比较长的引理和定理的证明，方便读者在阅读简洁的原文后查阅。</td></tr></tbody></table><p>你可以通过一些工具来规范你的写作。</p><h3>Abstract的写作技巧</h3><p>以下内容摘抄自网络：</p><blockquote><p>套路一般是，先说大环境下本领域的发展，分类，极其对其他领域的作用。进而转到其中某某模块(你针对的模块)对整体的性能至关重要。该模块已被多个其他领域应用，如U,V,W(列出参考文献)。针对该模块，大家都使用的方法是Z，近十年来一直如此。后续针对传统方法Z的不足，近几年有学者提出了D方法(ECCV2017)，用以改善XXXX。接着针对D方法的不足，又有人提出了E，解决了D的问题，比如提高D的速度，或者D的稳定性，或者是D的推广版本。E是相当经典的，这么多年以来成为了主导，可谓人尽皆知。最新的研究进展中，有学者还提出了F, H等变体，分别考虑了XXXX，有的还应用在其他领域中。但是(关键来了)，上述这些方法，都仅仅考虑了XXXX层面，因素。However, we observed that......，they may suffer from……，they face the problem of……这个地方最好需要你用一句神来之笔的观点说出这些方法的问题，从而让人眼前一亮，让人恍悟好像这些方法确实是这么一回事，甚至让人恍悟原来E方法能有提升是出于背后这么个原因。总之你若能总结到类似于这样的深刻道理，那基本上审稿人对你论文的看法就是very novel。加之你的提升有1.5个点这么多，那么审稿人对你实验结果的看法也将是significant或very convincing。</p></blockquote><ul><li>为了流畅地书写Abstract，你需要有足够的文献储备，这样才能做到信手拈来。</li><li>注意语言表达要客观，要尊重，还要犀利。</li></ul><h3>论文的投稿流程</h3><ul><li>Have an idea （你有一个想法）</li><li>Do your research （完成你的研究和实验）</li><li>Write your paper （进行论文写作）</li><li>Identify  a journal （选择要发表的期刊。老手的一些想法是有想法之后先选择期刊再进行论文写作，有许多直接被拒的论文不是写的不好而是找错了地方。）</li><li>Submit your paper （提交你的论文）</li><li>Receive reviewers&#x27; comments （收到审稿人评价）</li><li>Revise and submit （修改后重新提交）</li><li>Receive further comments （收到后续审稿意见）</li><li>Revise and submit （修改后重新提交）</li><li>......</li><li>Get accepted and published （被接受并发表）</li></ul><h3>论文的审稿流程</h3><ul><li>Editor-in-Chief receives manuscript （主编收到论文）</li><li>Checks for quality, scope （检查论文质量。Desk rejection 也就是拒稿往往是在这一步发生的）</li><li>Assigns to an editor including the EIC herself （分配给编辑审阅）</li><li>Handling editor invites reviewers （编辑邀请审稿人）</li><li>May or may not use suggested reviewers （接受或者不接受该审稿人评阅）</li><li>Receives reviewers&#x27; comments （收到审稿人意见）</li><li>Makes decision : Reject, major revision, minor revision or accept （做决定：拒绝，重大修改，轻微修改，还是接受论文）</li><li>reject and resubmit （拒绝并重新提交）</li><li>Receives revised manuscript （接收修改后的论文）</li><li>Invites reviewers （邀请评阅）</li><li>Likely the previous reviewers, but could also be new reviewers （可能是之前的审稿人，也可能是新的审稿人）</li><li>Receives reviewers&#x27; comments （收到审稿人意见）</li><li>......</li><li>Accepts （接受）</li></ul><h3>常见的简单审稿意见（在写作期就可以避免）</h3><table><thead><tr><th align="center">审稿意见</th><th align="left">审稿意见全文</th><th align="left">参考中文</th><th align="left">举例</th></tr></thead><tbody><tr><td align="center">参考文献不要扎堆</td><td align="left">Eliminate multiple references. After that please check the manuscript thoroughly and eliminate all the lumps in the manuscript. This should be done by characterizing each reference individually. This can be done by mentioning 1 or 2 phrases per reference to show how it is different from the others and why it deserves mentioning</td><td align="left">消除出现在同个位置的多个引用。在那之后，请彻底检查手稿，消除所有的扎堆文献。这应该通过单独描述每个引用来实现。这可以通过在每次引用中提到1或2个短语来说明它与其他的不同之处，以及为什么它值得提到。</td><td align="left">修改如“前人的工作有<!-- -->[<!-- -->1<!-- -->]<!-- -->[<!-- -->2<!-- -->]<!-- -->[<!-- -->3<!-- -->]<!-- -->[<!-- -->4<!-- -->]<!-- -->[<!-- -->5<!-- -->]<!-- -->[<!-- -->6<!-- -->]<!-- -->[<!-- -->7<!-- -->]<!-- -->”的大量引用，仅保留必要的。</td></tr><tr><td align="center">结论中增加意义量化及对比</td><td align="left">In the conclusions, in addition to summarizing the actions taken and results, please strengthen the explanation of their significance. It is recommended to use quantitative reasoning comparing with appropriate benchmarks, especially those stemming from previous work.</td><td align="left">在结论中，除了总结所采取的行动和结果外，请加强对其重要性的解释。建议使用定量推理与适当的基准进行比较，特别是那些源于以前工作的基准。</td><td align="left">增加如“我们在前人提出的xxx之上又做出了怎样的贡献”的语句。</td></tr><tr><td align="center">结论中增加意义量化及对比</td><td align="left">Please revise the conclusion in paragraphs. Conclusions are not just about summarizing the key results of the study, it should highlights the insights and the applicability of your findings/results for further work. Please make it more concise and show only the high impact outcomes. Report your Conclusions in one or maximum 2 paragraph. Avoid <a href="https://en.wikipedia.org/wiki/Bullet_(typography)">bullet form</a>).</td><td align="left">请在段落中修改结论。结论不仅仅是总结研究的关键结果，它应该突出你的发现结果对进一步工作的洞察力和适用性。请让它更简洁，只显示高影响的结果。一段或最多两段报告你的结论。避免<a href="https://en.wikipedia.org/wiki/Bullet_(typography)">子弹形式</a>。</td><td align="left">一般结论不超过半页纸。请不要在结论里使用分条分点的段落形式。</td></tr><tr><td align="center">删除多余的虚词</td><td align="left">Eliminate the use of redundant words, e.g. in this way, recently, respectively, therefore, currently, thus, hence, finally, to do this, first, in order, however, moreover, nowadays, consequently, in addition, additionally, furthermore. Revise all similar cases, as removing these term(s) would not significantly affect the meaning of the sentence.</td><td align="left">消除多余虚词的使用，例如：这样做、最近、分别、目前、因为、因此，最后、这件事、首先、顺序、然而、此外、如今、因此、此外等词汇。修改或删除这些“删除后不会对句子的意思产生重大影响”的词。</td><td align="left">删除诸如“关于这件事...”等无关语义的连接，对各种虚词的使用仅保留必要的部分。</td></tr><tr><td align="center">对一千以上的数字增加分隔符</td><td align="left">Add a separator for the numbers over 1,000. Check all numbers including those in the tables/figures.</td><td align="left">为超过1000的数字添加分隔符。检查所有数字包括表/图中的数字。</td><td align="left">例如将“1000”改为“1,000”。</td></tr><tr><td align="center">检查下标格式</td><td align="left">Check all format, e.g. &quot;SO2&quot; in Fig.5, 2 should be in subscript, check all。</td><td align="left">在Fig.5中，2应该是下标，请检查全部格式。</td><td align="left">例如二氧化硫$SO_2$不小心写成了$SO2$，请检查并改正。</td></tr><tr><td align="center">请使用国际制单位</td><td align="left">Please use SI unit. E,g, m instead of meter, t instead of tons. d instead of day, y instead of years or yr, h instead of hours, M instead of million, kg instead of kilogram or Kg (including those in figures/tables) and leave a space between the value and unit. Please check all.</td><td align="left">请使用国际制单位。例如，“m”代替米，用“t”代替吨。用“d”代替“天”，用“y”代替“年”用“h”代替“小时”用“M”代替“百万”，用“kg”代替“公斤”或“Kg”(包括图表中的单位)，并在数值和单位之间留一个空格。</td><td align="left">原文中已经足够举例。</td></tr><tr><td align="center">没有添加页码或行号</td><td align="left">I suggest the authors add page and line numbers when they re-submit it. It will be easier for reviewers to make comments.</td><td align="left">我建议作者在重新提交时添加页码和行号。评审者可以更容易地发表评论。</td><td align="left">加上页码，需要行号的位置增加行号。</td></tr><tr><td align="center">图表缺少摘要</td><td align="left">Please provide a graphical abstracts.</td><td align="left">请提供图表摘要。</td><td align="left">一张没有任何文字描述的图表。</td></tr></tbody></table><h3>常见的复杂审稿意见</h3><table><thead><tr><th align="center">审稿意见</th><th>审稿意见全文</th><th>参考中文</th></tr></thead><tbody><tr><td align="center">文章亮点问题</td><td>This is not a highlight of the research.</td><td>这不是研究的重点。</td></tr><tr><td align="center">文章缺少亮点</td><td>I think highlights are mandatory for this journal, and they are missing in this paper.</td><td>我认为本期刊论文必须具备Highlight的部分，但是这篇论文中没有</td></tr><tr><td align="center">文章类型比较像报告而不是论文</td><td>On the whole, the manuscript is more like a thesis or a report rather than a scientific research. In my understanding, this is not acceptable in a scientific paper of this field of knowledge. The paper is well-written and organized. However, it seems more a technical report than a scientific contribution at this moment. I&#x27;m not sure about the SCIENTIFIC CONTRIBUTION of this paper, since most of the results and discussions seems to be more TECHNICAL than SCIENTIFIC.</td><td>总的来说，稿件更像是理论或报告，而不是科学研究。(这种形 式)在这一知识领域的学术论文中是不能接受的。这篇论文写得很好，条理清晰。然而，目前看来，它更像是一份技术报告，而不是一份学术成果。我不确定这篇论文的学术贡献，因为大多数结果和讨论似乎缺乏学术性。</td></tr><tr><td align="center">文章类型比较像报告而不是论文</td><td>This existing review article makes few relevant contributions to the academic environment. The whole article looks like a technical report, rather than a scientific one. Please tell readers, what the research gaps are, what the scientific contributions are. Then, pls. re-organize the text.</td><td>现有的综述文章对学术环境的相关贡献很少。整篇文章看起来像是一篇技术报告，而不是学术论文。请告诉读者(这一领域的)研究空白是什么，科学贡献是什么，并重新组织文章。</td></tr><tr><td align="center">文章新颖性不足</td><td>My first and primary concern lies in the novelty of this work, as <!-- -->|<!-- -->feel that the novelty issue has not been sufficiently.</td><td>我首先关注的是这个作品的新颖性，因为我觉得这个新颖性问题在目前的版本中没有得到充分的强调。应该回答一个重要的问题:这项工作是否填补了一些以前的文章不能解决的知识空白？</td></tr><tr><td align="center">论文价值体现不足</td><td>The author needs to provide the contributions of this study more specific.</td><td>作者需要更具体地提供本研究的贡献。</td></tr><tr><td align="center">没有体现研究的重要性</td><td>The second major concern is related to the significance of this work, that is, how the results derived from the work be benefitting to the WEEE management? A list of issues can be proposed.</td><td>第二个主要的关注点与这项工作的意义有关，即这项工作的结果如何有益于WEEE管理可以提出一系列问题。</td></tr><tr><td align="center">没有体现研究的重要性</td><td>The discussion is related to the theory, but the relevance of the findings to the modernization of the state of art is not clear. The : methodology is well designed, but there are missing elements that relate the proposed to what was actually found by the authors. Contributions are unclear.</td><td>讨论与理论有关，但研究结果与当前（研究的）技术方法水平的相关性并不清楚。该方法设计得很好，但是缺少一些与作者实际发现的内。容相关的元素。贡献尚不清楚。</td></tr><tr><td align="center">摘要需要重新写</td><td>The format of the abstract is not correct. Please read the author guidelines. Is too descriptive and lengthy. Please add some quantities to it and shorten it through focusing on the main point.</td><td>摘要的格式不正确。请阅读作者指南。太描述性和冗长。请增加一些定量描述，并通过主要观点来缩减摘要内容。</td></tr><tr><td align="center">摘要需要重新写</td><td>|<!-- --> suggest the authors to rewrite the abstract with a focus on background, objectives, methodology, main findings and conclusion. Please add a sentence which shows the necessity of the study.</td><td>我建议作者重写摘要，重点关注背景、目标、方法、主要发现和结论。请说明研究的必要性。</td></tr><tr><td align="center">引言有些跑题</td><td>Actually, it is a little far from the topic of this study. Literature review - this section is quite extensive in its coverage, but tends to summarize disaggregate studies of EV adoption derived from surveys. The research has seemingly overlooked the existing literature which applies spatial models to EV registrations, which is surprising given that this is the focus of the work. I&#x27;d advise the authors to examine the following papers as they <!-- -->[a]<!-- --> might find them useful for results comparisons and <!-- -->[b]<!-- --> demonstrate that the association between EV registrations and charge point availability has been considered.</td><td>事实上，这离本研究的主题有些偏离。文献综述-本节涵盖范围相当广泛，但倾向于总结从调查中得出的采用电动汽车的研究令人惊讶的是，这项研究似乎忽略了（总结）将空间模型应用于电动汽车登记的现有文献，因为这是工作的重点。我建议作者们检查一下下面的论文，因为他们<!-- -->[a]<!-- -->可能会发现它们有利于结果对比，并且<!-- -->[b]<!-- -->证明了电动汽车注册和充电桩可用性之间的关联。</td></tr><tr><td align="center">引言思路不清晰</td><td>I do not think the authors make it very clear of their contributions to this field.</td><td>我认为作者没有很清楚地对他们在这个领域的贡献进行说明。</td></tr><tr><td align="center">引言思路不清晰</td><td>In the &quot;Introduction&quot; and &quot;Literature review&quot; section, I do not think the authors make it very clear of their contributions to this field. They have citied almost 90 papers about the relevant studies at the city scale, which is quite a lot. But it is still not clear to me, what specific research questions the authors are asking and how they contribute to the existing studies.</td><td>在“介绍”和“文献综述”部分，我认为作者并没有很清楚地说明他们对这个领域的贡献。他们引用了近90篇城市尺度的相关研究论文，数量相当可观。但我仍然不清楚，作者研究了哪些具体的问题，以及他们有何贡献。</td></tr><tr><td align="center">研究方法介绍不够</td><td>Methodology - the preliminary focus groups are an interesting component to the work, but are not outlined in sufficient detail. l&#x27;d advise that more clarity is offered concerning <!-- -->[a]<!-- --> who participated, <!-- -->[b]<!-- --> how the narratives were analyzed, <!-- -->[c]<!-- --> the main findings.</td><td>方法-初步的焦点小组是工作中的重要的组成部分，但没有(对此)给出充分的描述。我建议，应该就<!-- -->[a]<!-- -->参与的人，<!-- -->[b]<!-- -->如何分析的，<!-- -->[c]<!-- -->主要发现，进行更清晰的说明。</td></tr><tr><td align="center">应用对象新但方法旧</td><td>While this is the first application in XXXX, such an approach has been utilized in the past.</td><td>虽然这是在XXXX领域的第一次应用，但这种方法在过去也被使用过。</td></tr><tr><td align="center">缺乏对比</td><td>The author should provide the advantages of this paper compared to other types of works.</td><td>作者应该提供这篇论文相较于其他工作的优点。</td></tr><tr><td align="center">讨论不够深入</td><td>I read the results and discussion section completely. The discussion section is the main part of a paper, but this manuscript mainly reported the data of the modelling without discussing it through adding available reasoning for justifying the result. I recommend author adding several reasoning and comparison through available publications in the literature.</td><td>我完整地阅读了结果和讨论部分。讨论部分是论文的主要部分，但这篇稿件主要描述了模型的数据，并没有通过增加可用的推理来对结果进行讨论。我建议作者在结合文献的基础上添加一些推理和比较。</td></tr><tr><td align="center">讨论部分建议不够详细</td><td>Please discuss qualitatively and quantitatively for &quot;Suggestions for further improvements&quot; in detail.</td><td>请就“进一步改进的建议”进行详细的定性和定量讨论。</td></tr><tr><td align="center">讨论部分缺乏启示意义</td><td>Please insert a section on the implications of the study. Who benefits with it? What problem can the study help to solve? What&#x27; s next?</td><td>请插入一个部分讨论该研究的影响。谁从中受益了?这项研究可以帮助解决什么问题?下一步是什么?</td></tr><tr><td align="center">结论需要精炼</td><td>The conclusion part should be more refined to make the findings and contributions of the paper clearer. Furthermore, please note the difference between the conclusions and abstract.</td><td>结论部分应该更加精炼，使论文的发现和贡献更加清晰。此外，请注意结论和摘要之间的区别。</td></tr><tr><td align="center">结论需要精炼</td><td>I believe that this result can be obtained without too much analysis. As an alternative, it is recommended to use quantitative reasoning comparing with appropriate benchmarks.</td><td>我相信这个结果不需要太多的分析就可以得到。作为一-种替代方法，建议使用定量推理与适当的基准进行比较。</td></tr><tr><td align="center">文章贡献不够明确</td><td>So a clearer illustration of contribution or innovation should be further provided in the introduction and conclusion.</td><td>因此在引言和结论中应该进一步提供一个更清晰的贡献或创新的说明。</td></tr><tr><td align="center">语言使用需要修缮</td><td>The presentation should be further improved by native speakers, especially for those grammatical errors, typographical errors, and bad structured sentences.</td><td>文献应该由母语者进一步改进，特别是那些语法错误、排版错误、句子结构有问题的地方。</td></tr></tbody></table><h1>修改原稿</h1><p>Submit the original manuscript showing clearly all textual changes using track changes. Just highlighting textual changes in yellow (or other color) is not acceptable. This includes all edits related to reviewer(s) comments and the Editorial points. Do also submit the clean revised version of the manuscript.</p><p>请使用修订模式修改原稿，或使用其他颜色标注修改的部分。也请提交一份干净的完全修改版。</p><hr/><h2>Q&amp;A</h2><ol><li><p>Q：为什么每篇笔记后面首先会跟一个笔记作者的信息的三级标题？</p><p>A：为了建立静态索引。这样你在搜索框里就能直接通过搜索笔记作者的名称来找到他/她写下的全部笔记。这样做只是临时的，新的办法还没有找到。</p></li><li><p>Q：为什么笔记的格式不统一？</p><p>A：因为本部分是又很多人一起写下的笔记。大家写笔记的风格都不一样。</p><p>Q：那怎样读到适合自己的笔记格式？</p><p>A：如果你更倾向于阅读某位笔记作者的笔记，可以在网页右上方的搜索框搜索其名称。</p></li><li><p>Q：这些笔记的排列有什么顺序吗？</p><p>A：有，时间顺序。越新的笔记越靠下。将来unlimited-paper-works将会从ml.akasaki.space迁移出去，到时候会使用更合理的排列顺序。</p></li><li><p>Q：读了这些笔记就一定懂了这些论文吗？</p><p>A：并不。有机会请阅读原论文。这些笔记并不保证完善，甚至可能出现错误。</p></li></ol><hr/><h2>随时会变的没什么用的内容</h2><p>这次的没什么用内容是我学习的过程中遇到一些困惑，以及我的牢骚。简述之就是：才疏学浅，领域又发展太快，感觉出现了泡沫，找不到方向。</p><blockquote><p>现在越看越觉得除了那些创新backbone的论文，其他这些论文新技术天天出，但是都感觉在哪似曾相识（似乎就是以前看过的哪几个论文东拼西凑一下），看完了感觉似乎自己也能想出来（但其实完全想不出来），论文效果似乎很好，结果跑了发现完全跑不到，等差不多快跑到了新技术又出了。</p></blockquote><p>我最近主要的学习方向是使用深度学习(deep learning)技术的计算机视觉(CV, computer vision)分割(segmentation)任务。可能是因为深度学习技术发展的太快了，尤其是卷积神经网络(CNN, convolutional neural network)之后，一直到不久前GAN(generative adversarial network)开始流行于各个任务，仅仅花了不到十年。</p><p>在<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han%2C+Y">Yizeng Han</a>等的综述论文<a href="%5B08%5DDynamic-Neural-Networks-A-Survey">Dynamic Neural Networks: A Survey</a>中，作者将视觉领域的神经网络近十年的发展分为这样几个阶段：</p><ol><li><p>快速发展阶段（Fast developing stage），2012~2015</p><p>神经网络的设计变得多样化，出现了包括AlexNet、VGG、GoogLeNet在内的一系列代表性网络结构。</p></li><li><p>发展成熟阶段（Mature stage），2015~2017</p><p>这个阶段出现了很多至今都起到了很重要的影响的或是依然被大家经常使用的网络结构，例如ResNet、DenseNet等</p></li><li><p>繁荣发展阶段（Properous stage），2017~Now</p><p>人们设计了很多多样化的效果优秀的神经网络，并且大量出现了很多新型的神经网络，例如轻量级网络CondenseNet、ShuffleNet，利用自动搜索技术设计的模型NASNet、DARTS，还有这篇论文想要介绍的动态神经网络MSDNet、Block-Drop、Glance and Focus等，以及突然就火起来的Transformer。</p></li></ol><p>在之前几十年的AI发展中，大部分时候图像处理走的都很慢。但是在当前阶段，想要再做哪怕一点点improvement都很难。</p><p>一些开创性的工作比如<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ioffe%2C+S">Sergey Ioffe</a>等人的<a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>，或是一些非线性的激活函数，或是<a href="http://yann.lecun.com/">Yann LeCun</a>等人在<a href="../ch2p2/%5B1%5DLeNet">Gradient-Based Learning Applied to Document Recognition</a>（LeNet）中第一次使用卷积神经网络进行手写数字识别，这些工作都要去从数学那一块找新东西，这些方法有的以前可能只在理论上出现过，并且难以被理解或证明有效性，投入实验后也不一定能work，上述这些工作通过实验证明了其有效性，后人就开始在其基础上开展工作了。很遗憾的是这些一旦出现就会产生巨大影响的工作不会经常出现。</p><p>很多对后人具有重要且深远影响的工作例如<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Simonyan%2C+K">Karen Simonyan</a>等人的<a href="../ch2p2/%5B7%5DVGGNet">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>（深度卷积网络的初步探索）以及<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K">Kaiming He</a>等人的<a href="../ch2p2/%5B11%5DResNet">Deep Residual Learning for Image Recognition</a>（首次提出残差网络结构）都具有巨大的创新：比如使用了全新的网络结构，或是全新的目标函数（loss function, 或称为损失函数），或者很新的正则化（regularizer）方法。最近也有一些具有重大影响的新结构，例如<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani%2C+A">Ashish Vaswani</a>等人的<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>也就是Transformer中，结合了最新的神经网络注意力(Attention)机制，抛弃了的CNN和RNN结构，整个网络结构完全是由Attention机制组成。这些方法往往需要大量的积累和实验，不可能经常做出。</p><p>最新的工作它们之间往往具有很大的相关性，即便它们出自不同的作者。例如，<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu%2C+C">Changqian Yu</a>等人的论文<a href="./%5B24%5DBiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>中，有许多思想参考了<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu%2C+J">Jie Hu</a>等人的<a href="./%5B23%5DSqueeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a>。根据某项创新，可能会出现更多类似的创新，应用于同领域会应用于其他领域。这类工作的出现速度非常之快：研究者在阅读前人工作时发现不做或可以修改的点，就会进行实验并迅速发出一篇论文。在结果中，研究者往往会注明新的研究达到了多高的精度，有时还会附赠一份源代码。但是其他人尝试这份源代码时，却发现精度远远达不到论文中提及的水平。实际上，模型的表现不确定因素实在是太多。</p><p>就中国人的数量来说，如果每个研究生都要水出一篇论文，可能会导致领域内论文平均质量的下降。这个领域再也回不到每篇论文都具有很大创新和影响的时代了。</p><blockquote><p>有研究者网友打趣地说：计算机视觉方面的论文可以分为以下几类：「只想浑个文凭」、「教电脑生成更多猫的照片」、「ImageNet上试验结果精度提升0.1%」、「手里有很棒的数据集但并不打算公开」、「3年过去了，代码还在赶来的路上」、「实验证明还是老的baseline更好」、「我们的数据集更大」、「花钱越多，效果越好」...</p></blockquote><p>本人是一个名不见经传大学的本科生，既没有卓越的能力，也没有远见的眼光。想研究一个方向，只能摸索着来。也许有的时候努力方向是错的，但是完全不能自知。可以阅读的文献是大量的，但其中包含的有用的信息总量却不高。对于当前这个方向会怎样继续发展、应该从哪里着手创新，既没有神明现身说法，也是才疏学浅的我所不能自己知晓的——这种具有独特风味的难受也许只有菜鸡才能深有体会吧。</p><p>等一位贵人点醒无知的我。我的邮箱：$<a href="mailto:miya@akasaki.space">miya@akasaki.space</a>$。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Devil is in the Decoder - Classification, Regression and GANs]]></title>
        <id>The Devil is in the Decoder - Classification, Regression and GANs</id>
        <link href="https://ml.akasaki.space/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇讲各种各样解码器的论文。原论文（The Devil is in the Decoder: Classification, Regression and GANs）。]]></summary>
        <content type="html"><![CDATA[<p>这是一篇讲各种各样解码器的论文。<a href="https://arxiv.org/pdf/1707.05847.pdf">原论文（The Devil is in the Decoder: Classification, Regression and GANs）</a>。</p><p>由于“解码器（decoder，有些时候也被称为feature extractor）”的概念与像素级的分类、回归等问题多多少少都有瓜葛。以下是decoder被应用于像素级的任务：</p><ul><li>分类：语义分割、边缘检测。</li><li>回归：人体关键点检测、深度预测、着色、超分辨。</li><li>合成：利用生成对抗网络生成图像等。</li></ul><p>所以decoder是稠密预测（Dence prediction，像素级别的很多问题都可以叫做稠密的）问题的关键。</p><h1>Abstract（摘要）</h1><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原作</a>。本文只是对原作阅读的粗浅笔记。</p><hr/><p>​		语义分割、深度预测等计算机视觉任务往往需要对输入进行逐像素的预测，用于解决此类问题的模块通常由编码器组成。编码器（行为上是下采样的，通常情况下是卷积、池化组成的）在学习高维度特征的同时会降低输入图像的空间分辨率；在这之后是将其恢复原始分辨率的解码器（行为是上采样的，通常情况下是转置卷积等操作组成的）：</p><pre><code>编码器（特征提取器，降低特征图分辨率）---解码器（提高特征图分辨率）
</code></pre><h2>相关研究（Related works）</h2><p>​		这篇论文主要的内容是针对各种像素级的计算机视觉任务，对各种解码器进行了较为广泛的比较。以下是这篇论文的主要贡献：</p><ol><li>提出选择不同类型的解码器对效果的影响非常巨大</li><li>为解码器引入了类似残差（residual connection）的新连接</li><li>介绍了一种比较新颖的解码器：双线性加和上采样（bilinear additive upsampaling）</li><li>prediction artifacts（真的没想好怎么翻译）</li></ol><hr/><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502073352860.png" alt="image-20210502073352860"/></p><p>我们将需要逐像素预测的问题成为密集预测（dence prediction）的问题。通常编码器-解码器结构是用于解决这种密集预测问题的：首先，编码器（特征提取器）在增加通道数量的同时降低了图像的空间分辨率（通常为8~32倍）；然后，解码器进行上采样恢复到输入原图大小。从概念上讲，此类解码器可以被视为和编码器相反的操作：一个解码器至少由一个提高空间分辨率的层（通常称为上采样层）以及可能保持空间分辨率的层（例如单位卷积、残差快或是起始块）组成。其中 ，用于保持空间分辨率的层已经有了比较成熟的研究，所以这一篇论文只分析提升空间分辨率的部分。</p><p>目前在单个计算机视觉领域内使用最多的是转置卷积（transposed convolution），它在分割、深度预测、超分辨重建等任务中都有比较详细的论文进行研究。详见原论文中的相关字段。</p><p>还有一些为了加快模型速度进行的研究，例如：二维卷积在图像分类和语义分割的背景下被分解成两个一维卷；还有一些比较新颖的堆叠的沙漏结构（似乎也可以叫金字塔结构），它是由堆叠的多个编码器-解码器组成。</p><h2>现存的上采样层设计Existing upsampling layers）</h2><h3>转置卷积（Transposed Convolution）</h3><p>转置卷积是最常用的上采样层，有的时候也被称为“反卷积”或是“上卷积”。在输入和输出的关联关系上，转置卷积可以看作是卷积的一种反向操作，但实际上这并不是严格意义的逆运算，逆运算应该是可以被精确计算的，而转置卷积的计算结果并不是精确结果。转置卷积的一种示意如下图：</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502090940399.png" alt="image-20210502090940399"/></p><p>如图，常见的转置卷积一般会通过某种方式在输入中填充0，以获得一张更大的特征图，其后使用一个标准的卷积运算获得一个比最初始的输入大一些的特征图作为输出。</p><h3>分解的转置卷积（Decomposed transposed convolution）</h3><p>分解的转置卷积和转置卷积是相似的：</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502091343561.png" alt="image-20210502091343561"/></p><p>只不过分解的转置卷积将主卷积运算分为多个低秩卷积。例如，在图像中，分解的转置卷积通过两个一维的卷积对二维的卷积进行模拟。例如上图中，对于输入，先在行上进行隔行填充，然后使用一维的卷积核进行卷积，再在列上进行隔列填充，再使用一维的卷积核进行卷积。</p><p>分解的转置卷积严格意义上是转置卷积的子集。</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502091954758.png" alt="image-20210502091954758"/></p><p>如上图，这样做的优势是降低了可训练变量的数量（降低了参数量）。</p><p>分解的转置卷积已经在inception结构中获得了成功：在ILSVRC2012分类赛中获得了the state of the art的成果。</p><h3>深度到空间的转换（Depth-to-space）</h3><p>这种方法（Depth to space）有时也被称为“subpixel convolution”的基本思路是将特征通道移入空间域：</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502092533184.png" alt="image-20210502092533184"/></p><p>如上图，本应堆叠在channel维度的不同特征被融合进一个深度为1的平面特征图。这种方法能够很好地保留空间特征，因为它所做的仅仅是改变它们的位置而不是将它们堆叠进channel，而这种方法的缺点是引入了对齐伪像。为了能够和其他几个上采样方法进行横向对比，这篇论文在进行从深度到空间的转换实验之前的下采样卷积比其他上采样层的输出通道多了四倍。</p><h3>插值法（Interpolation）</h3><h4>最临近插值法（Nearest Interpolation）</h4><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502104856704.png" alt="image-20210502104856704"/></p><p>最近邻法不需要计算只需要寻找原图中对应的点，所以最近邻法速度最快，但是会破坏原图像中像素的渐变关系，原图像中的像素点的值是渐变的，但是在新图像中局部破坏了这种渐变关系。</p><h4>线性插值法（linear interpolation）</h4><p>线性插值法（单线性插值法）是指使用连接两个已知量的直线来确定在这个两个已知量之间的一个未知量的值的方法。 </p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502110046370.png" alt="image-20210502110046370"/></p><p>根据初中的知识，2点求一条直线公式，这是双线性插值所需要的唯一的基础公式。</p><h4>双线性插值（Bilinear interpolation）</h4><p>双线性插值可以理解为进行了两次单线性插值：</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502110140524.png" alt="image-20210502110140524"/></p><p>先在x方向求2次单线性插值，获得R1(x, y1)、R2(x, y2)两个临时点，再在y方向计算1次单线性插值得出P(x, y)（实际上调换2次轴的方向先y后x也是一样的结果）</p><h3>双线性上采样+卷积（Bilinear upsampling + Convolution）</h3><p>双线性上采样+卷积的意思就是在双线性插值之后进行卷积运算。为了和其他上采样方法比较，这篇论文中假设在上采样之后还要进行额外的卷积运算。这种方法的缺点是占用了大量内存和计算空间：双线性插值会二次增加特征量，但同时保持原来的“信息量”。由于假设了双线性上采样之后接有卷积运算，因此这种方法理论上比转置卷积方法的开销高四倍。</p><h3>双线性上采样+可分离卷积（Bilinear upsampling+Separable convolution）</h3><p>可分离的卷积用于构建简单且同质的网络结构，其结果优于InceptionV3。</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502111611626.png" alt="image-20210502111611626"/></p><p>如上图：一个可分离的卷积又两个操作组成：一个是对每个通道的卷积，另一个是使用<code>(1x1)</code>卷积核的逐点卷积对通道进行“混合”。</p><h3>双线性加性上采样（Bilinear additive upsampleing）</h3><p>这个方法是这篇论文在对上述现存的方法进行了叙述后提出的新方法。</p><p>该论文建议继续使用双线性上采样，但是该论文还将每N个连续的通道相加，从而将输出降低了N倍：</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502112224648.png" alt="image-20210502112224648"/></p><p>如上图，该方法的过程是确定性的，唯一可调的参数是N。虽然这个方法很像是之前说的“深度到空间的转换（Depth-to-space）”，但是这个方法并不会导致空间伪像的出现，也就是不需要考虑对齐操作。</p><p>在这篇论文的实验中，作者选择参数N的标准是让进行双线性加性上采样后和之前的浮点数相等，这使得这种上采样的性能开销类似于转置卷积。</p><h2>跨层连接和残差连接方法（Skip connections and residual connections）</h2><h3>跨层连接（Skip connections）</h3><p>跨层连接有时也被叫做跳跃连接。这种方法已经在很多解码器结构中获得成功，并且在很多其他的计算机视觉任务中取得了不错的成绩。</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502113600719.png" alt="image-20210502113600719"/></p><p>在这种方法中，解码器的每一层输入有两个来源：第一个是上层解码器得到的输出；第二个是在编码器中输出大小和自身输入大小匹配的一层输出的特征。</p><h3>解码器的残差连接（Residual connections for decoders）</h3><p>残差连接已经在很多不同的计算机视觉任务中被证明是有效的（来源是ResNet）。但是，残差连接并不能直接应用于解码器：在解码器中，下一层比上一层具有更大的空间分辨率和更少的通道数，这和起初残差被提出时的条件恰好相反。所以该论文提出了一个可以解决这些问题的转换方法：特别是上面提出的双线性加性上采样（Bilinear additive upsampleing）方法将输入转化为所需的空间大小和所需的通道数而无需提供任何额外的参数。其转化的特征包含了原始特征的很多信息。因此，可以使用这种转换方法（不进行额外的卷积）进行转换，然后将转换结果输入到任何上采样层的输出中作为下一个上采样层的输入，从而形成类似残差的连接：</p><p><img src="./src/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs/image-20210502114838945.png" alt="image-20210502114838945"/></p><p>上图是对这种方法进行的图形化解释。在后面的内容中，这篇论文通过实验证明了这种方法的有效性。</p><h2>实验和实验设置（Task and experimental setups）</h2><p>实验部分请查看<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原论文</a>。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey]]></title>
        <id>Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</id>
        <link href="https://ml.akasaki.space/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇神经对抗的综述文章，非常非常非常详细的介绍了当前神经对抗攻击的发展情况和已有的攻击和防御算法。原论文：Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey]]></summary>
        <content type="html"><![CDATA[<p>这是一篇神经对抗的综述文章，非常非常非常详细的介绍了当前神经对抗攻击的发展情况和已有的攻击和防御算法。原论文：<a href="https://arxiv.org/pdf/1801.00553.pdf">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></p><blockquote><p>Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.</p></blockquote><p>本文主要对文章进行翻译，还加入了个人对一些算法的理解与解释。这篇文章我大概看了一个星期。真的是一篇非常不错的综述论文。</p><hr/><p>最近的研究表明，它们很容易受到对抗性攻击，因为输入的细微扰动会导致模型预测不正确的输出。 对于图像来说，这样的扰动往往太小而难以察觉，但它们完全欺骗了深度学习模型</p><h2>Inroduction</h2><p>尽管深度学习以非凡的准确性执行各种各样的计算机视觉任务，Szegedy等人<!-- -->[22]<!-- -->首先发现了深度神经网络在图像分类背景下的一个有趣的弱点。他们发现，尽管现代深度网络的准确性很高，但却令人惊讶地容易受到对抗性攻击，这种攻击的形式是对人类视觉系统(几乎)察觉不到的图像的微小干扰。</p><p>即使是3d打印现实世界的物体也可以欺骗深度神经网络分类器</p><p>在第2节中，我们首先用计算机视觉的说法描述了与对抗性攻击有关的常见术语。在第3节，我们回顾了对抗性攻击任务的图像分类和超越。一个单独的部分是奉献给处理对抗攻击的方法在现实世界的条件。第4节将审查这些方法。在文献中，也有主要分析对抗性攻击存在性的著作。我们将在第5节讨论这些贡献。防御对手攻击的方法是</p><h2>在adversarial attack中的常用术语</h2><ul><li><em>Adversarial example/image</em> ：被故意扰乱的干净图像的修改版本.</li><li><em>Threat model</em> ：指一种方法所考虑的潜在攻击类型，例如黑盒攻击.</li><li><em>Universal perturbation</em> : 可以大概率干扰任何一张图片识别结果的干扰</li><li><em>White-box attacks</em> ：在知道目标模型的完整知识，包括它的参数值、架构、训练方法，在某些情况下还有它的训练数据的情况下进行攻击.</li><li><em>Adversarial perturbation</em> ：被加在原图像上的噪声使原图像成为对抗样本</li><li><em>Adversarial training</em> ：使用对抗图像样本来进行训练</li><li><em>Adversary</em> ：一般指创造对抗样本的人，有时候样本本身也叫做这个</li><li><em>Black-box attacks</em> ：测试过程中，在不了解该模型的情况下，为目标模型提供生成的实例的攻击.</li><li><em>Fooling ratio/rate</em> ：表示经过训练的模型在图像被扰动后改变其预测标签的百分比</li><li><em>One-shot/one-step methods</em> ：通过执行一步计算产生一个对抗性的扰动，如一次计算模型损失梯度。与之相反的是迭代方法，即多次执行相同的计算以获得单个扰动。</li><li><em>Quasi-imperceptible</em> ：对于人类的感知来说，干扰对图像的影响非常轻微。</li><li><em>Rectifier</em> ：修改一个对抗性的示例，以将目标模型的预测恢复为同一示例的干净版本上的预测。</li><li><em>Targeted attacks</em> ：骗过一个模型，让它错误地预测出敌对形象的特定标签。</li></ul><h2>3. 神经对抗攻击</h2><p>第三节中论文主要分为两大部分来进行介绍</p><p>在第3.1部分中，我们回顾了攻击深度神经网络的方法，这些方法执行计算机视觉中最常见的任务，即分类/识别。在第3.2部分中讨论了主要用于攻击深度学习的方法。</p><h3>3.1 Attacks for classification</h3><h4>3.1.1 <em>Box-constrained L-BFGS</em></h4><p> Szegedy等人首先证明了图像存在小扰动，如扰动图像可以欺骗深度学习模型，使其误分类。设Ic∈Rm表示一个向量化的干净图像——下标‘c’强调该图像是干净的。为了计算一个可加性扰动ρ∈Rm，它会对图像产生非常轻微的扭曲，从而欺骗网络，Szegedy等人提出解决问题的公式:
$$
\min <em>{\boldsymbol{\rho}}<!-- -->|<!-- -->\boldsymbol{\rho}<!-- -->|</em>{2} \text { s.t. } \mathcal{C}\left(\mathbf{I}<em>{c}+\boldsymbol{\rho}\right)=\ell ; \mathbf{I}</em>{c}+\boldsymbol{\rho} \in<!-- -->[0,1]<!-- -->^{m}
$$
其中$&#x27; \ell^{\prime}$为图像的标签，C(.)为深度神经网络分类器。在这种情况下，上式成为一个困难的问题。因此他们利用box-constrained的L-BFGS寻求一个近似解。这是通过找到最小c &gt; 0来实现的，对于它，下面问题的最小ρ满足条件$\mathcal{C}\left(\mathbf{I}<em>{c}+\boldsymbol{\rho}\right)=\ell$:
$$
\min </em>{\boldsymbol{\rho}} c|\boldsymbol{\rho}|+\mathcal{L}\left(\mathbf{I}<em>{c}+\boldsymbol{\rho}, \ell\right) \text { s.t. } \mathbf{I}</em>{c}+\boldsymbol{\rho} \in<!-- -->[0,1]<!-- -->^{m}
$$
$\mathcal{L}(., .)$计算分类器的损失。我们注意到上式使得具有凸损失函数的分类器具有精确的结果。然而，对于深度神经网络来说，情况通常不是这样。计算的扰动只是添加到图像，使它成为一个对抗的例子。</p><p>上述方法能够计算当将噪声添加到干净图像时对神经网络的扰动，但是对抗性图像看起来与人类视觉系统的干净图像相似。Szegedy等人观察到一个神经网络计算的扰动也能够欺骗多个网络。这些惊人的结果发现了深度学习中的一个盲点。在这个发现的时候，计算机界正在迅速适应这样一种印象：深度学习特征定义了一个空间，在这个空间中，感知距离可以很好地近似于欧几里德距离。因此，这些相互矛盾的结果引发了研究人员对计算机视觉深度学习对抗性攻击的广泛兴趣。</p><hr/><h5>补充说明：L-BFGS算法</h5><p>参考博客链接：<a href="https://blog.csdn.net/weixin_39445556/article/details/84502260">https://blog.csdn.net/weixin_39445556/article/details/84502260</a></p><p>我们知道算法在计算机中运行的时候是需要很大的内存空间的.就像我们解决函数最优化问题常用的梯度下降,它背后的原理就是依据了泰勒一次展开式.泰勒展开式展开的次数越多,结果越精确,没有使用三阶四阶或者更高阶展开式的原因就是目前硬件内存不足以存储计算过程中演变出来更复杂体积更庞大的矩阵.L-BFGS算法翻译过来就是有限内存中进行BFGS算法,L是limited memory的意思.</p><p>算法为什么叫做BFGS呢？这就是四个数学家名字的简称而已，不用过多的在意</p><p>学习BFGS必须要先了解牛顿法的求根问题.</p><p><strong>牛顿法求根问题</strong></p><p> 牛顿发现,一个函数的根从物理的角度就可以根据函数图像迭代求得.牛顿法求根的思路是:</p><p>​    a.在X轴上随机一点x1,经过x1做X轴的垂线,得到垂线与函数图像的交点f（x1）</p><p>​    b.通过f（x1）做函数的切线,得到切线与X轴的交点x2</p><p>​    c.迭代a/b两步.</p><p>​    下面附上一张动图方便理解:</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/70-20210501181806130.gif"/></p><p>​	通过图片我们可以看到.在X轴上取的点会随着迭代次数的增加而越来越接近函数的根.经过无限多次的迭代$x<em>n$,就等于函数f(x)的根.但牛顿法在实际应用的时候我们不会让算法就这么迭代下去,所以当$x</em>{k-1}$和$x<em>{k}$相同或者两个值的差小于一个阈值的时候,$x</em>{k}$就是函数$f(x)$的根.</p><p>​    那么问题来了,怎么样找到$f(x)$的导函数与X轴的交点.请看下图:</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806068.png"/></p><p>图片是上边动图从$x<em>{1}$到$x</em>{2}$的动作.可以看到,三角形绿色的直角边的值是$x<em>{1} - x</em>{2}$ ,  $x<em>{1}$是已知的(随机出来的),而且函数表达式f(x)也是已知的(不知道要求的函数咱们折腾啥呢).所以三角形的另一条直角边$f(x</em>{1})$也是已知的.根据导函数定义,函数f(x)在$x<em>{1}$ 点的导函数就是$f^{&#x27;}(x) = \frac{f(x</em>{1})}{x<em>{1} - x</em>{2}}$ (公式一).</p><p>公式一变换一下得到: $x<em>{2} =x</em>{1} - \frac{ f(x<em>{1})}{f^{&#x27;}(x</em>{1})}$ (公式二 ),同理可以得出每一次迭代$x<em>{k}$的表达式都是 $x</em>{k} =x<em>{k-1} - \frac{ f(x</em>{k-1})}{f^{&#x27;}(x_{k-1})}$ (公式三).</p><p>所以,根据牛顿法求根的思路,我们可以总结(模仿)一下使用牛顿法求根的步骤:</p><p>a.已知函数f(x)的情况下,随机产生点$x_{0}$.</p><p>b.由已知的$x<em>{0}$点按照$x</em>{k} =x<em>{k-1} - \frac{ f(x</em>{k-1})}{f^{&#x27;}(x_{k-1})}$的公式进行k次迭代.</p><p>c.如果$x<em>{k}$与上一次的迭代结果$x</em>{k-1}$相同或者相差结果小于一个阈值时,本次结果$x_{k}$就是函数f(x)的根.</p><p>以上为牛顿法的求根的思路.</p><p><strong>牛顿法求函数的驻点</strong></p><p>我们知道,机器学习过程中的函数最优化问题,大部分优化的都是目标函数的导函数,我们要求得导函数为零时的点或近似点来作为机器学习算法表现最好的点.现在我们知道了牛顿求根法,那把牛顿求根法的函数换成咱们要优化的导函数不就行了么.要求的的导函数为零的点叫做驻点.所以用牛顿法求函数驻点同求函数的根是没有任何区别的.只是公式二中的$f(x)$变为了$f^{&#x27;}(x)$,原来的$f^{&#x27;}(x)$变成了一阶导函数$f^{&#x27;}(x)$的导函数也就是二阶导函数$f^{&#x27;&#x27;}(x)$,求$x_{k}$的迭代公式如下:</p><p>$x<em>{k} = x</em>{k-1} - \frac{f^{&#x27;}(x<em>{k-1})}{f^{&#x27;&#x27;}(x</em>{k-1})}$    (公式四)</p><p>这样,我们通过几何直觉,得到了求解函数根的办法,那这么厉害的一个想法,有没有什么理论依据作为支撑呢?当然有了,要不我也不这么问.</p><p><strong>牛顿法求驻点的本质</strong></p><p>牛顿法求驻点的本质其实是二阶泰勒展开.我们来看二阶泰勒展开式:</p><p>$$
\varphi(x)=f\left(x<em>{k}\right)+f^{\prime}\left(x</em>{k}\right)\left(x-x<em>{k}\right)+\frac{1}{2} f^{\prime \prime}\left(x</em>{k}\right)\left(x-x_{k}\right)^{2}
$$</p><p>$\varphi (x)$是我们要求解的原函数的近似式.当我们要对原函数求解时,可以直接求得$\varphi (x)$的导函数$\varphi^{&#x27;} (x)$ 令$\varphi^{&#x27;} (x) = 0$时的结果就是原函数的解.所以对$\varphi (x)$求导,消去常数项后得到公式如下:</p><p>$$
f^{\prime}\left(x<em>{k}\right)+f^{\prime \prime}\left(x</em>{k}\right)\left(x-x_{k}\right)=0
$$</p><p>经过变换后所得的公式就是上边的<strong>公式四</strong>.所以,牛顿法求驻点的本质就是对函数进行二阶泰勒展开后变换所得到的结果.</p><p>在一元函数求解的问题中,我们可以很愉快的使用牛顿法求驻点,但我们知道,在机器学习的优化问题中,我们要优化的都是多元函数,所以x往往不是一个实数,而是一个向量.所以将牛顿求根法利用到机器学习中时,x是一个向量,$f^{&#x27;}(x)$也是一个向量,但是对$f^{&#x27;}(x)$求导以后得到的$f^{&#x27;&#x27;}(x)$是一个矩阵,叫做<strong>Hessian矩阵</strong>.等价公式如下:</p><p>$$
\mathbf{x}<em>{k+1}=\mathbf{x}</em>{k}-H<em>{k}^{-1} \cdot \mathbf{g}</em>{k}, \quad k=0,1, \cdots
$$</p><p>公式中,$g<em>{k}$为公式四中的$f^{&#x27;}(x),H</em>{k}^{-1}$代表二阶导函数的倒数.</p><p>当x的维度特别多的时候,我们想求得$f^{&#x27;&#x27;}(x)$是非常困难的.而牛顿法求驻点又是一个迭代算法,所以这个困难我们还要面临无限多次,导致了牛顿法求驻点在机器学习中无法使用.有没有什么解决办法呢?</p><p><strong>BFGS算法</strong></p><p>BFGS算法是通过迭代来逼近$H_{k}^{-1}$的算法.逼近的方式如下(公式五):</p><p>$$
D<em>{k+1}=\left(I-\frac{\varepsilon</em>{k} \mathbf{y}<em>{k}^{T}}{\mathbf{y}</em>{k}^{T} \mathbf{s}<em>{k}}\right) D</em>{k}\left(I-\frac{\mathbf{y}<em>{\mathbf{s}} \mathbf{s}</em>{k}^{T}}{\mathbf{y}<em>{k}^{T} \mathbf{s}</em>{k}}\right)+\frac{\mathbf{s}<em>{k} \mathbf{s}^{T}}{\mathbf{y}</em>{k}^{T} \mathbf{s}<em>{k}}
$$
公式五中的 $D</em>{k}$就是$H<em>{k}^{-1}.s</em>{k} = x<em>{k+1} - x</em>{k},y<em>{k} = g</em>{k+1} - g<em>{k}$.  $g</em>{k}$是原函数的导函数.</p><p>$D<em>{k}$的迭代公式复杂无比,还好我们不用记住它.BFGS是通过迭代来逼近$H</em>{k}^{-1}$矩阵,第一步的D矩阵是单位矩阵.</p><p>我们要通过牛顿求驻点法和BFGS算法来求得一个函数的根,两个算法都需要迭代,所以我们干脆让他俩一起迭代就好了.两个算法都是慢慢逼近函数根,所以经过k次迭代以后,所得到的解就是机器学习中目标函数导函数的根.这种两个算法共同迭代的计算方式,我们称之为On The Fly.个人翻译:让子弹飞~</p><p>回顾一下梯度下降的表达式$\Theta<em>{k} = \Theta</em>{k+1} - \alpha \cdot g$ ,在BFGS算法迭代的第一步,单位矩阵与梯度g相乘,就等于梯度g,形式上同梯度下降的表达式是相同的.所以BFGS算法可以理解为从梯度下降逐步转换为牛顿法求函数解的一个算法.</p><p>虽然我们使用了BFGS算法来利用单位矩阵逐步逼近H矩阵,但是每次计算的时候都要存储D矩阵,D矩阵有多大呢.假设我们的数据集有十万个维度(不算特别大),那么每次迭代所要存储D矩阵的结果是74.5GB.我们无法保存如此巨大的矩阵内容,如何解决呢?</p><p><strong>使用L-BFGS算法.</strong></p><p><strong>L-BFGS算法:</strong></p><p>我们每一次对D矩阵的迭代,都是通过迭代计算$s<em>{k}$和$y</em>{k}$得到的.既然存不下D矩阵,那么我们存储下所有的$s<em>{k}$和$y</em>{k}$想要得到$D<em>{10}$就用单位矩阵同存储下的$s</em>{1}$和$y<em>{1}$到$s</em>{10}$和$y_{10}$计算就可以了.这样一个时间换空间的办法可以让我们在数据集有10000个维度的情况下,由存储10000 x 10000的矩阵变为了存储十个1 x 10000的10个向量,有效节省了内存空间.</p><p>但是,仅仅是这样还是不够的,因为当迭代次数非常大的时候,我们的内存同样存不下.这个时候只能丢掉一些存不下的数据.假设我们设置的存储向量数为100,当s和y迭代超过100时,就会扔掉第一个s和y,存储s<em>{2}到s</em>{101}和y<em>{2}到y</em>{101},每多一次迭代就对应的扔掉最前边的s和y.这样虽然损失了精度,但确可以保证使用有限的内存将函数的解通过BFGS算法求得到.</p><p><strong>所以L-BFGS算法可以理解为对BFGS算法的又一次近似.</strong></p><hr/><h4><em>3.1.2 Fast Gradient Sign Method (FGSM)</em></h4><p>FGSM是一种白盒攻击；</p><p>Szegedy等人<!-- -->[22]<!-- -->观察到，对抗性训练可以提高深层神经网络对对抗性例子的鲁棒性。（对抗性训练就是指将生成的对抗性样本加入到训练集当中来进行进一步的训练）</p><p>为了实现有效的对抗性训练，Goodfello等人<!-- -->[23]<!-- -->开发了通过解决以下问题，有效计算给定图像的对抗性扰动的方法：
$$
\rho=\epsilon \operatorname{sign}\left(\nabla \mathcal{J}\left(\boldsymbol{\theta}, \mathbf{I}_{c}, \ell\right)\right)
$$
$\nabla \mathcal{J}(.,.,.)$计算围绕模型参数当前值的成本函数的梯度;$\epsilon$是限制微扰范数的小标量值。这种方法叫做FGSM（Fast Gradient Sign Method）算法。</p><p>有趣的是，FGSM产生的对抗性例子利用了高维空间中深层网络模型的“线性”，而这种模型当时通常被认为是高度非线性的。古德费罗等人<!-- -->[23]<!-- -->假设，现代深层神经网络的设计（有意）为计算增益引入线性行为，也使它们容易受到廉价的分析扰动。在相关文献中，这一观点通常被称为“线性假设”，这一点得到了FGSM方法的证实。</p><p>FGSM对图像进行扰动，以增加分类器在结果上的损失。sign函数保证了损失大小最大化，而ε本质上限制了$\ell<em>{\infty}$ -norm的扰动。Miyato等人<!-- -->[103]<!-- -->提出的计算方法，如下所示：
$$
\boldsymbol{\rho}=\epsilon \frac{\nabla \mathcal{J}\left(\boldsymbol{\theta}, \mathbf{I}</em>{c}, \ell\right)}{\left<!-- -->|<!-- -->\nabla \mathcal{J}\left(\boldsymbol{\theta}, \mathbf{I}<em>{c}, \ell\right)\right<!-- -->|</em>{2}}
$$
在上面的方程中，计算的梯度用$\ell<em>2$范数正则化。Kurakin等人<!-- -->[80]<!-- -->将该技术称为&quot;快速梯度L2&quot;方法，并提出了使用$\ell</em>{\infty}$范数进行归一化的替代方法，这个方法被叫做“快速梯度$\ell_{\infty}$”方法。从广义上讲，在计算机视觉对抗性攻击的相关文献中，所有这些方法都被叫做 ‘one-step’ or ‘one-shot’ “一步式”或“一次性”方法。</p><hr/><p>补充说明：FGSM算法 </p><p>参考文章： <a href="https://cloud.tencent.com/developer/article/1167792">干货 | 攻击AI模型之FGSM算法</a></p><p>FGSM最早由Goodfellow在其论文《Explaining and Harnessing Adversarial Examples》<!-- -->[23]<!-- --> 中提出。以最常见的图像识别为例，我们希望在原始图片上做肉眼难以识别的修改，但是却可以让图像识别模型产生误判。假设图片原始数据为x，图片识别的结果为y，原始图像上细微的变化肉眼难以识别，使用数学公式表示如下。
$$
\widetilde{x}=x+\eta
$$
将修改后的图像输入分类模型中，x与参数矩阵相乘。
$$
w^{T} \widetilde{x}=w^{T} x+w^{T} \eta
$$
对分类结果的影响还要受到激活函数的作用，攻击样本的生成过程就是追求以微小的修改，通过激活函数的作用，对分类结果产生最大化的变化。Goodfellow指出，<strong>如果我们的变化量与梯度的变化方向完全一致</strong>，那么将会对分类结果产生最大化的变化。</p><p>$$
\eta=\operatorname{\epsilon sign}(\operatorname{grad}(w, x, y))
$$
其中sign函数可以保证变化量与梯度函数方向一致。</p><p>当x的维数为n时，模型的参数在每个维度的平均值为m，每个维度的微小修改与梯度函数方向一致，累计的效果为：
$$
mn \epsilon
$$
可见当原始数据的维度越大，攻击的累计效果越明显。以一个更加直观的例子来说明FGSM的原理。</p><p>假设具有2000个样本，每个数据具有1000维，每维的数据的数值的大小都在0-1之间随机生成，分类标签只有2种。</p><pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import *
print(tf.__version__)
print(tf.test.is_gpu_available())
import sklearn
from sklearn.preprocessing import MinMaxScaler

# feather number
n_features = 1000
x,y=datasets.make_classification(n_samples=2000,
                                 n_features=n_features,
                                 n_classes=2,
                                 random_state=random_state)
# Standardized to 0-1
x = MinMaxScaler().fit_transform(x)
</code></pre><p>分类模型是一个非常简单的神经网络，输入层大小为1000，输出层为1，激活函数为sigmoid。</p><pre><code class="language-python">model = tf.keras.models.Sequential([layers.Dense(1,activation=&#x27;sigmoid&#x27;)])
</code></pre><p>sigmoid函数是非常经典的激活函数，取值范围为0-1，特别适合表示概率分布。</p><p>损失函数使用最简单的mse，优化方式使用adam，考核的指标为准确度accuracy。</p><pre><code class="language-python">model.compile(loss=&#x27;mse&#x27;,optimizer=&#x27;adam&#x27;,metrics=[&#x27;accuracy&#x27;])
model.fit(  #使用model.fit()方法来执行训练过程，
    x, y, #告知训练集的输入以及标签，
    batch_size = 16, #每一批batch的大小为32，
    epochs = 50, #迭代次数epochs为500
    validation_split = 0.2, #从测试集中划分80%给训练集
    validation_freq = 10 #测试的间隔次数为20
)
</code></pre><p>批处理大小为16，经过50轮训练。</p><pre><code class="language-python">model.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=100)
</code></pre><p>最终训练结果，损失值稳定在0.015左右，准确度为70% 左右；</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806123.png" alt="在这里插入图片描述"/></p><pre><code class="language-python">model.summary()
</code></pre><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181805986.png" alt="在这里插入图片描述"/></p><p>由于数据是随机生成的，我们取1号举例，可以看到标签是0，结果也是很接近1的</p><pre><code class="language-python">import numpy as np
x1 = x[1]
y1 = y[1]
x1 = np.expand_dims(x1,axis=0)
y1_predict = model.predict(x1)
print(y1)
print(y1_predict)

Out:
0
[[0.15540801]]
</code></pre><p>根据公式：
$$
\eta=\operatorname{\epsilon sign}(\operatorname{grad}(w, x, y))
$$
我们知道下一步需要计算：模型在x1处的梯度：</p><pre><code class="language-python">x1 = tf.convert_to_tensor(x1)
with tf.GradientTape(persistent=True) as g:
  g.watch(x1)
  y = model(x1)
gradient = g.gradient(y, x1) 
</code></pre><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806077.png" alt="在这里插入图片描述"/></p><p>接下来跟着公式进行计算即可：</p><p>e我们取0.1 可以认为对原来改变的很小</p><pre><code class="language-python">e = 0.1
n = np.sign(gradient)
x_ = x1 + n * e
print(model(x_))
</code></pre><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806027.png" alt="在这里插入图片描述"/></p><p>可以看到预测的结果完全变了！</p><hr/><h4><em>Basic &amp; Least-Likely-Class Iterative Methods</em></h4><p>one step方法通过在的方向上迈出一大步（即一步梯度上升）增加分类器的损失来扰动图像。这个想法的一个直观扩展是迭代地采取多个小步骤同时调整每一步后的方向。基本迭代法（BIM）<!-- -->[35]<!-- -->正是这样做的，并且迭代计算如下：
$$
\mathbf{I}<em>{\rho}^{i+1}=\operatorname{Clip}</em>{\epsilon}\left<!-- -->{<!-- -->\mathbf{I}<em>{\rho}^{i}+\alpha \operatorname{sign}\left(\nabla \mathcal{J}\left(\boldsymbol{\theta}, \mathbf{I}</em>{\rho}^{i}, \ell\right)\right<!-- -->}<!-- -->\right.
$$
$\mathbf{I}_{\rho}^{i}$表示第i次迭代时的扰动图像，Clip 就是剪掉变化过大的部分；</p><p>BIM算法从$\mathbf{I}<em>{\rho}^{0}=\mathbf{I}</em>{c}$开始，按照公式$\lfloor\min (\epsilon+4,1.25 \epsilon)\rfloor$<strong>确定的迭代次数运行</strong>。Madry等人<!-- -->[55]<!-- -->指出，BIM等价于投影梯度下降（PGD）的$\ell_{\infty}$版本，PGD是一种标准的凸优化方法。</p><p>与将FGSM扩展到 “<strong>一步目标类</strong>” 变体类似（通过迭代的方式使得图像的识别趋近于某一个类别），Kurakin等人<!-- -->[35]<!-- -->也将BIM扩展到迭代最小可能类方法（ILCM）。在这种情况下，用分类器预测的<strong>最不可能类的目标标</strong>签$l_{target}$替换中的图像的标签$l$。</p><p>ILCM方法生成的对抗性示例已被证明严重影响了分类的精度，即使ε的值非常小，例如&lt;16。</p><p>ILCM的实现过程可以说与FGSM如出一辙，区别就在于迭代的次数和诱导分类器预测的目标标签不同；</p><h4><em>3.1.4 Jacobian-based Saliency Map Attack (JSMA)</em></h4><p>目前通过限制$\ell<em>{\infty}$ 或者 $\ell</em>{2}$ -norms来实现对网络扰动的情况比较常见；Papernot等人<!-- -->[60]<!-- -->也通过限制扰动的$l_0$范数来制造对抗性攻击。这意味着目标是只修改图像中的几个像素，而不是干扰整个图像来欺骗分类器。他们生成所需对抗图像的算法关键如下:</p><ul><li>该算法一次修改一个干净图像的像素，并监控变化对结果分类的影响。</li><li>通过使用网络层的输出的梯度计算显著性图(computing a saliency map)来执行监视。</li></ul><p>在map中，较大的值表示可以成功愚弄网络将$l_{target}$预测为&quot;修改图像&quot;的标签而不是原始标签$l$的可能性较高。一旦map被计算出来，算法就会选择最有效的像素来欺骗网络并改变它。重复这个过程，直到敌对图像中允许的最大像素数被改变，或者愚弄成功。</p><h4><em>3.1.5 One Pixel Attack</em></h4><p>对抗攻击的一个极端情况是，只改变图像中的一个像素以欺骗分类器。有趣的是，Su等人<!-- -->[68]<!-- -->声称，在70.97%的测试图像上，通过改变每幅图像的一个像素，成功地愚弄了三种不同的网络模型。他们还报告说，网络对错误标签的平均置信度为97.47%。</p><p>Su等人利用（Differential Evolution）差异进化的概念计算了对抗性例子<!-- -->[148]<!-- -->。对于一个干净的图像，他们首先在$\mathbb{R}^{5}$中创建了一组400个向量，使得每个向量包含xy坐标；然后给这些向量随机的RGB。</p><p>然后，他们随机修改向量的元素来创建子对象，使得子对象在下一次迭代中与其父对象竞争适应度（fitness criterion），同时使用网络的概率预测标签作为适应度准则。最后幸存的子对象用于改变图像中的像素。</p><p>即使这样一个简单的进化策略，Su等人<!-- -->[68]<!-- -->也能成功地愚弄了深层网络。注意，差分进化使他们的方法能够产生对抗性的例子，而不需要获得任何关于网络参数值或其梯度的信息。他们的技术需要的唯一输入是目标模型预测的概率标签。让就是说这种攻击的方法属于<strong>黑盒攻击</strong>；</p><h4><em>3.1.6 Carlini and Wagner Attacks (C&amp;W)</em></h4><p>卡里尼和瓦格纳<!-- -->[36]<!-- -->提出了一系列的三次对抗性攻击，这是在对抗性干扰<!-- -->[38]<!-- -->的防御升华之后提出的。这些攻击通过限制扰动的$\ell<em>{2}$、$\ell</em>{\infty}$和$\ell_{0}$范数使扰动具有准不可察觉性，并且证明了针对目标网络的防御蒸馏几乎完全不能抵抗这些攻击。此外，本文还证明了利用不安全（未蒸馏）网络生成的对抗性例子可以很好地转移到安全（蒸馏）网络，这使得计算出的扰动适合于黑盒攻击。</p><p>然而更常见的是利用对抗性例子的可转移性来生成黑盒攻击，Chen等人<!-- -->[41]<!-- -->还提出了基于“零阶优化（ZOO）”的攻击，直接估计目标模型的梯度来生成对抗性例子。这些攻击的灵感来自C&amp;W攻击。</p><hr/><p>补充说明：C&amp;W攻击</p><h4><em>3.1.7 DeepFool</em></h4><p>Moosavi-dezfouli等人<!-- -->[72]<!-- -->提出了一个迭代计算最小范数对抗性扰动的方法。DeepFool用干净的图像初始化，该图像被假定位于由分类器的决策边界限定的区域中。这个区域决定了图像的类标签。At each iteration, the algorithm perturbs the image by a small vector that is computed to take the resulting image to the boundary of the polyhydron that is obtained by linearizing the boundaries of the region within which the image resides.每次迭代中添加到图像上的扰动被累加，一旦扰动图像根据网络的原始决策界改变其标签，就计算出最终的扰动。作者证明了DeepFool算法能够计算出比FGSM<!-- -->[23]<!-- -->计算的扰动范数更小的扰动，同时具有相似的愚弄率。</p><p>实现过程与FGSM也是比较类似的；</p><p>下面我们看一下Deep Fool算法和FGSM算法去攻击同一个图像修改量的对比图：</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806643.png"/></p><p>可以看到Deep Fool的修改量明显的要小于FGSM；</p><p>除了迭代环节，DeepFool与FGSM的算法完全相同。在迭代环节，我们可以通过NumPy的inalg.norm函数对梯度进行处理，然后迭代更新图片内容。</p><pre><code class="language-python"># Deep Fool 攻击代码的迭代
while cost &lt; 0.6:
  cost,gradients = grab_cost_and_gradients_from_model([hacked_image,0])
  r= gradients*cost/pow(np.linalg.norm(gradients), 2)
  hacked_image +=r
  hacked_image = np.clip(hacked_image, max_change_below, max_change_above)
  hacked_image = np.clip(hacked_image, -1.0, 1.0)
  
# FGSM算法的迭代

while cost &lt; 0.60:
    cost, gradients = grab_cost_and_gradients_from_model([hacked_image, 0])
    n=np.sign(gradients)
    hacked_image +=n*e
    hacked_image = np.clip(hacked_image, max_change_below, max_change_above)
    hacked_image = np.clip(hacked_image, -1.0, 1.0)
  
</code></pre><h4><em>3.1.8 Universal Adversarial Perturbations</em></h4><p>​	然而像FGSM<!-- -->[23]<!-- -->、ILCM<!-- -->[35]<!-- -->、Deep-Fool<!-- -->[72]<!-- -->等方法计算扰动来愚弄单个图像上的网络，Moosavi Dezfouli等人<!-- -->[16]<!-- -->计算的“普遍”对抗扰动能够以高概率愚弄“任何”网络。如图1所示，这些图像不可知的扰动对于人类视觉系统来说仍然是准不可察觉的。为了正式定义这些扰动，让我们假设干净的图像是从分布$\Im<em>{c}$采样的。如果扰动ρ满足以下约束条件，则它是“普适的”：
$$
\underset{\mathbf{I}</em>{c} \sim \Im<em>{c}}{\mathrm{P}}\left(\mathcal{C}\left(\mathbf{I}</em>{c}\right) \neq \mathcal{C}\left(\mathbf{I}<em>{c}+\boldsymbol{\rho}\right)\right) \geq \delta \text { s.t. }<!-- -->|<!-- -->\boldsymbol{\rho}<!-- -->|</em>{p} \leq \xi
$$
其中$P(.)$表示概率，$\delta \in(0,1]$表示愚弄比率，$<!-- -->|<!-- -->.<!-- -->|<em>{p}$表示$\ell</em>{p}$范数，$\xi,$是预定义的参数。$\xi,$值越小，就越难察觉图像中的扰动。严格地说，满足上述公式的扰动应称为$(\delta, \xi)$ -universal ，因为它们对上述参数有很强的依赖性。然而，这些扰动在文献中通常被称为普遍的对抗性扰动（universal adversarial pertur- bations）。</p><p>作者通过限制 $\ell<em>{2}$范数和$\ell</em>{\infty}$范数计算了universal perturbations ，结果表明，对于最先进的图像分类器，其范数上界为各自图像范数的4%的扰动，这样的扰动已经达到了约0.8或着更高的愚弄率。他们计算扰动的迭代方法与DeepFool策略<!-- -->[72]<!-- -->有关，该策略将数据点（即图像）逐渐推到其类的决策边界。然而，在这种情况下，“所有”训练数据点被依次推到各自的决策边界，并且通过每次将累加器向后投影到半径 $\xi$的所需$\ell_{p}$球，在所有图像上计算的扰动被逐渐累积。</p><p>Moosavi Dezfouli等人<!-- -->[16]<!-- -->提出的算法在针对单个网络模型时计算扰动，例如ResNet<!-- -->[147]<!-- -->。这些扰动也可以很好地推广到不同的网络（特别是具有相似结构的网络）。从这个意义上说，作者声称扰动在某种程度上是“<strong>双重普遍的</strong>”。此外，高愚弄率只使用大约2000张训练图像来学习扰动就可以实现了。</p><p>Khrulkov等人<!-- -->[190]<!-- -->还提出了一种将普遍对抗性扰动构造为网络特征映射的雅可比矩阵奇异向量的方法，这种方法允许仅使用少量图像获得相对较高的欺骗率。另一种产生普遍扰动的方法是Mopuri等人<!-- -->[135]<!-- -->提出的快速特征愚弄。他们的方法产生了与数据无关的普遍扰动。</p><h4><em>3.1.9 UPSET and ANGRI</em></h4><p>​	Sarkar等人<!-- -->[146]<!-- -->提出了两种黑盒攻击算法，即 UPSET: Universal Perturbations for Steering to Exact Targets, 和ANGRI: Antagonistic Network for Generating Rogue Images 有针对性地愚弄深层神经网络。对于“n”类，UPSET寻求产生“n”图像不可知扰动，这样当扰动被添加到不属于目标类的图像时，分类器将扰动图像分类为来自该类。UPSET的效果来自一个residual generating network $\mathrm{R}(.),$ 它将目标类“t”作为输入，并生成一个扰动$\mathrm{R}(\mathrm{t})$ 以供愚弄。整体方法使用所谓的UPSET- network解决以下优化问题：
$$
\mathbf{I}<em>{\boldsymbol{\rho}}=\max \left(\min \left(s \mathbf{R}(\mathbf{t})+\mathbf{I}</em>{c}, 1\right),-1\right)
$$</p><p>其中，$\mathbf{I}_{c}$中的像素值被标准化在<!-- -->[-1,1]<!-- -->之间，并且$s^{\prime}$是标量. 为确保为有效图像，将剪裁间隔<!-- -->[-1,1]<!-- -->之外的所有值。与图像不可知的UPSET相比，ANGRI以一种密切相关的方式计算图像特定的扰动. ANGRI产生的扰动也被用于有针对性的愚弄。据报道，这两种算法在MNIST<!-- -->[10]<!-- -->和CIFAR-10<!-- -->[152]<!-- -->数据集上都实现了高愚弄率。每一种算法的具体实现我会在接下来的其他文章中逐个进行介绍；</p><h4><em>3.1.10 Houdini</em></h4><p>​	Cisse等人<!-- -->[131]<!-- -->提出了“Houdini”，这是一种通过生成对抗性的例子来愚弄 &quot;基于梯度学习的算法&quot;。对抗性例子可以根据任务损失进行调整。产生对抗性例子的典型算法采用网络可微损失函数的梯度来计算扰动。然而，任务损失通常不适合这种方法。例如，语音识别的任务损失是基于字错误率的，这不允许直接利用损失函数梯度。Houdini是专门为此类任务生成对抗性示例的。除了可以成功生成攻击分类算法的敌对图像外，Houdini还被证明成功攻击了流行的自动语音识别系统<!-- -->[151]<!-- -->. 作者还通过在黑盒攻击场景中愚弄google voice，证明了语音识别中攻击的可转移性。此外，成功的目标攻击和非目标攻击也证明了深度学习模型的人体姿态估计。</p><h4><em>3.1.11 Adversarial Transformation Networks (ATNs)</em></h4><p>​	Baluja和Fischer<!-- -->[42]<!-- -->训练了前馈神经网络（feed-forward neural net- works），用于生成对抗其他目标网络或网络集的对抗性示例。训练网络就叫做Adversarial Transformation Networks (ATNs).  通过最小化由两部分组成的联合损失函数，计算了这些网络产生的对抗性例子; 第一部分限制对抗示例与原始图像具有感知相似性，而第二部分旨在改变目标网络对结果图像的预测。</p><p>沿着相同的方向，Hayex和Danezis<!-- -->[47]<!-- -->还使用attacker neural network 来学习黑匣子攻击的对抗性例子。在给出的结果中，attacker neural network计算的示例与干净的图像在本质上不可区分，但它们被目标网络以压倒性的概率错误分类-在MNIST数据<!-- -->[10]<!-- -->上将分类精度从99.4%降低到0.77%，在CIFAR-10数据集<!-- -->[152]<!-- -->上将分类精度从91.4%降低到6.8%。</p><h4><em>3.1.12 Miscellaneous Attacks</em></h4><p>这里论文里列举的实在太多了，就以论文中的表格来概括吧；</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806370.png" alt="在这里插入图片描述"/></p><p>表格中4星以上的攻击方法，之后应该都会出专门的文章进行学习和介绍的，也包括学习过程中的代码实现；</p><h3><strong>3.2 Attacks beyond classification/recognition</strong></h3><p>​	除了Houdini<!-- -->[131]<!-- -->之外，第3.1节中回顾的所有主流攻击都直接集中在分类任务上——通常愚弄基于CNN的分类网络<!-- -->[10]<!-- -->。然而，由于对抗性威胁的严重性，除了计算机视觉中的分类/识别任务外，攻击也被积极地研究。接下来让我们回顾深层神经网络攻击分类之外算法的工作。</p><h4><em>3.2.1 Attacks on Autoencoders and Generative Models</em></h4><p>Tabacof等人<!-- -->[128]<!-- -->研究了针对autoencoders的对抗性攻击<!-- -->[154]<!-- -->，并提出了一种扭曲输入图像（使其具有对抗性）的技术，这种技术会误导自动编码器重建完全不同的图像。他们的方法攻击神经网络的内部特征，使得攻击图像的特征与目标图像的特征相似。</p><p>然而，据<!-- -->[128]<!-- -->报道，与典型的分类器网络相比，自动编码器似乎对对抗性攻击更具鲁棒性。Kos等人<!-- -->[121]<!-- -->还探索了计算深层生成模型的高级示例的方法，例如变分自动编码器（VAE）和VAE生成对抗网络（VAE-GANs）</p><p>GAN 像<!-- -->[153]<!-- -->这样的现在在计算机视觉领域非常流行，应用程序可以学习数据分布并生成真实的图像. 作者介绍了针对VAE和VAE-GANs的三种不同类型的攻击。由于这些攻击的成功，我们可以得出结论，深层生成模型也能够攻击,这些攻击会使得输入转化为非常不同的输出。这项工作进一步支持了“ 对抗性例子是当前神经网络结构的普遍现象”这一假设。<strong>也就是说当前神经网络普遍会受到对抗性例子的影响；</strong></p><h4><em>3.2.2 Attack on Recurrent Neural Networks</em></h4><p>​	Papernot等人<!-- -->[110]<!-- -->成功地为递归神经网络（RNN）生成了对抗性输入序列。RNN是一种深度学习模型，特别适合学习顺序输入和输出之间的映射<!-- -->[155]<!-- -->。Papernot等人证明，为前向神经网络（如FGSM<!-- -->[23]<!-- -->）计算对抗性示例的算法也可用于欺骗RNN。特别是，作者成功地愚弄了流行的长短时记忆（LSTM）RNN架构<!-- -->[156]<!-- -->。结论是，循环神经网络模型（如RNN）也不能免疫非循环神经网络（即CNN）中最初发现的对抗性扰动. </p><h4><em>3.2.3 Attacks on Deep Reinforcement Learning</em></h4><p>Lin等人<!-- -->[134]<!-- -->提出了两种不同的对抗性攻击，专门用于深度强化学习<!-- -->[157]<!-- -->。第一种攻击，被称为“策略定时攻击”，对手通过在一个事件中的一小部分的时间内进行攻击来使得agent的奖励值最小化。这提出了一种确定何时制作和应用对抗性示例的方法，使攻击不被发现。在第二种攻击中，称为‘enchanting attack’（附魔攻击），对手通过集成生成模型和规划算法将agent（智能体）引诱到指定的目标状态。生成模型用于预测agent的未来状态，而规划算法用于生成引诱agent的行为。针对由最先进的深度强化学习算法<!-- -->[157]<!-- -->、<!-- -->[158]<!-- -->训练的代理，成功地测试了这些攻击。</p><p>在另一项研究中，Huang等人<!-- -->[62]<!-- -->证明，FGSM<!-- -->[23]<!-- -->也可用于在深度强化学习的背景下显著降低模型的准确度。</p><h4><em>3.2.4 Attacks on Semantic Segmentation and Object De- tection</em></h4><p>在Moosavi Dezfouli<!-- -->[16]<!-- -->的启发下，Metzen等人<!-- -->[67]<!-- -->证明了图像不可知伦中准不可察觉pertur-bations的存在，这种pertur-bations可以欺骗深层神经网络，显著破坏图像的预测分割。此外，他们还表明，可以计算噪声向量，在保持大部分图像分割不变的情况下（例如，从道路场景中移除行人），从分割的类中移除特定的类。</p><p>虽然有人认为“用于语义图像分割的高级扰动空间可能比图像分类的小”，但这种扰动对于不可见的验证图像具有很高的概率去推广。Arnab等人<!-- -->[51]<!-- -->还评估了基于FGSM<!-- -->[23]<!-- -->的用于语义分割的对抗性攻击，并指出许多关于这些用于分类的攻击的观察结果并没有直接转移到分割任务。</p><p>Xie等人<!-- -->[115]<!-- -->计算了用于语义分割和目标检测的对抗性示例，观察到这些任务可以表示为对图像中的多个目标进行分类-目标是分割中的一个像素或一个感受野，以及检测中的目标建议。</p><p>在这种观点下，他们的方法称为“密集敌方生成”（Dense Adversary Generation），它优化了一组像素/方案上的损失函数，以生成敌方示例。生成的例子被测试来愚弄各种基于深度学习的分割和检测方法。他们的实验评估不仅成功地愚弄了目标网络，而且还表明所产生的扰动在不同的网络模型中具有良好的通用性。在下图中，展示了使用<!-- -->[115]<!-- -->中的方法进行分割和检测的网络欺骗的代表性示例。</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806647.png"/></p><h2>4. ATTACKS IN THE REAL WORLD</h2><h3><em>4.0.1 Attacks on Face Attributes</em></h3><p>人脸属性是现代安全系统中新兴的软生物特征识别技术之一。虽然人脸属性识别也可以被归类为一个分类问题，但由于人脸识别本身被视为计算机视觉中的一个主流问题，因此我们分别回顾了这方面的一些有趣的攻击。</p><p>Rozsa等人<!-- -->[130]<!-- -->，<!-- -->[160]<!-- -->利用CelebA基准<!-- -->[161]<!-- -->探索了多种深度学习方法的稳定性，通过生成对抗性的例子来改变人脸识别的结果；看下图：</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806535.png"/></p><p>通过使用所谓的“快速翻转属性”技术攻击深度网络分类器，他们发现深度神经网络对对手攻击的鲁棒性在不同的面部属性之间存在很大差异。对抗性攻击可以有效地将目标属性的标签转换为相关属性。</p><p>Shen等人<!-- -->[144]<!-- -->提出了两种不同的技术来生成具有高“吸引力分数”但“主观分数”较低的人脸的对抗性示例，用于使用深度神经网络进行人脸吸引力评估。有关人脸识别任务的进一步攻击，请参阅<!-- -->[185]<!-- -->。第3节回顾的文献假设对手直接用图像扰动反馈深层神经网络。此外，还使用标准图像数据库评估了攻击的有效性。尽管这些设置已经证明足以说服许多研究人员，对抗性攻击是实践中深入学习的真正关注点，但我们在文献中也遇到了一些实例（例如<!-- -->[48]<!-- -->，<!-- -->[30]<!-- -->）这种担忧被轻描淡写，对抗性的例子被认为“只是好奇的问题”，几乎没有实际的担忧。因此，本节专门介绍在实际情况下处理对抗性攻击的文献，以帮助解决争论</p><h3>4.1 Cell-phone camera attack</h3><p>Kurakin等人<!-- -->[35]<!-- -->首先证明了防御攻击的威胁也存在于物理世界中。为了说明这一点，他们打印了敌对的图像，并用手机摄像头拍下了照片。这些图像被输入到Tensor-Flow相机演示应用程序<!-- -->[181]<!-- -->，该应用程序使用谷歌的Inception模型<!-- -->[145]<!-- -->进行对象分类。结果表明，即使通过相机观察，也有很大一部分图像被错误分类。在图6中，示出了原稿的示例。</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806426.png" alt="在这里插入图片描述"/></p><p>以下网址还提供了一段视频<!-- -->[https://youtu.be/zQ uMenoBCk ]<!-- -->(<a href="https://youtu.be/zQ">https://youtu.be/zQ</a> uMenoBCk )显示了进一步的图像敌对攻击的威胁。这项工作研究了FGSM<!-- -->[23]<!-- -->、BIM和ILCM<!-- -->[35]<!-- -->在物理世界中的攻击方法。</p><h3>4.2 Road sign attack</h3><p>​	Etimov等人<!-- -->[75]<!-- -->在<!-- -->[36]<!-- -->和<!-- -->[88]<!-- -->中提出的攻击的基础上，设计了物理世界的鲁棒扰动。他们证明了强大的物理条件，如在视角，距离和分辨率的变化是攻击的可能性。他们提出的算法称为RP2鲁棒物理扰动，用于生成对抗性的例子，道路标志识别系统。这个算法实现了高愚弄率在实际驾车设置。在这项工作中，针对物理路标引入了两种攻击类别：（a）海报打印：攻击者打印一张受到干扰的路标海报，并将其放置在真实的路标上（见下图）（b） 贴纸扰动：印刷在纸上，纸贴在真正的标志上。对于（b）两种类型的扰动进行了研究，（b1）细微扰动：占据整个标志，（b2）伪装扰动：标志上的涂鸦贴纸形式。因此，所有这些干扰都只需要彩色打印机，而不需要其他特殊硬件。成功地产生了（a）和（b）的扰动，使得扰动对物理世界中的自然变化保持鲁棒性，这表明了现实世界中敌对例子的威胁。</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806396.png" alt="在这里插入图片描述"/></p><p>​	Lu等人<!-- -->[30]<!-- -->之前曾声称，由于移动车辆中的物理条件不断变化，对抗性示例对于自动车辆中的目标检测不是一个问题。然而，他们所采用的攻击方法<!-- -->[22]<!-- -->、<!-- -->[23]<!-- -->、<!-- -->[35]<!-- -->有些原始。Etimov等人<!-- -->[75]<!-- -->的发现与<!-- -->[66]<!-- -->中的结果正交。然而，在后续研究中，Lu等人<!-- -->[19]<!-- -->表明，像YOLO 9000<!-- -->[149]<!-- -->和FasterrRcnn<!-- -->[150]<!-- -->这样的探测器“目前”没有被Etimov等人<!-- -->[75]<!-- -->引入的攻击所欺骗。在后续研究中，Lu等人<!-- -->[19]<!-- -->表明，像YOLO 9000<!-- -->[149]<!-- -->和Faster rRcnn<!-- -->[150]<!-- -->这样的探测器“目前”没有被Etimov等人<!-- -->[75]<!-- -->引入的攻击所欺骗。Zeng等人<!-- -->[87]<!-- -->还认为，图像空间中的对抗性扰动在物理空间中并不能很好地推广; 然而，Athalye等人<!-- -->[65]<!-- -->表明，我们实际上可以打印3D物体，以便在物理世界中进行成功的对抗性攻击。我们在第4.3节讨论<!-- -->[65]<!-- -->。</p><p>​	Gu等人<!-- -->[33]<!-- -->还探讨了一个有趣的概念，神经网络外包训练面临的威胁。他们表明，有可能训练一个在用户的训练和验证样本上表现出最先进性能，但在攻击者选择的输入上表现糟糕的网络（BadNet）。他们在真实场景中演示了这种攻击，创建了一个街道标志分类器，当在停车标志上添加特殊标签时，该分类器将停车标志识别为限速标志。此外，研究发现，即使网络后来用额外的训练数据进行了微调，网络的愚弄行为仍然在合理的程度上存在。</p><h3>4.3 Generic adversarial 3D objects</h3><p>Athalye等人<!-- -->[65]<!-- -->介绍了一种构建3D对象的方法，这种方法可以在各种角度和视点上愚弄神经网络。他们的“对变换的期望”（EOT）框架能够构建对整个图像/对象变换分布具有对抗性的示例。他们的端到端方法能够打印任意对抗性的3D对象。在我们看来，这项工作的结果确定，对抗性攻击是一个真正的关注在物理世界的深入学习。图8展示了一个3D打印海龟的例子，它被EOT框架修改为步枪。以下网址提供了演示物理世界中被EOT愚弄的视频：<a href="https://www.youtube.com/watch?v=YXy6oX1iNoA&amp;feature=youtu.be">https://www.youtube.com/watch?v=YXy6oX1iNoA&amp;feature=youtu.be</a></p><h3>4.4 Cyberspace attacks</h3><p>Papernot等人<!-- -->[39]<!-- -->在现实世界中对网络空间中的深层神经网络分类器发起了第一次攻击。他们在合成数据上训练了一个替代网络来代替目标黑盒分类器，并举例说明了MetaMind、Amazon和Google对远程托管神经网络的攻击。他们能够证明各自的目标网络错误地分类了84.24%，96.19%和88.94%的由他们的方法产生的对抗性例子。实际上，攻击者在其威胁模型中唯一可用的信息是目标网络的输出标签，用于攻击者提供的输入图像。</p><p>在一项相关的工作中，Liu等人<!-- -->[88]<!-- -->开发了一种基于集合的攻击，并展示了它对攻击的成功。 </p><p>Grosse等人<!-- -->[61]<!-- -->展示了为用作恶意软件分类工具的神经网络构建有效的通用攻击。与图像识别相比，恶意软件分类领域在对抗性设置中引入了额外的约束，如用离散输入代替连续输入域，用要求等价功能行为代替视觉相似性条件。然而，Grosse等人<!-- -->[61]<!-- -->表明，创建有效的对抗性示例对于恶意软件分类仍然是可能的。在<!-- -->[64]<!-- -->、<!-- -->[107]<!-- -->、<!-- -->[125]<!-- -->中还可以找到针对基于深度学习的恶意软件分类的成功对手攻击的更多示例。</p><h2>5 EXISTENCE OF ADVERSARIAL EXAMPLES</h2><p>​	在有关计算机视觉深度学习中的对抗性攻击的文献中，关于对抗性例子存在着不同的观点。这些观点通常与研究人员在攻击或防御深层神经网络时所做的局部经验观察很好地吻合。然而，它们在普及性方面往往不够。例如，Goodfello等人<!-- -->[23]<!-- -->流行的线性假设很好地解释了FGSM和相关攻击。然而，Tanay和Griffin<!-- -->[74]<!-- -->证明了线性分类器的图像类不会受到敌对示例的影响，这与线性假设不一致。更不用说，线性假设本身与先前流行的观点有很大的偏差. 对抗性的例子来源于深度神经网络引起的高度非线性的决策边界。文献中也有其他例子，其中线性假设没有得到直接支持<!-- -->[119]<!-- -->。</p><p>​	决策边界的平坦性<!-- -->[69]<!-- -->、决策边界的大局部曲线<!-- -->[70]<!-- -->和网络的低灵活性<!-- -->[71]<!-- -->是关于存在相互不完全一致的对抗性例子的观点的例子。很明显，只需修改图像中的一个像素就可以形成对抗性示例，但目前的文献似乎对存在对抗性示例的原因缺乏共识。<strong>这一事实也使得对抗性例子的分析成为一个积极的研究方向</strong>，有望探索和解释由深度神经网络（目前更普遍地被视为黑盒模型）所诱导的决策边界的性质。下面，我们回顾了主要集中在分析对抗性扰动的存在性以进行深入学习的工作。我们注意到，除了下面回顾的文献外，与对抗性攻击（第3节）和防御（第6节）相关的工作通常提供对抗性扰动的简要分析，同时推测导致存在对抗性例子的现象。</p><h3>5.1 Limits on adversarial robustness</h3><p>​	Fawzi等人<!-- -->[118]<!-- -->提出了一个研究分类器对 对抗性扰动不稳定性的框架。他们根据数据集类别之间的“可分辨性度量”建立了分类器鲁棒性的基本限制，其中可分辨性定义为线性分类器的两个类别平均值之间的距离和所研究非线性模型的二阶矩矩阵之间的距离分类器。这项工作表明，对抗性的例子也存在于深层神经网络以外的分类中。本文的分析将对抗性不稳定性的现象追溯到分类器的低灵活性，这与当时的主流观点不完全正交，即网络的高非线性使它们容易受到对抗性例子的影响。</p><h3>5.2 Space of adversarial examples</h3><p>​	Tabacof和Eduardo<!-- -->[25]<!-- -->在MNIST<!-- -->[10]<!-- -->和ImageNet<!-- -->[11]<!-- -->数据集上生成了浅层和深层网络分类器的对抗性示例，并利用不同分布和强度的噪声探测对抗性示例的像素空间。作者经验性地证明了对抗性例子出现在像素空间的大区域中，这与<!-- -->[23]<!-- -->中的类似主张是一致的。然而，在某种程度上与线性假设相反，他们认为一个弱的、浅的和更线性的分类器也像一个强的深分类器一样容易受到敌对例子的影响。</p><p>​	Tramer等人<!-- -->[132]<!-- -->提出了一种估计对抗性例子空间维数的方法。据称，对抗性的例子跨越一个相邻的高维空间（例如，维度≈25）。由于高维性，不同分类器的子空间可以相交，从而导致对抗性例子的可转移性。有趣的是，他们的分析表明，即使分类器容易受到直接攻击，也有可能防御基于传输的攻击。</p><h3>5.3 Boundary tilting perspective</h3><p>​	Tanay和Griffin<!-- -->[74]<!-- -->对深层神经网络中存在的对抗性例子提出了“边界倾斜”的观点。他们认为，为了学习和评估分类器而采样的单个类数据通常存在于该类的子流形中，并且当分类边界接近该子流形时，该类存在对抗性示例。他们将分类器的“对抗强度”概念形式化，并将其简化为所考虑的分类器边界和最近的质心分类器之间的“偏离角”。然后证明了分类器的对抗强度可以通过“边界倾斜”决策来改变。作者还认为分类器的对抗稳定性与其正则化有关。在Tanay和Griffin看来，关于存在对抗性例子的线性假设<!-- -->[23]<!-- -->是“不可信的”。</p><h3>5.4 training cause adversaries预测的不确定性与进化停滞</h3><p>​	Cubuk等人<!-- -->[91]<!-- -->认为，“对抗性测试的起源主要是由于神经网络对其预测的固有不确定性”。他们根据经验计算了不确定性的函数形式，这与网络结构、训练协议和数据集无关。有人认为，这种形式只依赖于统计的网络逻辑差异。这最终会导致敌对攻击导致的欺骗比率，从而显示出相对于扰动大小的通用缩放。他们研究了基于FGSM<!-- -->[23]<!-- -->、ILCM和BIM<!-- -->[35]<!-- -->的攻击来证实他们的说法。它还声称，网络在清晰图像上的准确性与其对抗性的健壮性相关（关于这个方向的更多论据，见第5.5节）。</p><p>​	Rozsa等人<!-- -->[102]<!-- -->假设对抗性干扰的存在是训练图像上决策边界进化停滞的结果。他们认为，一旦分类正确，单个训练样本就不会导致模型（即神经网络）的训练损失，这最终会使它们接近决策边界。因此，通过添加微小的扰动，就有可能将这些（和类似的）样本丢弃到错误的类区域。他们提出了一种批量调整网络梯度（BANG）算法来训练网络，以缓解训练过程中的进化停滞。</p><h3>5.5 Accuracy-adversarial robustness correlation</h3><p>​	为了解释对抗性攻击的存在，Rozsa等人<!-- -->[97]<!-- -->实证分析了八个深度网络分类器的准确度与其对<!-- -->[23]<!-- -->、<!-- -->[94]<!-- -->中介绍的三种对抗性攻击的鲁棒性之间的关系。所研究的分类器包括AlexNet<!-- -->[9]<!-- -->、VGG-16和VGG-19网络<!-- -->[163]<!-- -->、伯克利训练的GoogLeNet和普林斯顿GoogLeNet版本<!-- -->[18]<!-- -->、ResNet-52、ResNet-101和ResNet-152<!-- -->[147]<!-- -->。使用<!-- -->[23]<!-- -->和<!-- -->[94]<!-- -->中提出的技术，借助大型ImageNet数据集<!-- -->[11]<!-- -->生成对抗性示例。他们的实验结果表明，分类精度较高的网络通常对敌方实例也表现出较强的鲁棒性。他们还得出结论，对抗性的例子在相似的网络拓扑之间传输得更好。</p><h3>5.6 More on linearity as the source</h3><p>​	Kortov和Hopfield<!-- -->[127]<!-- -->在稠密联想记忆（DAM）模型的背景下检验了普遍扰动的存在<!-- -->[164]<!-- -->。与典型的现代深层神经网络相比，DAM模型采用了神经元之间更高阶（多于二次）的相互作用。作者已经证明，使用具有较小交互功率的DAM模型生成的对抗性测试无法愚弄具有高阶交互的模型，这类似于使用具有ReLU激活的深层神经网络来诱导线性<!-- -->[165]<!-- -->。作者提供了独立于FGSM<!-- -->[23]<!-- -->攻击的对抗性例子存在的经验证据，但支持Goodfello等人<!-- -->[23]<!-- -->的线性假设。</p><h3>5.7 Existence of universal perturbations</h3><p>​	Moosavi-Dezfouli等人<!-- -->[16]<!-- -->最初认为，普遍对抗性扰动利用了分类器诱导的决策边界之间的几何相关性。它们的存在部分归功于包含决策边界的nor-mals的子空间，使得法线也围绕着自然图像。在<!-- -->[70]<!-- -->中，他们进一步建立了他们的理论，并证明了存在共同的方向（在数据点之间共享），沿着这些方向，分类器的决策边界可以高度正弯曲。他们认为这样的方向在宇宙扰动的存在中起着关键作用。基于他们的发现，作者还提出了一种新的几何方法来有效地计算普遍对抗摄动。</p><p>值得注意的是，先前Fawzi等人<!-- -->[69]<!-- -->也将分类器鲁棒性的理论界与决策边界的曲率联系起来。同样，Tramer等人<!-- -->[77]<!-- -->也认为，数据点附近决策边界的曲率是神经网络易受黑盒攻击的原因。在最近的另一项工作中，Mopuri等人<!-- -->[193]<!-- -->提出了一个类GAN模型来研究给定目标模型的普遍对抗扰动的分布。学习的分布也显示出良好的跨模型传递性。</p><h2>6 DEFENSES AGAINST ADVERSARIAL ATTACKS</h2><p>目前，对抗性攻击的防御主要沿着三个方向发展：</p><p>1） 在学习过程中使用修改过的训练方式或在测试过程中使用修改过的输入。</p><p>2） 修改网络，例如通过添加更多层/子网络、更改丢失/激活功能等。</p><p>3） 当分类筛选看不见的示例时，使用外部模型作为网络附加组件。</p><p>​	第一个方法不直接处理学习模型。其他两类更关注神经网络本身。这两类技术又可分为两类，即完全防御和仅检测。“完全防御”方法旨在使目标网络能够在对抗性示例上实现其原始目标，例如，分类器以可接受的精度预测对抗性示例进行预测。另一方面，“仅检测”方法意味着对潜在的对抗性示例发出危险信号，以便在任何进一步处理中拒绝它们。所描述的类别的分类也示于下图中。剩下的部分是根据这个分类法组织的。在所使用的分类法中，“修改”网络和使用“附加组件”的区别在于前者在训练期间对原始的深层神经网络结构/参数进行了更改。另一方面，后者保持原始模型的完整性，并在测试过程中附加外部模型；</p><p><img src="./src/Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey/20210501181806435.png"/></p><h3>6.1 Modified training/input</h3><h4><em>6.1.1 Brute-force adversarial training【”蛮力对抗训练“】</em></h4><p>​	自从发现深层神经网络的对抗性例子<!-- -->[22]<!-- -->以来，相关文献中有一个普遍的共识，即神经网络对这些例子的鲁棒性随着对抗性训练而提高。因此，引入新的对抗性攻击的大多数贡献，例如<!-- -->[22]<!-- -->、<!-- -->[23]<!-- -->、<!-- -->[72]<!-- -->（见第3节）同时提出了对抗性训练作为抵御这些攻击的第一道防线。尽管对抗性训练提高了网络的健壮性，但要想真正有效，它需要使用强攻击进行训练，并且网络的体系结构具有足够的表现力。由于对抗性训练需要增加训练/数据量，我们称之为“暴力的“策略。 </p><p>​	文献中还普遍观察到，暴力对抗训练可使网络正规化（例如，见<!-- -->[23]<!-- -->，<!-- -->[90]<!-- -->），以减少过拟合，进而提高网络抵抗对抗攻击的鲁棒性。受此启发，Miyato等人<!-- -->[113]<!-- -->提出了一种“虚拟对抗训练”方法来平滑神经网络的输出分布。Zheng等人<!-- -->[116]<!-- -->还提出了一种相关的“稳定性训练”方法，以提高神经网络对输入图像小失真的鲁棒性。值得注意的是，尽管对抗性训练可以提高神经网络的鲁棒性，但Moosavi Dezfouli<!-- -->[16]<!-- -->表明，<strong>对于已经经过对抗性训练的网络，可以再次计算有效的对抗性例子。</strong>从这里就可以看出暴力对抗并不是一种很好的方法；</p><h4><em>6.1.2 Data compression as defense 【数据压缩防御】</em></h4><p>​	Dziugaite等人<!-- -->[123]<!-- -->指出，大多数流行的图像分类数据集都包含JPG图像。基于这一观察，他们研究了JPG压缩对FGSM计算的扰动的影响<!-- -->[23]<!-- -->。据报道，JPG压缩实际上可以在很大程度上扭转FGSM扰动导致的分类精度下降。然而，我们得出结论，单靠压缩是远远不能有效防御的。Guo等人也对JPEG压缩进行了研究。郭等人<!-- -->[82]<!-- -->也对JPEG压缩进行了研究，以降低对抗性图像的有效性。Das等人<!-- -->[37]<!-- -->也采取了类似的方法，使用JPEG压缩来去除图像中的高频成分，并提出了一种基于集合的技术来对抗FGSM<!-- -->[23]<!-- -->和DeepFool方法<!-- -->[72]<!-- -->产生的敌对攻击。尽管<!-- -->[37]<!-- -->中报告了令人鼓舞的结果，但没有对更强的攻击进行分析。e.g. C&amp;W攻击<!-- -->[36]<!-- -->。此外，Shin和Song<!-- -->[186]<!-- -->已经证明了存在能够在JPEG压缩中幸存的对抗性示例。在我们之前的工作<!-- -->[81]<!-- -->中，我们还发现离散余弦变换（DCT）下的压缩不足以抵御普遍扰动<!-- -->[16]<!-- -->。基于压缩的防御的一个主要限制是，较大的压缩也会导致对干净图像的分类精度损失，而较小的压缩通常不能充分消除对抗性扰动。</p><p>​	在另一个相关的方法中，Bhagoji等人<!-- -->[169]<!-- -->提出使用主成分分析来压缩输入数据，以增强对抗性。然而，Xu等人<!-- -->[140]<!-- -->指出，这种压缩也会破坏图像的空间结构，因此通常会对分类性能产生不利影响。</p><h4><em>6.1.3 Foveation based defense 【中心凹防御】</em></h4><p>​	Luo等人<!-- -->[119]<!-- -->证明，使用L-BFGS<!-- -->[22]<!-- -->和FGSM<!-- -->[23]<!-- -->的对抗性攻击具有显著的鲁棒性，通过在图像的不同区域应用神经网络的“中心凹”机制是可能的。假设基于CNN的分类器在大型数据集（如ImageNet<!-- -->[11]<!-- -->）上训练，通常对图像中对象的缩放和平移变化具有鲁棒性。然而，这种不变性并没有扩展到图像中的对抗模式。这使得中央凹成为一个可行的选择，以减少在<!-- -->[22]<!-- -->，<!-- -->[23]<!-- -->中提出的对抗性攻击的有效性。然而，中心凹还没有证明其有效性更强大的攻击。</p><h4><em>6.1.4 Data randomization and other methods 【数据随机化和其他方法】</em></h4><p>​	Xie等人<!-- -->[115]<!-- -->表明，随机调整敌对例子的大小会降低其有效性。此外，在这些示例中添加随机填充也可以降低网络的欺骗率。Wang等人<!-- -->[138]<!-- -->用一个单独的数据转换模块转换输入数据，以消除图像中可能的敌对干扰。在文献中，我们还发现有证据表明，训练期间的数据增强（例如，高斯数据增强<!-- -->[46]<!-- -->）也有助于提高神经网络对敌对攻击的鲁棒性，尽管这一点很小。</p><h3>6.2 Modifying the network</h3><p>​	对于修改神经网络以抵御对手攻击的方法，我们首先讨论了“完全防御”方法。“仅检测”方法在第6.2.8节中单独进行了审查。</p><h4><em>6.2.1 Deep Contractive Networks【深度收缩网络】</em></h4><p>​	在使深度学习对对抗性攻击具有鲁棒性的早期尝试中，Gu和Rigazio<!-- -->[24]<!-- -->引入了深度收缩网络（DCN）。结果表明，去噪自动编码器<!-- -->[154]<!-- -->可以减少对抗性噪声，但是将其与原始网络叠加可以使生成的网络更容易受到干扰。基于这一观察，DCNs的训练过程使用了类似于压缩自动编码器的平滑度惩罚<!-- -->[173]<!-- -->。自从DCNs最初被提出以来，已经引入了许多更强的攻击。使用自动编码器实现神经网络对抗鲁棒性的相关概念也可以在<!-- -->[141]<!-- -->中找到。</p><h4><em>6.2.2 Gradient regularization/masking 【梯度正则化/掩蔽】</em></h4><p>​	Ross和Doshi Velez<!-- -->[52]<!-- -->研究了输入梯度正则化<!-- -->[167]<!-- -->作为对抗鲁棒性的一种方法。他们的方法训练可微模型（例如深层神经网络），同时根据输入的变化惩罚导致输出的变化程度。这意味着，一个小的对抗性扰动不太可能彻底改变训练模型的输出。结果表明，该方法与暴力对抗训练相结合，对FGSM<!-- -->[23]<!-- -->和JSMA<!-- -->[60]<!-- -->等攻击具有很好的鲁棒性。然而，这些方法中的每一种都几乎使网络的训练复杂度增加了一倍，这在许多情况下都是不可行的。</p><p>​	此前，Lyu等人<!-- -->[28]<!-- -->还使用了将网络模型的损失函数梯度与输入相关的概念，以结合网络对基于L-BFGS<!-- -->[22]<!-- -->和FGSM<!-- -->[23]<!-- -->的攻击的鲁棒性。类似地，Shaham等人<!-- -->[27]<!-- -->试图通过在每次参数更新时最小化模型在对抗性示例中的损失来提高神经网络的局部稳定性。他们用最坏情况下的对抗性例子而不是原始数据来最小化模型的损失。在一项相关的工作中，Nguyen和Sinha<!-- -->[44]<!-- -->通过在网络的logit输出中添加噪声，引入了一种基于掩蔽的防御C&amp;W攻击的方法<!-- -->[36]<!-- -->。</p><h4><em>6.2.3 Defensive distillation 【防御蒸馏】</em></h4><p>​	Papernot等人<!-- -->[38]<!-- -->利用了“蒸馏”的概念<!-- -->[166]<!-- -->，使得深层神经网络能够抵御对手的攻击。Hinton等人<!-- -->[166]<!-- -->引入蒸馏作为一种训练过程，将更复杂网络的知识转移到更小的网络中。Papernot等人<!-- -->[38]<!-- -->引入的程序变体本质上是利用网络的知识来提高自身的健壮性。以训练数据的类概率向量的形式提取知识，并反馈训练原始模型。结果表明，这样做可以提高网络对图像小扰动的恢复能力。<!-- -->[108]<!-- -->中也提供了这方面的进一步经验证据。此外，在后续工作中，Papernot等人<!-- -->[84]<!-- -->还通过解决<!-- -->[38]<!-- -->中遇到的数值不稳定性，扩展了防御蒸馏方法。值得注意的是，第3.1节中介绍的“卡里尼和瓦格纳”（C&amp;W）攻击<!-- -->[36]<!-- -->被认为是成功对抗防御蒸馏技术的。我们还注意到，防御蒸馏也可以看作是“梯度掩蔽”技术的一个例子。然而，鉴于其在文献中的流行性，我们将其单独描述。</p><h4><em>6.2.4 Biologically inspired protection</em></h4><p>​	Nayebi和Ganguli<!-- -->[124]<!-- -->证明了神经网络对具有高度非线性激活的对抗性攻击的自然鲁棒性（类似于非线性树突计算）。Nayebi和Ganguli<!-- -->[124]<!-- -->证明了神经网络对具有高度非线性激活的对抗性攻击具有自然鲁棒性（类似于非线性树突计算）</p><p>​	Krotov和Hopfield<!-- -->[127]<!-- -->的记忆模型也采用了类似的原理来抵抗对抗性例子。考虑到Goodfel-low等人<!-- -->[23]<!-- -->、<!-- -->[124]<!-- -->和<!-- -->[127]<!-- -->的线性假设，现代神经网络对对抗性例子的敏感性似乎是激活线性效应的概念得到了进一步的发展。我们注意到，Brendel和Bethge<!-- -->[187]<!-- -->声称，由于计算的数值限制，这些攻击在生物启发保护（Biologically inspired protection）<!-- -->[124]<!-- -->上攻击失败。稳定计算（Stabilizing the computations）再次成功攻击受保护的网络。</p><h4><em>6.2.5 Parseval Networks</em></h4><p>​	西塞等人<!-- -->[131]<!-- -->提出了“Parseval”网络作为对抗性攻击的防御。这些网络通过控制网络的全局Lipschitz常数采用分层正则化。考虑到一个网络可以被看作是一个函数的组合（在每一层），通过为这些函数保持一个小的Lipschitz常数，可以对小的输入扰动进行鲁棒性处理。Cisse等人提出通过用“parseval紧框架”参数化网络的权重矩阵来控制其谱范数<!-- -->[172]<!-- -->，因此命名为“parseval”网络。</p><h4><em>6.2.6 DeepCloak</em></h4><p>Gao等人<!-- -->[139]<!-- -->建议在处理分类的层之前插入一个掩蔽层。添加的层通过向前传递干净的和敌对的一对图像进行显式训练，并对这些图像对的前一层的输出特征之间的差异进行编码。有人认为，添加层中最主要的权重对应于网络中最敏感的特征（就对抗性操纵而言）。因此，在分类时，这些特征通过强制将添加层的主要权重设为零来掩盖。</p><h4><em>6.2.7 Miscellaneous approaches</em></h4><p>Zantedeschi等人<!-- -->[46]<!-- -->提出使用有界ReLU<!-- -->[174]<!-- -->来降低图像中敌对模式的有效性，这是使神经网络对敌对攻击具有鲁棒性的其他显著努力之一。Jin等人<!-- -->[120]<!-- -->介绍了一种前馈CNN，它使用加性噪声来减轻对抗性例子的影响。Sun等人<!-- -->[56]<!-- -->提出了“超网络”，它使用统计滤波作为一种方法，使网络健壮。Madry等人<!-- -->[55]<!-- -->从稳健优化的角度研究了对抗性防御。他们表明，与PGD对手进行对抗性训练可以成功地防御一系列其他对手。后来，卡里尼等人<!-- -->[59]<!-- -->也证实了这一观察结果。Na等人<!-- -->[85]<!-- -->采用了一种网络，该网络通过统一的嵌入进行正则化，用于分类和低水平的相似性学习。利用干净嵌入和相应的敌对嵌入之间的距离对网络进行惩罚。斯特劳斯等人<!-- -->[89]<!-- -->研究了一系列方法来保护网络免受干扰。Kadran等人<!-- -->[136]<!-- -->修改了神经网络的输出层，以增强对抗性攻击的鲁棒性。</p><p>Wang等人<!-- -->[129]<!-- -->，<!-- -->[122]<!-- -->通过利用网络中的不可逆数据转换开发了可以抵抗神经对抗样本的神经网络。Lee等人<!-- -->[106]<!-- -->开发了多种规则化网络，使用训练目标最小化干净图像和敌对图像的多层嵌入结果之间的差异。Kotler和Wong<!-- -->[96]<!-- -->提出学习基于ReLU的分类器，该分类器对小的对抗性扰动具有鲁棒性。他们训练的神经网络达到高精度（&gt; 90%）对抗规范环境中的任何对抗样本（$\epsilon=0.1$ for $\ell_{\infty}$，在MNIST数据集当中）。Raghunathan等人<!-- -->[189]<!-- -->研究了具有一个隐藏层的神经网络的防御问题。他们的方法在MNIST数据集上生成了一个网络和一个证书，使得对图像像素的干扰不超过ε=0.1的攻击都不会导致超过35%的测试错误。Kolter和Wong<!-- -->[96]<!-- -->以及Raghunathan等人<!-- -->[189]<!-- -->是为数不多的防御敌对攻击的可证明方法。考虑到这些方法在计算上不适用于更大的网络，唯一被广泛评估的防御措施是Madry等人<!-- -->[55]<!-- -->的防御措施，在MNIST上对大epsilon（0.3/1）的准确率为89%，在CIFAR上对中等epsilon（8/255）的准确率为45%。另一个可以被视为具有保证的对抗性攻击/防御的工作线索与深度神经网络的验证有关，例如<!-- -->[191]<!-- -->，<!-- -->[192]<!-- -->。OrOrbia等人<!-- -->[194]<!-- -->的研究表明，对抗性训练的许多不同建议是更普遍的正规化目标实例，他们称之为DataGrad。提出的DataGrad框架可以看作是分层压缩自编码惩罚的扩展。</p><h4><em>6.2.8 Detection Only approaches</em></h4><p><strong>SafetyNet:</strong> Lu等人<!-- -->[66]<!-- -->假设，对抗性的例子在网络（后期）中产生的ReLU激活模式与干净图像产生的模式不同。基于这一假设，他们提出在目标模型上附加一个径向基函数SVM分类器，使得SVM使用由网络后期ReLUs计算的离散码。为了检测测试图像中的扰动，使用支持向量机将其编码与训练样本的编码进行比较。由<!-- -->[23]<!-- -->、<!-- -->[35]<!-- -->、<!-- -->[72]<!-- -->生成的对抗性示例的有效检测通过其框架SafetyNet进行了证明。</p><p><strong>Detector subnetwork</strong>: Metzen等人<!-- -->[78]<!-- -->提出用一个子网来增强目标网络，该子网被训练用于检测输入中的敌对扰动的二进制分类任务。研究表明，将这样的网络附加到模型的内部层并使用对抗性训练有助于检测使用FGSM<!-- -->[23]<!-- -->、BIM<!-- -->[35]<!-- -->和DeepFool<!-- -->[72]<!-- -->方法产生的扰动。然而，Lu等人<!-- -->[66]<!-- -->后来表明，这种方法同样容易受到反措施的影响。</p><p><strong>Exploiting convolution filter statistics</strong>: Li和Li<!-- -->[105]<!-- -->利用基于CNN的神经网络中卷积滤波器的统计特性将输入图像分类为干净的或敌对的。利用这些统计量设计了一个级联分类器，并对文献<!-- -->[22]<!-- -->、<!-- -->[114]<!-- -->中方法生成的85%以上的对抗性图像进行了检测。</p><p><strong>Additional class augmentation</strong>: Grosse等人<!-- -->[57]<!-- -->提出用一个额外的类来增强潜在的目标神经网络模型，在这个类中，模型被训练来分类所有的对抗性例子。Hosseini等人<!-- -->[32]<!-- -->也采用了类似的策略来检测黑匣子攻击。</p><h3>6.3 <strong>Network add-ons</strong></h3><h4><em>6.3.1 Defense against universal perturbations</em></h4><p>Akhtar等人<!-- -->[81]<!-- -->提出了一个防御框架，以抵抗使用普遍扰动产生的对抗性攻击<!-- -->[16]<!-- -->。该框架将额外的“预输入”层附加到目标网络中，并训练它们校正每一个turbed图像，以便分类器的预测与对同一图像的干净版本的预测相同。预输入层称为扰动校正网络（PRN），在不更新目标网络参数的情况下对其进行训练。通过从训练图像的PRN输入-输出差异中提取特征来训练单独的检测器。测试图像首先通过PRN，然后利用其特征检测扰动。如果检测到对抗性干扰，则使用PRN的输出对测试图像进行分类。图10示出了由PRN执行的校正。对去除的模式分别进行检测分析。</p><h4><em>6.3.2 GAN-based defense</em></h4><p>​	Lee等人<!-- -->[101]<!-- -->使用流行的生成性对抗网络框架<!-- -->[153]<!-- -->来训练对FGSM<!-- -->[23]<!-- -->类攻击具有鲁棒性的网络。作者建议直接沿着generator network训练网络，试图为该网络产生扰动。在训练过程中，分类器不断尝试正确分类干净的和受干扰的图像。我们将此技术归类为“附加”方法，因为作者建议始终以这种方式训练任何网络。在另一个基于GAN的防御中，Shen等人<!-- -->[58]<!-- -->使用网络的生成器部分来校正扰动图像。</p><h4><em>6.3.3 Detection Only approaches</em></h4><p><strong>Feature squeezing:</strong> Xu等人<!-- -->[43]<!-- -->提出<strong>利用特征压缩来检测图像的对抗性扰动</strong>。他们将两个外部模型添加到分类器网络中，这样这些模型可以减少图像中每个像素的颜色位深度，并对图像进行空间平滑。比较了目标网络对原始图像和压缩图像的预测。如果预测结果之间存在较大差异，则该图像被认为是一个对抗性的例子。尽管<!-- -->[43]<!-- -->证明了这种方法对更经典的攻击的有效性<!-- -->[23]<!-- -->，但后续工作<!-- -->[140]<!-- -->也声称该方法对更强大的C&amp;W攻击的效果相当好<!-- -->[36]<!-- -->。他等人<!-- -->[76]<!-- -->还将特征压缩与<!-- -->[175]<!-- -->中提出的系综方法相结合，以表明防御的强度并不总是通过组合它们而增加。</p><p><strong>MagNet:</strong> Meng和Chen<!-- -->[45]<!-- -->提出了一个框架，该框架使用一个或多个外部探测器将输入图像分类为敌对或干净。在训练过程中，该框架旨在学习多种清晰图像。在测试阶段，发现远离流形的图像被视为对抗性的，并被拒绝。靠近流形（但不完全在流形上）的图像总是被重组为位于流形上，并且分类器被送入重组后的图像。将附近的图像吸引到干净图像的流形中，并将远处的图像丢弃的概念也启发了框架的名称，即磁铁。值得注意的是，卡里尼和瓦格纳（Carlini and Wagner）<!-- -->[188]<!-- -->最近证明，这种防御技术也可以在稍大的扰动下被击败。</p><p><strong>其他方法：</strong>Liang等人<!-- -->[50]<!-- -->将图像的扰动视为噪声，并使用标量量化和空间平滑滤波器分别检测这些扰动。在一个相关的方法中，Feinman等人<!-- -->[86]<!-- -->提出通过利用不确定性估计（of dropout neural networks）和在神经网络的特征空间中执行密度估计来检测对抗性扰动。最后，利用所提出的特征，将单独的二值分类器训练成多示例检测器。Gebhart和Schrater<!-- -->[92]<!-- -->将神经网络计算视为图中的信息流，提出了一种通过对诱导图应用持久同调来检测对抗性扰动的方法。</p><h2>7 OUTLOOK OF THE RESEARCH DIRECTION</h2><p>​	在前面的几节中，我们全面回顾了最近关于对抗性攻击深度学习的文献。尽管在技术细节的这些部分中报告了一些有趣的事实，但下面我们将对这一新兴研究方向进行更一般性的观察。在没有深入了解这一领域的技术知识的情况下，讨论为读者提供了更广阔的前景。我们的论点是基于上述文献。</p><p><strong>这种威胁是真实存在的：</strong>虽然很少有研究表明，对抗性攻击深度学习可能不是一个严重的问题，但大量相关文献表明并非如此。在第3节和第4节中回顾的文献清楚地表明，对抗性攻击可以严重降低深度学习技术在多个计算机视觉任务上的性能。特别是，第4节回顾的文献确定，在现实世界中，深度学习容易受到对抗性攻击。因此，我们可以得出结论，对抗性攻击对实践中的深度学习构成了真正的威胁。</p><p><strong>对抗性脆弱性是一种普遍现象：</strong>文献回顾表明，在计算机视觉中，不同类型的深层神经网络（如MLPs、CNNs、RNNs）在识别、分割、检测等多种任务上都被成功地愚弄。尽管现有的研究大多集中于在分类/识别任务上愚弄深度学习，但根据调查的文献，我们可以很容易地观察到，深度学习方法通常容易受到对手攻击。</p><p><strong>对抗性例子通常具有很好的泛化性</strong>：文献中所报道的对抗性例子的一个最常见的特性是它们在不同的神经网络之间有很好的传递。对于架构相对相似的网络来说尤其如此。在黑盒攻击中，对抗性例子的推广经常被利用。</p><p><strong>对抗性脆弱性的原因需要更多的研究：</strong>关于深层神经网络对微妙的对抗性扰动的脆弱性背后的原因，文献中有不同的观点。通常情况下，这些观点并不完全一致。这方面显然需要进行系统的调查。</p><p><strong>线性确实促进了脆弱性：</strong>Goodfello等人<!-- -->[23]<!-- -->首先提出，现代深层神经网络的设计迫使它们在高维空间中线性行为，也使它们容易受到对手的攻击。虽然这一概念很流行，但在文献中也遭到了一些反对。我们的调查指出，多个独立的贡献，保持神经网络的线性负责他们的脆弱性对抗性攻击。基于这一事实，我们可以说，线性确实促进了深层神经网络对对手攻击的脆弱性。然而，这似乎不是用廉价的分析扰动就成功愚弄深层神经网络的唯一原因。</p><p><strong>反措施是可能的：</strong>尽管存在多种防御技术来对抗对抗对手的攻击，但文献中经常显示，通过设计反措施，可以再次成功地攻击被防御的模型，例如<!-- -->[49]<!-- -->。这一观察结果需要新的防御措施也提供一个对明显的反措施的鲁棒性估计。</p><p><strong>高度活跃的研究方向：</strong> 深层神经网络对一般扰动的脆弱性的深刻暗示使得对抗性攻击及其防御的研究近年来非常活跃。在这项调查中回顾的大多数文献都是在过去两年中出现的，目前这方面的贡献源源不断。一方面，人们提出了一些技术来保护神经网络免受已知的攻击，另一方面，越来越多的强大的攻击被设计出来。最近，还组织了一次卡格尔竞赛。我们希望，这项高水平的研究活动最终会使深度学习方法足够强大，能够用于现实世界中的安全和安保关键应用。</p><h2>8 CONCLUSION</h2><p>本文首次全面综述了计算机视觉深度学习对抗性攻击的发展方向。尽管深度神经网络在各种各样的计算机视觉任务中具有很高的精度，但人们发现它们容易受到细微的输入扰动的影响，从而导致它们完全改变其输出。由于深度学习是当前机器学习和人工智能发展的核心，这一发现导致了许多设计对抗性攻击及其深度学习防御的最新贡献。本文回顾了这些贡献，主要集中在最有影响和最有趣的作品在文学。从回顾的文献来看，对抗性攻击显然是对实践中深入学习的真正威胁，尤其是在安全和安保关键应用中。现有文献表明，目前的深度学习不仅可以在网络空间中有效地攻击，而且可以在物理世界中有效地攻击。然而，由于这一研究方向的高度活跃性，可以希望深度学习能够在未来显示出相当强的对抗对手攻击的鲁棒性。</p>]]></content>
        <author>
            <name>Sonder</name>
            <uri>https://github.com/AndSonder</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Semantic Segmentation]]></title>
        <id>Progressive Semantic Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[03]Progressive-Semantic-Segmentation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[原论文：Progressive Semantic Segmentation]]></summary>
        <content type="html"><![CDATA[<p>原论文：<a href="https://arxiv.org/pdf/2104.03778.pdf">Progressive Semantic Segmentation</a></p><h2>问题描述</h2><p>当对大型图片进行语义分割时，可能会导致显存炸掉。收到内存限制，可以选择下采样，或将图像划分为局部块。但前者会丢失细节，后者会却反全局视图。</p><h2>后处理改善分割细节</h2><h3>经典方法</h3><p>条件随机场(CRF),引导滤波器（GF），两个速度慢，改进是渐进的。</p><p>深度学习的引导过滤器(DGF)可以提高推理速度</p><h3>迭代实例分割（ISS）</h3><p>通过多次将输入图像和分割图像通过细化模块来多次细化输出。</p><p>这种细化过程是自反的，因此每一个细化层的输入图像都是相同的</p><h3>CascadedPSP</h3><p>才用与ISS相同的细化模式，但是对于每一个细化层的输入与ISS不同，才用的是不同分辨率的原图和分割图像进行细化。</p><h2>MagNet</h2><p>核心模块为：segmentation and refinement</p><p> 在refinement模块中，每一层的输入两张分割图</p><ol><li>前几层的累积结果</li><li>当前层的当前规模和比例的分割模块计算结果</li></ol><p>在sementation中，可以应用与多个分割主干</p><p>refinement与分割主干无关，可以在任意一个主干训练，在另一个主干使用。</p><h3>多级过程通道处理</h3><p><img src="./src/progressive-semantic-segmentation/image-20210430160921956.png" alt="image-20210430160921956"/></p><p>用s表示每一个处理阶段，s = 1对应最初的粗分割处理块，s=m对应最后的细分结果</p><p>令h,w为最大的可被分割模块处理的大小</p><p>使用$h^s ,w^s$表示level s对应块输入图像的高，宽</p><p>通常$h^s ,w^s$的比例是递减的</p><h3>特定比例级别s的处理块</h3><p>先将输入图片X划分为若干个$h^s$ X $w^s$的小块，并对这些小块进行语义分割。</p><p>对于每一个小块的位置用集合$p_s$表示，$p_s = <!-- -->{<!-- -->p|p=(x,y,w^s,h^s)<!-- -->}<!-- -->$</p><p>x,y：当前块左上角在图像上的位置</p><p>$h^s ,w^s$:当前块的宽高</p><p>随着比例s的增加，巨型窗口的宽度和高度减小$P_s$的基数增加</p><p>对于特等的窗口p，将使用$X_p$表示在窗口P处提取的图像块</p><p>对于每个窗口p，执行以下操作：</p><ol><li>对于当前层的窗口$X_P^s$进行图像分割，将上一个层对应窗口，提取出其长宽为$h_s,w_s$的部分</li><li>对$X_p^s$和$Y^{s-1}_p$进行下采样，使得图像达到h,w，他们大小可以输入GPU中，并通过分割模块和细分模块进行处理。</li><li>$X_p^s$的分割结果用$O_p^s$表示</li></ol><h2>Refinement模块（细分模块）</h2><p>用于优化各个窗口的细节部分</p><p>主要有两个输入</p><pre><code>1. 先前一个模块的细分结果+下采样最终获得的特征图
2. 原图经过Segmentation的结果
</code></pre><h3>refinement结构图</h3><p><img src="./src/progressive-semantic-segmentation/image-20210430162107104.png" alt="image-20210430162107104"/></p><p>O：是分割结果</p><p>Y：是上一层细分结果</p><p>R：O和Y经过细分模块出来的图像</p><p>$R^u$：R的不确定性图，计算方式为对每一个像素，其对应点的值为，置信度最高-置信度第二高的值</p><p>$Y^u$：Y的不确定性图，同R的计算方式</p><p>这个做法与PointRend类似，但是只用了一个不确定性图，而这里用了两个。</p><p>最终获得的像素得分图（每个像素预测为1类和预测为2类的差值）计算方式为：
$$
Q = F(Y^u\cdot(1-R^u))
$$
其中$\cdot$符号表示对每个像素进行相乘</p><p>$F$表示中值滤波平滑处理（这个我也不是很懂）</p><p>通过最终计算处的得分图，找出图像中需要优化的k个点。</p><p>优化网络的实际结构：</p><p><img src="./src/progressive-semantic-segmentation/image-20210430164954374.png" alt="image-20210430164954374"/></p><h3>对比与其他算法的优点</h3><ol><li>SegFix：无法恢复被覆盖较大的小物体，如标志杆。</li><li>PointRend：缺乏全局上下文信息。</li></ol><h2>MagNet-Fast</h2><p>是MagNet的快速版，优化点在于减少了分块次数，同时在细分模块中只使用$Y^u$进行优化点选取，这样大大地提高了计算速度。</p><p>在做过这样的简化后对之前的算法在miou指标上，仍有提升</p><p><img src="./src/progressive-semantic-segmentation/image-20210430203924695.png" alt="image-20210430203924695"/></p><h2>消融实验结果</h2><p><img src="./src/progressive-semantic-segmentation/image-20210430204112176.png" alt="image-20210430204112176"/></p><p>经过作者团队的测试，选取的优化点k数量为$2^{16}$时能获得最佳效果</p><h2>选择优化点数量增加效率对比度</h2><p><img src="./src/progressive-semantic-segmentation/image-20210501223901657.png" alt="image-20210501223901657"/></p>]]></content>
        <author>
            <name>Zerorains</name>
            <uri>https://github.com/zeroRains</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation]]></title>
        <id>Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</id>
        <link href="https://ml.akasaki.space/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇关于数据依赖型解码器的理论和测试工作的论文。原论文是Decoders Matter for Semantic Segmentation : Data-Dependent Decoding Enables Flexible Feature Aggregation。]]></summary>
        <content type="html"><![CDATA[<p>这是一篇关于数据依赖型解码器的理论和测试工作的论文。原论文是<a href="https://arxiv.org/pdf/1903.02120.pdf">Decoders Matter for Semantic Segmentation : Data-Dependent Decoding Enables Flexible Feature Aggregation</a>。</p><p>近年来，常见的语义分割方法利用编码器-解码器结构进行逐像素的预测任务。在这些解码器每一层的最后通常是一层双线性上采样的过程，用于将像素恢复至原有像素大小。本论文的研究表明，这种与数据无关的双线性上采样方法可能会导致结果并不完美。</p><p>所以，本论文提出了一种依赖于输入数据的上采样取代双线性上采样，称为“DUpsampling”。这个新的方法利用在语义分段标签中的空间冗余，能够从低分辨率的CNN输出中恢复分辨率并实现逐像素预测。该方法在分辨率相对较低的输入上能获得更加精确的分割效果，并且显著降低了计算的复杂度。也就是说：</p><ul><li>这种新的上采样层重建能力非常强</li><li>这种方法对任何CNN编码器的组合和使用表现出很好的兼容性</li></ul><p>本论文还通过实验标明了，DUpsampling性能优越，并且无需任何后处理。</p><h2>Abstract（摘要）</h2><blockquote><p>Recent semantic segmentation methods exploit encoder-decoder architectures to produce the desired pixel-wise segmentation prediction. The last layer of the decoders is typically a bilinear upsampling procedure to recover the final pixel-wise prediction. We empirically show that this oversimple and data-independent bilinear upsampling may lead to sub-optimal results.
In this work, we propose a data-dependent upsampling (DUpsampling) to replace bilinear, which takes advantages of the redundancy in the label space of semantic segmentation and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. The main advantage of the new upsampling layer lies in that with a relatively lower-resolution feature map such as 1/16 or 1/32 of the input size, we can achieve even better segmentation accuracy, significantly reducing computation complexity. This is made possible by 1) the new upsampling layer&#x27;s much improved reconstruction capability; and more importantly 2) the DUpsampling based decoder&#x27;s flexibility in leveraging almost arbitrary combinations of the CNN encoders&#x27; features. Experiments demonstrate that our proposed decoder outperforms the state-of-the-art decoder, with only 20% of computation. Finally, without any post-processing, the framework equipped with our proposed decoder achieves new state-of-the-art performance on two datasets: 88.1% mIOU on PASCAL VOC with 30% computation of the previously best model; and 52.5% mIOU on PASCAL Context.     </p></blockquote><p>如果有时间的话请阅读<a href="https://arxiv.org/pdf/1903.02120.pdf">原作</a>。本文只是对原作阅读的粗浅笔记。</p><hr/><h2>Introduction（介绍）</h2><p>现阶段，基于FCN的稠密预测方法在语义分割领域内取得了巨大的成功，事实证明，CNN组成的编码器的特征提取功能非常强大。很重要的一点是，卷积运算所具有的参数共享特性让训练和预测变得高效（卷积运算的一些特性可以参考<a href="../ch2p1/%5B1%5Dconvolutional-nn-and-ops">这篇文章</a>）。</p><p>在原始的FCN方法中，编码器在提取高级特征的过程中往往会导致原图的分辨率被降低很多倍，从而导致精细的像素空间信息部分丢失，这使在原图分辨率上的预测（尤其是在对象边界上的预测）往往不够准确。DeepLab中引入了空洞卷积（空洞卷积的大致概念可以参考<a href="./%5B01%5DThe-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">这篇文章</a>中关于空洞卷积方法的部分）实现了在不降低原图大小的情况下扩大感受野（接收场）的效果。</p><p><img src="./src/Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation/image-20210503104215395.png" alt="image-20210503104215395"/></p><p>上图是一个在DeepLabv3+中使用的典型的Encoder-Decoder（编码器-解码器）结构。这个结构的编码器对输入进行了下采样比例为4的下采样后输入到解码器，并在最终的多特征图合并前对编码器产生的高阶特征进行了上采样，最后，使用双线性插值上采样完全恢复分辨率。在这个过程中，编码器是由CNN表示的，其任务是在原图上提取出不同级别的特征；解码器是由很多上采样表示的，其任务是将编码器产生的特征恢复到原图大小。</p><p>在以前的成果中，解码器通常由几个卷积层和双线性上采样层构成，这些层的主要目标是恢复被CNN忽略的细粒度信息。简单的双线性上采样方法对逐像素预测的恢复能力优先，它是一个机械的过程，不考虑每个像素预测之间的相关性，也就是说，它是独立于数据的过程。这就导致我们往往需要在双线性上采样之前的卷积解码器内就需要将CNN的产物产生为较高分辨率的特征图（通常至少恢复到原图大小的1/4或1/8），以便获得良好的预测结果。</p><p>这就产生了两个问题：</p><ol><li>为了让解码器输出更高一点分辨率的特征图作为双线性上采样的输入，需要空洞卷积大幅减小移动步幅，这导致训练和预测开销增大。例如，为了达到最好的分割效果，DeepLabv3中的编码器的下采样时步幅降低了4倍（从32降低到8），这就导致了DeepLabv3推理相对缓慢。</li><li>为了达到更好地效果，解码器往往必须将编码器产生的特征上采样到更低的特征维度中进行融合，这也是双线性上采样的不足导致的。这个局限性限制了特征聚合结构设计的空间，因此很可能导致次优的特征组合在解码器中被聚合在一起。这篇论文在后面证明了如果能很好的设计聚合特征的部分而不受特征图分辨率的限制，可以找到更好的特征聚合策略。</li></ol><p>为了解决双线性上采样的一系列问题，这篇论文提出了一种新的与数据相关的上采样方式，称为“DUpsampling”，该方法利用分割标签空间中旷阔的特征空间，在CNN构成的编码器输出的低分辨率特征图的恢复中有更好地兼容性，从而使卷积解码器的压力更小，也不需要编码器降低下采样率。</p><p>这种设计使融合特征的分辨率和最终预测输出的分辨率脱钩。这种结耦可以让解码器使用任意的特征进行聚合，从而设计出更好的特征聚合策略，更好地提高分割性能。</p><p>同时，<strong>DUpsampling能够通过 1×1 卷积无缝拼接进任何卷积网络中</strong>，因此并不需要任何额外的、特殊的设计的代码或是网络结构：</p><p><img src="./src/Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation/image-20210503110219533.png" alt="image-20210503110219533"/></p><p> 上图是DUpsampling融合进编码器-解码器网络的的设计形式的一种描述。</p><h2>Approach（这篇论文的方法）</h2><p>在这一部分，该论文首先用DUpsampling重新定义语义分割中的相关步骤；然后提出自适应的softmax函数，这使DUpsampling的训练变得更加容易；最后这篇论文展示了如何通过融合被下采样后的低级（？）特征来大大改善这个框架。</p><h3>数据依赖型的上采样</h3><h4>原有方法</h4><p>令$Y\in {0,1,2,...,C}^{H \times W}$是数据集的ground truth标签map，其中$H$表示原图的高度，$W$表示原图的宽度，$C$表示分割数据集中的种类数量。</p><p>令$\hat{F} \in R^{\hat{H}\times \hat{W}\times \hat{C}}$是CNN组成的编码器的输出，其中$\hat{H}$表示输出特征图的高度，$\hat{W}$表示编码器输出特征图的宽度，$\hat{C}$表示编码器输出特征图的通道数量。经过下采样后，编码器输出的特征图的大小往往要比原图小很多。我们姑且认为$\frac{\hat{H}}{H} = \frac{\hat{W}}{W} = \frac{1}{16}$或$\frac{1}{32}$。</p><p>由于分割需要逐像素的预测，所以一般会将$\hat{F}$上采样到$Y$所在的空间大小后才能计算损失，所以我们令$F = bilinear(\hat{F})$表示双线性上采样后的$\hat{F}$。在典型的分割方法中，损失函数可以表示为：
$$
L(F, Y) = Loss(softmax(F), Y))
$$</p><h4>存在问题</h4><p>是一种典型的交叉熵损失函数。但是这篇论文表双线性上采样太简单并且在特征图上采样重建的结果上表现得并不好，没有达到最佳。为了弥补双线性上采样的重建能力的不足，解码器不得不将CNN编码器产生的低分辨率特征图先上采样到一个较高的分辨率作为双线性上采样的输入。就像之前说的，可以使用空洞卷积解决这个问题，但同时也带来了更高的计算性能消耗。例如，在空洞卷积中将间隔步幅从16减小到8会导致超过3倍的性能消耗。</p><h4>提出方案</h4><p>有一个值得注意的问题，就是标签$Y$并不是i.d.d（Independent and identically distributed，独立同分布）的，也就是说，Y内部包含了很多结构信息，也就是说，尝试压缩Y使其变为$\hat{Y} \in R^{\hat{H}\times \hat{W}\times \hat{C}}$，可能不会造成太大的损失。之前大家的思路都是将$\hat{F}$上采样到和$Y$一样大的$F$，然后做损失函数；而这篇论文尝试将$Y$下采样到和$\hat{F}$一样大的$\hat{Y}$，然后在$\hat{Y}和\hat{F}$之间计算损失函数。</p><p>为了将$Y$下采样为$\hat{Y}$，该论文设计了一种变换，目的是最小化将$Y$变为$\hat{Y}$的导致的重构误差。具体来说：</p><ol><li>令$r = \frac{\hat{H}}{H} = \frac{\hat{W}}{W}$（通常这个值是16或者32）；</li><li>将$Y$划分为$\frac{r}{H}\times \frac{r}{W}$（如果不能整除，则填充到可以整除）大小的窗格，也就是有$r \times r$个子窗格；</li><li>对于每个子窗格$S\in {0,1}^{r \times r \times C}$，我们将其reshape为$v \in {0,1}^N$，其中$N = r\times r\times C$；</li><li>最后我们将$v$压缩为$x\in \R^{\hat{C}}$，然后将所有的X按照原有的垂直和水平顺序堆叠形成$\hat{Y}$。</li></ol><p>虽然还有很多办法能实现这种压缩，但是这篇论文在实验中发现使用简单的线性投影即将$v$与另一个投影矩阵$P\in \R^{\hat{C}\times N}$相乘就能达到不错的效果。正式地说，就是：
$$
x = Pv; \hat{v} = Wx
$$
其中$P\in \R^{\hat{C}\times N}$是用来将$v$压缩为$x$的矩阵，$W \in \R^{N\times \hat{C}}$是一个逆投影矩阵，用于将$x$重建回$v$，而$v&#x27;$代表了重建后的$x$。在上述参数中这篇论文省去了很多偏移项。$P$和$W$可以通过在训练集上最小化$v$和$\hat{v}$的运算求得。正式地说，就是：
$$
P^<em>, W^</em> = {argmin}<em>{P,W}\sum</em>{v}{||v - \hat{v}||}^2 , {argmin}<em>{P,W}\sum</em>{v}{||v - WPv||}^2
$$
你可以选择使用标准的SGD（随机梯度下降）方法来迭代优化计算这两个参数值。由于有正交性约束，我们也可以简单地使用<a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">PCA（主成分分析）</a>的方法来实现封闭式的解决办法（你可以暂时不懂这个方法）。</p><p>我们可以$\hat{Y}$来预训练这个网络：
$$
L(F, Y) = {||F - \hat{Y}||}^2
$$
接下来，由于我们拿到了训练的参数$P$和$W$，我们用学习到的重构矩阵$W$来上采样$\hat{F}$得到$F$，然后计算$F$与$Y$之间的误差，而不是对$Y$进行压缩处理：
$$
L(F, Y) = Loss(softmax(DUpsample(\hat{F})), Y))
$$
其中，$DUpsample(x)$是一个对输入$x$使用训练得到的进行线性上采样的过程。 相较于原来的方法，这个方法使用学习到的$W$进行上采样，很类似于$1\times 1$卷积的过程，只不过这次的卷积核是学习得到的$W$：</p><p><img src="./src/Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation/image-20210503182340370.png"/></p><p>这就是这篇论文提出的“DUpsample”的过程，上图是一个示意图。</p><p>当然，除了学习一个线性的上采样过程，这篇论文在实验中也尝试了学习一个非线性的“自动编码器”进行上采样实验，这比线性的上采样能够更大程度上地减小重建损失。不过在实验中这种方法的分割精度和学习一个线性上采样后得到的分割精度是几乎一样的。所以我们在这篇论文中只关注了更简单的线性的重建方法。</p><h4>本方法之于Depth-to-space和Sub-pixel方法的讨论</h4><p>Depth-to-space和Sub-pixel的方法在<a href="./%5B01%5DThe-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">这篇</a>文章中有过概述，可以参考其中的相关部分进行简要了解。</p><p>DUpsample的最简单的线性形式可以看作是Depth-to-space和Sub-pixel的带有预训练的上采样卷积核的改进版本。为了避免可训练参数（trainable variables）过多导致优化困难，通常情况下Depth-to-space和Sub-pixel方法会控制上采样的次数或比率（比如控制上采样率为4）。但是由于本方法使用预训练的上采样，所以理论上可以很大程度上提高上采样率。</p><h3>将DUpsample和Softmax相结合</h3><p>之前我们已经对DUpsample如何取代老式的双线性上采样进行了描述，接下来的一步是将DUpsamle结合到编码器-解码器结构中，从而形成端到端的可训练系统。虽然能通过$1\times 1$卷积直接将其融合到其中，但是这样仍然会产生一些麻烦，比如加大训练难度（毕竟层数变多了）。可能是由于$W$的学习过程中损失的计算使用了one-hot编码的$Y$（训练集的 ground truth），这篇论文的作者在实验中发现DUpsample方法连接Softmax函数时很难直接产出足够“尖锐”的值，这使得交叉熵损失函数可能在训练过程中被“卡住”，从而导致训练过程收敛缓慢。</p><p>为了解决该问题，这篇论文提出在softmax中添加temperature参数$T$，让softmax的激活能力更加尖锐或是软弱：
$$
Softmax(z_i) = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}
$$
由于参数$T$可以使用标准的反向传播算法自动学习，在这篇论文的实验中，作者证明自适应的temperature参数$T$能够让训练收敛得更快而无需引入更多超参数。</p><h3>更加灵活地聚合卷积产生的特征</h3><p>足够深的CNN在计算机视觉的很多领域获得了成功，但是过深的CNN会导致很多语义分割需要的细粒度信息丢失，同时有很多相关研究（比如ResNet之类的）表明，结合卷积程度较低的特征能够增强分割性能。</p><p>令$\hat{F}$为最终的CNN输出的特征图，该特征图将会被用于双线性上采样或是上述的DUpsample方法进行像素级的预测；$\hat{F}<em>i$和$\hat{F}</em>{last}$分别表示在骨干特征提取网络的第$i$层和最后一层输出的特征图。在这里，为了简单起见，这篇论文只对融合一个低层级特征进行了讨论，但是这个方法可以扩增至融合更多不同层级的特征，这也许会进一步提升分割的性能。</p><p><img src="./src/Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation/image-20210503104215395.png" alt="image-20210503104215395"/></p><p>上图中（这就是一开始的那张DeepLab的简要示意图）解码器（Decoder）部分的特征聚合阶段可以简要表示为：
$$
F = f(concat(F<em>i,upsample(F</em>{last})))
$$
其中$f$表示某种CNN，$upsample$使用的是双线性的方法。$concat$是在通道（channel）维度的串联运算符。这就产生了一个问题：由于$f$是CNN，所以其计算开销与输入直接相关，从而导致这种结构无法利用很低级的特征（因为在CNN中，越低级的特征可能对应的特征图就越大）。但是，为了获得更好地分割效果，在高分辨率图像上，解码器只能选择低层级特征进行聚合。</p><p>相反，在这篇论文提出的框架中，由于DUpsample的引入，更高层级的特征能够更好地恢复到更大的分辨率上，所以我们可以放心地将任何低层级特征下采样到$F<em>{last}$，然后将特征聚合用于最终的预测。在这种方法中，上个等式也将被改写为：
$$
F = f(concat(downsample(F_i),F</em>{last}))
$$
从改写之后的等式我们可以看出，$upsample$操作被取消，取而代之的是一个$downsample$操作，这使得计算的效率被大大提高。在这篇论文后面的实验环节中，这种方法也被证明能够很大程度上提升分割的性能和表现。</p><p>只有在使用DUpsample的时候，才能放心地使用之前提到的对低层级特征进行下采样的方法；否则整个分割网络的性能会被不够优秀的最终一层的上采样方法所限制。这就是为什么传统的方法中往往需要解码器将低分辨率的高层特征上采样到高分辨率低层级特征后才进行特征聚合的原因。</p><h2>实验</h2><p>实验部分请自行阅读<a href="https://arxiv.org/pdf/1903.02120.pdf">原论文</a></p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HLA-Face Joint High-Low Adaptation for Low Light Face Detection]]></title>
        <id>HLA-Face Joint High-Low Adaptation for Low Light Face Detection</id>
        <link href="https://ml.akasaki.space/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇讲低光照人脸检测的论文。原论文（HLA-Face Joint High-Low Adaptation for Low Light Face Detection）。]]></summary>
        <content type="html"><![CDATA[<p>这是一篇讲低光照人脸检测的论文。<a href="https://arxiv.org/pdf/2104.01984.pdf">原论文（HLA-Face Joint High-Low Adaptation for Low Light Face Detection）</a>。</p><ul><li>充分利用现有的正常光数据，并探索如何将面部探测器从正常光线调整到低光。这项任务的挑战是，正常和低光之间的差距对于像素级和物体级别来说太大而复杂。因此，大多数现有的lowlighenhance和适应方法不达到所需的performance。</li><li>本文是DARK FACE为基准，针对现有的正常照度图像，将图像调整成低照度图像，不需要标签</li><li>一个是像素级外观的差距，例如不足，照明，相机噪声和颜色偏置。另一个是正常和低光场景之间的物体级语义差异，包括但不限于路灯的存在，车辆前灯和广告板。传统的低光增强方法<!-- -->[5,6]<!-- -->设计用于提高视觉质量，因此不能填充语义差距，</li><li>通过使低光图像亮起并扭曲正常光图像，我们构建位于正常和低光之间的中间状态。</li></ul><p>摘要:</p><blockquote><p>Face detection in low light scenarios is challenging but vital to many practical applications, e.g., surveillance video, autonomous driving at night. Most existing face detectors heavily rely on extensive annotations, while col- lecting data is time-consuming and laborious. To reduce the burden of building new datasets for low light condi- tions, we make full use of existing normal light data and explore how to adapt face detectors from normal light to low light. The challenge of this task is that the gap between normal and low light is too huge and complex for both pixel-level and object-level. Therefore, most existing low- light enhancement and adaptation methods do not achieve desirable performance. To address the issue, we propose a joint High-Low Adaptation (HLA) framework. Through a bidirectional low-level adaptation and multi-task high- level adaptation scheme, our HLA-Face outperforms state- of-the-art methods even without using dark face labels for training. Our project is publicly available at: <!-- -->[https: //daooshee.github.io/HLA-Face-Website/]<!-- -->(https: //daooshee.github.io/HLA-Face-Website/)</p></blockquote><h2>问题</h2><ul><li>在不利照明条件下的面部检测仍然具有挑战性。在不足的图像捕获的图像遭受一系列降级，例如低可见性，密集噪声和彩色铸造。</li><li>现有的目标检测方案很少考虑到在低照度图像上的检测</li><li>监控视频分析和夜间自动驾驶中的潜在风险。</li><li>现有的图像增强增强之后还是检测不到</li><li>黑脸检测的问题是H和L之间的差距对于现有的处理方法来说太大并且复杂。</li><li>不同图像不仅只有像素级的不同，还有针对不同的场景和对象，但是增强和暗化图像只考虑像素级的差距</li><li>目前很多基于低照度的目标检测都需要依赖标签，限制了灵活性和鲁棒性</li></ul><h2>所做工作</h2><h3>相关工作</h3><ol><li>提出了一种对于低照度增强的解决方案，对于暗处人脸检测</li><li>提出了一种暗处人脸检测器不需要使用对于暗处人脸的标签(基于DSFD进行改进)</li><li>提出了一种优秀的方法对于高低级的联合适应</li><li>提出了高低级适应模式</li></ol><h3>总结</h3><ul><li>我们提出了一种框架，用于黑暗人脸检测检测，没有标签的黑暗人脸数据。通过联合低级和高级特征适应，与最先进的面部检测和适应方法相比，我们的模型实现了卓越的性能。</li><li>对于低级特征适应，我们设计了双向方案。通过使用噪音和颜色偏置的低光数据和扭曲常规光数据，我们设置中间状态，并使两个域每个域彼此迈向。</li><li>对于高级别特征的适应，我们引入了功能适应的跨领域自我监督学习。通过基于背景和对比学习，我们全面地关闭了多个域之间的特征距离，并进一步加强了表示。</li><li>我们通过组合低级和高级别的适应方法来提出一种优异的方法。</li></ul><h2>变量名</h2><ul><li><p>$H$:正常照度图像</p></li><li><p>$L$:低照度图像</p></li><li><p>$E(.)$:用于将图像进行增强的网络</p><p>  E(.)的网络结构:</p><p>  <img src="./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503121108631.png" alt="image-20210503121108631"/></p></li><li><p>$D(.)$:用于将图像暗化的网络</p></li></ul><h2>论文创新点</h2><ul><li>为了共同填补像素级和特征级别的黑暗人脸检测，我们提出了一种高低适应（HLA）方案。如图<strong>Figure2</strong>我们在L和H之间设置低级中间状态，并基于这些状态适应相应的高级表示。</li><li>增强和变暗都减少了low level distance。与l-to-h或h-to-l的单向翻译相比，我们的双向翻译：L-to-E(L)和H-to-D(H)，不仅可以缓解适应的难度，还提供更多的工具用于特征级适应。</li><li>通过向彼此推动多个状态的特征空间来减少high-level distance</li><li>通过对比度学习进一步增强了特征表示。</li></ul><h2>怎么做的？</h2><h3>High-Low Adaptation模式</h3><p>设置了一种中间状态在L和H之间，基于这些状态相互适应hign-level表示。通过低照度增强和暗化图像来缩短low-level之间的差距。与传统L-to-H和H-to-L单向转换相比，使用双向转换的模式，也就是L-to-E(L),H-to-D(H).通过把多种状态的特征空间合并到一起能减少hign-level的距离。再通过对比度学习能够增强特征。</p><h3>双向Low-level适应</h3><ul><li><p>两方面问题</p><ol><li>Hign-level的间隙共存可以混淆像素级传输模型。这里是指对于征程图像生成低照度图像的时候像素点转移的时候位置不对。就是指对于高亮部分的像素在生成的时候现有的研究都生成到错误的位置，(高亮部分一般有路灯，车大灯等)现在的一些错误有人身体上生成了奇怪的光，生成路灯的亮度在人脸上。MUNIT能够分辨物体和背景，但是生成的图像不够黑，跟低照度图像相比差别很大。</li><li>对于低照度增强本身来说比较困难。现有的一些低照度增强方法都是按照人类视觉的方式进行增强，不是考虑机器视觉。现在错误有画出了暗区域的边缘，有噪声的部分还是黑的，增强的对比度可能会影响检测性能。此外，DF数据集上幂集的噪声和偏色，现有的去噪和颜色重构方法不足以解决这些问题。</li></ol></li><li><p>解决方案</p><ul><li><p>提出了<strong>双向低级特征适应模式</strong>。正常图像低照度化是一个复杂的过程，可以分解成三个因素：亮度、噪声、色偏。所以对于$L$来说调整亮度变成$E(L)$,对于$H$来说用噪声和色偏形成$D(H)$.这样操作使得$E(L)$和$D(H)$更加相似。这样做降低了生成低照度图像和正常图像的困难。将特定的部分低照度，模型不会收到结构域之间的语义差距的干扰</p><p>  <img src="./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503114617138.png" alt="image-20210503114617138"/></p><p>  <img src="./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503120027621.png" alt="image-20210503120027621"/></p><ol><li><p>增强——变亮:</p><ul><li><p>调整亮度的同时不会让图像产生噪声和色偏(调整亮度后不需要额外再去噪和颜色重构).有些低照度图像亮度不均匀，有一些脸会被路灯照亮有些可能是变黑。因此还要避免过曝光和曝光不足。所以使用了基于<strong>非线性曲线映射</strong>方法
$$
LE(x,A)=x+Ax(1-x);<!-- -->\<!-- -->LE<em>n=LE(LE</em>{n-1};A_n);<!-- -->\<!-- -->
$$
进行迭代。(变量说明:$LE_0$是输入图像,$LE_n$是迭代n次的结果,$A_n$是神经网络预测出来的调整图，是三通道的)曲线映射不会引入额外的噪声或者伪图像(为什么?)，使用了一个7层具有对称Skip-connection的CNN和相应的目标函数。</p></li><li><p>针对增强，并没有进行很强烈的增强，只是一个弱增强，因为如果增强太多会导致噪声突出，隐藏噪声于黑暗之中，成像质量这样更好。对比较亮的区域进行强增强，用两倍的迭代基于上文的公式，扩大曲线估计网络，模型可以更亮的方式增强图像，但缺点是伴随而来的是噪声和色偏，但在$H-to-D(H)$中会进行处理。(文章里的illumination是指图像中某一块比较亮的区域)</p></li></ul></li><li><p>噪声合成:</p><ul><li>像素级别上的噪声可以通过调整亮度减少，但是要减少$E(L)$和$H$之间的距离仍然比较困难。通过分离颜色来引导噪声合成的过程(为什么?)</li><li>首先是进行一次双侧滤波器模糊,使用参数$d=25$和$σ=75$.模糊后的结果$E(L)_{blur}$作为颜色引导噪声合成。</li><li>其次使用Pix2Pix进行训练，将模糊后的结果$E(L)_{blur}$转移到所需要的$E(L)$</li><li>最后，对$H$也进行同样的操作进行添加噪声，最终得到的$H_{noise}$成功模仿了E(L)的噪声，颜色分布的差距在下一步进行处理。</li></ul></li><li><p>颜色抖动(修正):</p></li></ol><ul><li>$D(H)$的颜色分布要匹配$E(L)$，基于统计学分析，我们设置了亮度抖动区间$(0.4,1.2)$,对比度抖动区间$(0.6,1.4)$,饱和度抖动区间$(0.6,1.4)$,色度抖动区间$(0.8,1.2)$ (为什么?)</li></ul></li><li><p>使用<strong>多任务高级特征适应</strong>，很多特征适应方法都是基于图像对齐、伪标签训练、对抗学习等，图像对齐和伪标签训练不能处理差距比较大的。对抗学习不稳定，我们使用图像的自然信息也就是图像本身，进行自监督学习。通过自监督分类器共享域，特征被强制映射到相同的高维子空间，因此使得靠近高级特征间隙。学习方案：</p><p>  <img src="./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503120539220.png" alt="image-20210503120539220"/></p><p>  自监督学习模块结构</p><p>  <img src="./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503121622540.png" alt="image-20210503121622540"/></p><p>  骨干网络:</p><p>  (采用DSFD的主干，VGG16提取出6层多尺度特征:conv3_3,conv4_3,conv5_3,conv_fc7,conv6_2,conv7_2)</p><p>  <img src="./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503121806149.png" alt="image-20210503121806149"/></p><ol><li><p>使得$E(L)$与$H$靠近$(E(L)\lrarr H)$，基于上下文的自我监督学习设计目标，模型通过学习空间上的上下文信息。使用拼图方法能很好地完成上面的目标。将一个3x3的patch放到整个图像上，设这个patch排序数为30，相当于就是一个30分类问题,损失函数为
$$
L<em>{jig}^{E(L)}=L_c(F</em>{jig}^{E(L)},p<em>{jig}^{E(L)}) <!-- -->\<!-- --> L</em>{jig}^{H}=L<em>c(F</em>{jig}^{H},p<em>{jig}^{H}))
$$
(变量说明:$p</em>{jig}$表示排列的标签，$L<em>c$表示交叉熵损失函数,$F</em>{jig}$表示相应域中提取出来的特征(写在变量右上方表示对应域的下的值),E(L)和H共享一个强行把语义特征映射到相同空间的分类头，因此使得高级特征之间的间隙减少，最终损失函数:
$$
L<em>{E(L)\lrArr H}=L</em>{jig}^{E(L)}+L_{jig}^{H}
$$
以jigsaw方法的适应为例子的结构:（其他同理）</p><p>​	从刚才的主干中抽取出6层特征图，然后进行相应jigsaw操作.</p><p><img src="./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503121716018.png" alt="image-20210503121716018"/></p></li><li><p>使得$H$和$D(H)$靠近$(H\lrarr D(H))$，灵感来自于相对学习，在相对学习中，查询一个v,判断这个v是在正样本对还是负样本对，类似点乘的方法。目标函数
$$
L<em>q= -\log \frac{\sigma(v,v^+)}{\sigma(v,v^+)+\sum^N</em>{n=1}\sigma(v,v_n^-)},\sigma(x,y)=\exp(x\cdot y/\tau)
$$
(变量说明:$\tau$是温度超参数，这是一个$(N+1)$分类问题)</p><p>为了减少$H$和$D(H)$之间的差距，我们发挥相对学习的优势，能够使得样本更加靠近正样本。做了一个H正样本对是来源于D(H)的。损失函数为:
$$
L<em>{H\Lrarr D(H)}=L_q(H,D(H)^+,H^-)+L_q(D(H),H^+,D(H)^-)
$$
用这种方式，H和D(H)之间的相似特征会被增强，高级特征间隙更小。在单域上的对比学习中，通过$D(.)$作为增强的一部分，简化上述损失:$L</em>{H\Lrarr D(H)}=L_q(D_i^<em>(H),D_j^</em>(H)^+,D_k^<em>(H)^-)$(变量说明:$D^</em>(H)$有50%的概率是$H$，另外50%的概率是$D(H)$,使用MoCo进行优化)</p></li><li><p>增强$E(L)$$(E(L)\uparrow)$，通过对比学习提高E(L),损失函数为:</p><p>$$
L_{E(L)\uparrow}=L_q(E(L),E(L)^+,E(L)^-)
$$</p></li></ol></li></ul></li></ul><pre><code>* 最终损失函数
    $$
    L=\lambda_{det}L_{det}+\lambda_{E(L)\lrarr H}L_{E(L)\lrarr H}+λ_{H↔D(H)}L_{H↔D(H)}+λ_{E(L)↑}L_{E(L)↑}
    $$
    (说明:$det$下标为检测的损失,$E(L)\lrarr H$为$E(L)$与$H$靠近的损失,$H↔D(H)$为$H$与$D(H)$靠近的损失,$E(L)↑$为增强$E(L)$的损失)

## 实验结果

### 与目前现有方法比较

- 人脸检测

    ![image-20210503112828764](./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503112828764.png)

- 增强效果

    ![image-20210503112918707](./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503112918707.png)

- 暗化效果

    ![image-20210503113047596](./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503113047596.png)

- 无监督域域适应

    无监督域适应是基于Faster-RCNN,但是表现很差，所以使用DSFD作为人脸检测器，使用不同方式进行增强的效果，进行消融实验。

    ![image-20210503113256527](./src/HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection/image-20210503113256527.png)
</code></pre>]]></content>
        <author>
            <name>PommesPeter</name>
            <uri>https://blog.pommespeter.com/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepLab Series]]></title>
        <id>DeepLab Series</id>
        <link href="https://ml.akasaki.space/blog/[06]DeepLab-Series"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[DeepLab系列中包含了三篇论文：DeepLab-v1、DeepLab-v2、DeepLab-v3。]]></summary>
        <content type="html"><![CDATA[<p>DeepLab系列中包含了三篇论文：DeepLab-v1、DeepLab-v2、DeepLab-v3。</p><p>DeepLab-v1：<a href="https://arxiv.org/abs/1412.7062">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>DeepLab-v2：<a href="https://arxiv.org/abs/1606.00915">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>DeepLab-v3：<a href="https://arxiv.org/pdf/1706.05587.pdf">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>在这里我们将这三篇放在一起阅读。</p><p>后来甚至还出现了后续：</p><p>DeepLab-v3+：<a href="https://arxiv.org/abs/1802.02611">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>不过暂时没有写进来的打算。</p><h2>DeepLab-v1</h2><p>DeepLab-v1的原论文是<a href="https://arxiv.org/abs/1412.7062">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a>。</p><blockquote><p>In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or &#x27;atrous convolution&#x27;, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed &quot;DeepLab&quot; system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.</p></blockquote><p>在之前的语义分割网络中，分割结果往往比较粗糙，原因主要有两个，一是因为池化导致空间信息丢失，二是没有利用临近像素点类别之间的概率关系，针对这两点，作者提出了针对性的改进。首先使用<strong>空洞卷积（Atrous Convolution）</strong>，避免池化带来的信息损失，然后使用<strong>条件随机场（CRF）</strong>，进一步优化分割精度。阅读这篇论文应关注的重点问题就是空洞卷积和条件随机场。</p><h3>空洞卷积</h3><p>空洞卷积（Dilated/Atrous Convolution或是Convolution with holes ）的主要作用是在增大感受野的同时，不增加参数数量，而且VGG中提出的多个小卷积核代替大卷积核的方法，只能使感受野线性增长，而多个空洞卷积串联，可以实现指数增长。</p><h4>空洞卷积的优势</h4><ul><li>这种结构代替了池化，它可以保持像素空间信息。</li><li>它由于可以扩大感受野因而可以很好地整合上下文信息。</li></ul><p>Convolution with holes 字如其名，是在标准的卷积核中注入空洞，以此来增加感受野。相比于普通的卷积，空洞卷积多了一个超参数称之为空洞率（dilation rate）指的是kernel的间隔的像素数量。</p><p><img src="./src/DeepLab-Series/Atrous_conv.png" alt="Atrous_conv"/></p><p>上图是一张空洞卷积的示意图。在上图中，三个空洞卷积的大小都是$3\times 3$，而它们的空洞率分别是1、6和24，所以能用相同大小的卷积核得到不同的感受野。</p><h4>空洞卷积的问题</h4><ul><li><p>网格效应（The Gridding Effect）</p><p>空洞卷积层并不能随意设计，例如，我们简单地堆叠空洞率为2的$3\times 3$的空洞卷积核，那么连续三层卷积核在原图上的同个像素位置所对应的感受野如下图所示：</p><p><img src="./src/DeepLab-Series/image-20210514145720970.png" alt="image-20210514145720970"/></p><p>很明显，标圆圈的位置一直没有参与该位置的卷积运算。也就是并不是所有的像素都用来计算了，这会导致信息的连续性损失。这对密集预测（逐像素）的视觉任务来说是致命的。</p></li><li><p>相关性丢失</p><p>原论文中描述问题的话是：</p><blockquote><p>Long-ranged information might be not relevant.</p></blockquote><p>也就是说，我们从 dilated convolution 的设计背景来看就能推测出这样的设计是用来获取 long-ranged information。然而仅采用大 dilation rate 的信息或许只对一些大物体分割有效果，而对小物体来说可能则有弊无利了。如何同时处理不同大小的物体的关系，则是设计好 dilated convolution 网络的关键。</p></li></ul><h4>混合膨胀卷积（Hybrid Dilated Convolution, HDC）</h4><p>对于刚才提到的空洞卷积的问题，论文中提出了一种称为HDC的结构作为解决方案。这个方案具有以下特性：</p><ul><li>对于每层空洞卷积，其最大空洞卷积率的最小公因子不能为1。</li><li></li></ul><h3>条件随机场</h3><p>条件随机场，简单来讲就是每个像素点作为节点，像素与像素间的关系作为边，即构成了一个条件随机场。通过二元势函数描述像素点与像素点之间的关系，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关。所以这样CRF能够使图片在分割的边界出取得比较好的效果。</p><h2>DeepLab-v2</h2><p>DeepLab-v2的原论文是<a href="https://arxiv.org/abs/1606.00915">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a>。</p><blockquote><p>Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called &quot;semantic image segmentation&quot;). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our &quot;DeepLab&quot; system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the &#x27;hole&#x27; algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.</p></blockquote><p>DeepLab-v2对DeepLab-v1的改进是：</p><ul><li>使用了金字塔多尺度特征获得更好的分割效果。</li><li>将骨干网络由VGG替换为了ResNet。</li><li>稍微修改了learning-rate。</li></ul><p>其中ASPP的引入是最大也是最重要的改变。多尺度主要是为了解决目标在图像中表现为不同大小时仍能够有很好的分割结果，比如同样的物体，在近处拍摄时物体显得大，远处拍摄时显得小。具体做法是并行的采用多个采样率的空洞卷积提取特征，再将特征融合，类似于空间金字塔结构，形象的称为Atrous Spatial Pyramid Pooling (ASPP)。</p><h2>DeepLab-v3</h2><p>DeepLab-v3的原论文是<a href="https://arxiv.org/abs/1706.05587">Rethinking Atrous Convolution for Semantic Image Segmentation</a>。</p><blockquote><p>In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter&#x27;s field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3&#x27; system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.</p></blockquote><p>DeepLab-v3的改进是：</p><ul><li>提出了更通用的框架，适用于任何网络。</li><li>将ResNet最后的一些模块替换为使用空洞卷积进行的级联。</li><li>在ASPP中使用了Batch Normolization层。</li><li>去除了条件随机场。</li></ul>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Dataset Collaborative Learning for Semantic Segmentation]]></title>
        <id>Cross-Dataset Collaborative Learning for Semantic Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：Cross-Dataset Collaborative Learning for Semantic Segmentation]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/abs/2103.11351">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+L">Li Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+D">Dong Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yousong Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian%2C+L">Lu Tian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shan%2C+Y">Yi Shan</a></p><p>期刊：CVPR2021</p></blockquote><h2>主要结构</h2><p>DAB：Dataset-Aware Block(数据集感知块)</p><pre><code>    作为网络的基本计算单元，有助于捕获o不同功能数据集之间的同质表示和异构统计。

    主要由，一个数据集不变的卷积层，多个数据集特定的BatchNormal和一个激活层构成。
</code></pre><p>DAT：Dataset Alternation Training(数据集交替训练机制)</p><p>分割结果：</p><p><img src="./src/Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation/20210505160141image-20210505160138997.png" alt="image-20210505160138997"/></p><h2>跨数据域的训练</h2><p>最初的跨数据集训练机制是运用在基于帧的动作识别上的。</p><p>后来通过简单的标签级联和标签映射产塞回给你的混合数据集应用到了目标检测上</p><p>Domain adaptation(DA 领域适应)或knowledge transfer(知识转化)为跨数据训练提供了有效的技术，目的是通过使用来自源域的知识和足够的标记数据来提高带有注释数据不足或缺少的目标模型的性能</p><h2>跨数据集协作学习</h2><h3>参数共享可行性分析</h3><p>对比不同数据集在conv层和bn层的参数分布：</p><p><img src="./src/Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation/20210505101559image-20210503155720590.png" alt="image-20210503155720590"/></p><p>结论：</p><ol><li>Conv层中的权重具有相同的分布，即conv层的参数是可以共享的</li><li>对于BN层，不同数据集的均值和方差的分布具有不同的形状，可能无法共享。</li></ol><p>​		</p><h3>网络结构图</h3><p><img src="./src/Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation/20210505101605image-20210503160330765.png" alt="image-20210503160330765"/></p><p>在数据集感知块(DAB)其作为我们通常的CONV+BN+RELU的一个替代，即基本计算单元的替代</p><p>DAB的结构如下，其实很简单，使用的卷积层是一个固定数据集训练出来的，但是在BN层中选择的是不同的数据集i除拉的BN层。据论文说，这里又一个自动交换机的东西，自动地将不同分布的数据集分配到对应的BN层中。</p><p><img src="./src/Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation/20210505101608image-20210503161147231.png" alt="image-20210503161147231"/></p><p>假设我们有N个数据集，那么每个数据集的BN层可以通过下面这个公式设定
$$
DSBN<!-- -->{<!-- -->D_i<!-- -->}<!-- -->(X_i;\gamma_i,\beta_i) = \gamma_i\hat X_i+\beta_i
$$
其中:</p><ol><li>DSBNa表示的就是该数据集的BN</li><li>$\hat X<em>i = \frac{X_i-\mu_i}{\sqrt{\sigma_i^2+\epsilon}}$，$\mu_i=\frac 1B\sum</em>{j=1}^BX<em>i^j$，$\sigma_i^2=\frac 1B\sum</em>{j=1}^B(X_i^j-\mu_i)^2$</li><li>$\mu_i,\sigma_i^2$是均值和方差</li><li>$\gamma_i,\beta_i$表示仿射变换参数</li><li>B是一个batch的数量</li></ol><p>通过学习$\gamma_i,\beta_i$两个BN层的仿射参数，来捕获特定的数据信息。</p><h2>数据集交替训练</h2><p>目的：减小不同特征图分布的差异引起的训练不稳定性</p><h3>DAT机制</h3><p>在每次迭代过程中，针对不同的数据集进行不同batch的设置，先执行一个数据集的batch获得Loss1后，执行下一个数据集的batch获得loss2，直到获得所有的Loss，将他们相加起来最终获得总的loss，再分别对每一个数据集的batch进行反向传播。</p><p>作者尝试设置间隔训练t，即训练A数据集t次，才训练B数据集1次，经过作者实验，得出结论，当t=1时可以获得最好的效果。</p><p>./<img src="./src/Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation/20210505151847image-20210505151844779.png" alt="image-20210505151844779"/></p><h3>DAT优点</h3><ol><li>在每次迭代中反向传播每个数据集的损失将导致训练不稳定。而DAT可以通过优化从不同数据集计算出的总和损失来改进这个问题。</li><li>DAT提供了一种有效的方法来训练来自多个数据集的样本</li></ol><h3>全局损失函数</h3><p>$$
L=-\sum<em>{i=1}^{N} \sum</em>{j=1}^{M} w^{i} y<em>{j}^{i} \log \left(p</em>{j}^{i}\right)
$$</p><p>N：表示数据集的数量</p><p>M：表示图像像素的数量</p><p>$p^i_j和y^i_j$：分别表示第i个数据集的第j个像素的预测概论和相应标签</p><p>$w^i$：表示损失权重，作者将其设置为1，以使这些损失值范围具有可比性</p><h2>实验</h2><h3>实验细节</h3><p>基线：主干网络为ResNet-18的PSPNetn网络在ImageNet预训练的结果</p><p>优化器：SGD</p><ol><li>momentun:0.9</li><li>权重衰减(weight decay):0.0001</li><li>batch size:8</li><li>初始学习率：0.01,乘上 $(1-\frac{iter}{maxiter})^{0.9}$的多项式衰减策略</li></ol><p>图像随机裁剪为$512 \times512$</p><p>评估指标：miou</p><h3>结果</h3><p>实验先是对两个数据集进行实验，采用单数据集(Single-dataset)，微调(Finetuning)，标签映射(Label remapping)，和DAT的不同跨数据训练方式得到的结果如下:</p><p><img src="./src/Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation/20210505155011image-20210505155009177.png" alt="image-20210505155009177"/></p><p>对三个数据集训练的结果也有较好的提升：</p><p><img src="./src/Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation/20210505155137image-20210505155136346.png" alt="image-20210505155136346"/></p>]]></content>
        <author>
            <name>Zerorains</name>
            <uri>https://github.com/zeroRains</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Neural Networks - A Survey]]></title>
        <id>Dynamic Neural Networks - A Survey</id>
        <link href="https://ml.akasaki.space/blog/[08]Dynamic-Neural-Networks-A-Survey"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这篇论文是一篇对动态神经网络的综述，原论文"Dynamic Neural Networks: A Survey"主要讲了：]]></summary>
        <content type="html"><![CDATA[<p>这篇论文是一篇对动态神经网络的综述，原论文<a href="http://arxiv.org/abs/2102.04906">&quot;Dynamic Neural Networks: A Survey&quot;</a>主要讲了：</p><ul><li>概念（Introduction）</li><li>常见的动态神经网络<ul><li>Instance-wise Dynamic Networks</li><li>Spatial-wise Dynamic Networks</li><li>Temporal-wise Dynamic Network</li></ul></li><li>推理和训练（Inference and Training）</li><li>常见应用和代表性工作（Applications）</li></ul><p>这篇论文对近些年吸引了很多研究者的动态神经网络进行了较为系统的总结概括。</p><h2>Abstract（摘要）</h2><blockquote><p>Dynamic neural network is an emerging research topic in deep learning. Compared to static models which have fixed computational graphs and parameters at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to notable advantages in terms of accuracy, computational efficiency, adaptiveness, etc. In this survey, we comprehensively review this rapidly developing area by dividing dynamic networks into three main categories: 1) instance-wise dynamic models that process each instance with data-dependent architectures or parameters; 2) spatial-wise dynamic networks that conduct adaptive computation with respect to different spatial locations of image data and 3) temporal-wise dynamic models that perform adaptive inference along the temporal dimension for sequential data such as videos and texts. The important research problems of dynamic networks, e.g., architecture design, decision making scheme, optimization technique and applications, are reviewed systematically. Finally, we discuss the open problems in this field together with interesting future research directions.</p></blockquote><p>动态神经网络近些年的相关研究逐渐变多，比起固定计算图的传统的静态神经网络，动态神经网络能够可以根据输入的具体数据调整它们的结构或是参数，同时在速度和精度方面占有优势。一种比喻是：“在输入较为简单时，动态神经网络可以很快；在输入较为复杂时，动态神经网络可以精度很高”。</p><p>这篇论文概括地介绍了动态神经网络是如何“动态”的，以及动态带来了怎样的优势。 </p><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="http://arxiv.org/abs/2102.04906">原作</a>。本文只是对原作阅读的粗浅笔记。</p><hr/><h2>介绍（Introduction）</h2><p>神经网络随着硬件条件的发展逐渐追求更好的效果和更高的性能。作者将视觉领域的神经网络近十年的发展分为这样几个阶段：</p><ol><li><p>快速发展阶段（Fast developing stage），2012~2015</p><p>神经网络的设计变得多样化，出现了包括AlexNet、VGG、GoogLeNet在内的一系列代表性网络结构。</p></li><li><p>发展成熟阶段（Mature stage），2015~2017</p><p>这个阶段出现了很多至今都起到了很重要的影响的或是依然被大家经常使用的网络结构，例如ResNet、DenseNet等</p></li><li><p>繁荣发展阶段（Properous stage），2017~Now</p><p>人们设计了很多多样化的效果优秀的神经网络，并且大量出现了很多新型的神经网络，例如轻量级网络CondenseNet、ShuffleNet，利用自动搜索技术设计的模型NASNet、DARTS，还有这篇论文想要介绍的动态神经网络MSDNet、Block-Drop、Glance and Focus等，以及突然就火起来的Transformer。</p></li></ol><p>直到现在，CNN structure  has been never more varied。以CNN网络为例，CNN在ImageNet上的分类准确率正在逐渐提高，甚至达到超过人类的分类水平。随着模型的准确率逐渐提升，人们提出了这样的问题：</p><blockquote><p>如何平衡网络的精度和网络计算开销之间的关系</p></blockquote><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210507091720858.png" alt="image-20210507091720858"/></p><p>上面这张图片来自一篇名为<a href="https://arxiv.org/abs/1611.10012">Speed/accuracy trade-offs for modern convolutional object detectors</a>的论文，大致描述了一些知名网络结构的<code>开销-精度</code>图。这便是在神经网络成熟之后人们开始关注的问题。人们希望有更加靠近左上角的模型。</p><p>一个既成事实是，模型的精度往往和模型的宽度和深度相关，当希望达到更高的准确率时，往往会增加模型的深度和宽度，但是这往往会提高神经网络计算的开销，也就是说此时模型会走向模型的右上角。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210507093436025.jpg" alt="image-20210507093436025"/></p><p>以上面的图片为例，左图是一只的<a href="https://github.com/coronaPolvo">coronaPolvo</a>，右图是一只<a href="https://github.com/pommespeter">PommesPeter</a>。他们都是我的好朋友，但是我一瞬间即可认出左图，而认出右图则需要几秒钟。这是因为左图画面明亮，且coronaPolvo占据了画面的主体位置；而右侧的PommesPeter并没有占据画面的主体，并且被车的阴影遮挡，光照条件较差难易辨认。</p><p>有很多研究已经标明了人的大脑在处理信息时的处理过程会受到周围环境以及很多其他上下文的影响。<strong>这便是动态神经网路的基本构想：对于简单的样本，可以轻易认出；对于复杂的样本，可以多花一点时间。或者说，人名为能发现传统CNN的一个缺陷</strong>：</p><blockquote><p>Most convolutional neural networks recognize all instance with the same architecture.</p></blockquote><p>所以动态神经网络的能力就是：</p><blockquote><p>Dynamic neural networks can adapt their architecture for different instances.</p></blockquote><p>动态神经网络具有以下优势：</p><ul><li>高效（Efficiency）</li><li>更强的表达能力（Representation power）</li><li>更强的适应性（Adaptiveness）</li><li>兼容性（Compatibility）</li><li>设计的简单性（Generality）</li><li>设计的可解释性（Interpretability）</li></ul><p>下图是这篇论文的整体内容，涵盖非常的广，推荐阅读一下<a href="http://arxiv.org/abs/2102.04906">原文</a>。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210507090536138.png" alt="image-20210507090536138"/></p><hr/><h2>样本自适应的动态神经网络（Instance-wise dynamic networks）</h2><p>为了在简单样本上获得更快的推理速度，以及在复杂样本上获得更好的精度，一个最简单并且天真的思路就是，导入多个网络模型，并且在输入较为复杂时使用更复杂的网络，输入简单时使用简单的网络。这个方法基本行不通，因为网络无法提前知道一个输入到底是复杂的还是简单的。</p><p>所以，我们需要研究样本自适应的动态神经网络</p><blockquote><p>Instance-wise dynamic networks can adapt their architectures or parameters to each instance.</p></blockquote><p>对不同的输入样本，这种网络会动态调节自己的结构或参数。</p><p> 这篇论文将样本自适应的动态网络分为：</p><ul><li>动态结构（Dynamic architecture）<ul><li>动态深度（Dynamic Depth）<ul><li>早退机制（Early Existing）</li><li>跳层机制（Layer Skipping）</li></ul></li><li>动态宽度（Dynamic Width）<ul><li>跳过神经元（Skip Neurons）</li><li>跳过通道（Skip Channels）</li><li>跳过分支（Skip Branches）</li></ul></li><li>动态路由（Dynamic Routing）</li></ul></li><li>动态参数<ul><li>动态参数加权（Attention on weight）</li><li>动态卷积核形状（Kernel shape adaptation）</li></ul></li></ul><hr/><h3>动态结构（Dynamic Architecture）</h3><h4>动态深度（Dynamic Depth）</h4><h5>早退机制（early escape）</h5><p>简而言之，动态深度就是网络会根据某种机制判定样本是简单的还算复杂的，对于难一点的样本，网络可以将其一算到底，而对于简单一些的样本，网络计算到中间的时候就可以停止计算了。</p><p>有两种常见的实现方式：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509114605362.png" alt="image-20210509114605362"/></p><ul><li><p>(a)串联多个模型（Cascading of DNNS，比较早期的工作）</p><p>在这种方法中，一个网络包含了由浅到深的多个模型：输入通过某个模型后得到输出，经过训练得到的“决定函数”会根据输出的具体情况决定是将特征图直接放入线性分类器还是再次输入到下个模型中继续推理。</p><p>这种方法有一个问题，很多情况下，特征并不能被复用，或者说不同模型并不能很好地“级联”在一起，例如上图中(a)，若$Model_1$是VGG，而$Model_2$是ResNet，那么很明显这样的设计会导$Model_1$的输出并不能能作为$Model_2$的输入，从而演化成花费两个网络的计算开销，导致额外的性能浪费。</p></li><li><p>(b)添加中间出口（Network with intermediate classifiers）</p><p>如上图中(b)，这种方法往往通过在一个骨干网络的每个模块之后插入出口实现，即在每个模块的后方加入一个出口，通过设计不同的退出准则来决定模型每经过一个模块的计算之后是否已经不再需要后续网络了，从而实现早退。</p></li></ul><p>然而，这种使用早退的方法并不是最优的。有研究标明，如果在模型中添加中间出口，往往会影响模型的分类性能。原因是CNN的更深层输出的feature map往往才具有更多的语义信息，提前退出往往会导致特征提取“并不到位”。</p><p>比较简单的一种解决方法是使用多尺度的、密集连接的网络架构：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509142111488.png" alt="image-20210509142111488"/></p><p>在上图中，较低维度的特征通过<code>concat</code>等操作融合到深层的特征中参与分类，这样不同尺度的特征都能够参与分类。其是否退出的标准为某个分类器的置信度是否达到某个阈值。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509170120613.png" alt="image-20210509170120613"/></p><p>例如，在上图中，一张猫的照片参与了分类，当网络在某个分类器上的输出（一般是<code>softmax</code>输出）的置信度达到一定值的时候，网络就可以退出了，后面的网络将不被执行。从而实现了动态减小网络的计算量。</p><p>还有许多具有早退机制的网络，如果我看到了会单独写一节来介绍。</p><h5>跨层连接（skip connections）</h5><p>早退机制是通过在网络执行的某个阶段退出从而节省计算开销的，而跨层连接的动态神经网络则会执行完整个网络，只是在网络的中间层会出现跨层的连接方式。跨层连接一般被实现在一些具有类似于<code>skip connection</code>或是<code>residual connection</code>的网络结构中。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509171717763.png" alt="image-20210509171717763"/></p><p>跨层连接的一种简单实现方法是在具有跨层连接的网络中加入<code>Gating module</code>。如上图，经过训练的<code>Gating module</code>如果输出为1，则不跳过这一层；</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509171732525.png" alt="image-20210509171732525"/></p><p>当<code>Gating module</code>的输出为0时，就跳过这一层的计算，即输入直接被当作输出。<code>Gating module</code>一般是包含一个被称为门控函数（Gating function）的计算单元。</p><p>还有一种实现跳层的方法：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509172345469.png" alt="image-20210509172345469"/></p><p>如上图，输入在正式进入主干网络之前会先经过一个被称为<code>Policy Network</code>的网络，这个网络会决定对于当前输入应该跳过哪些层，并通知主干网络这样做。<code>Policy Network</code>的输出一般是一个向量，其中包含了哪些层应该被跳过的信息。</p><p>还有许多具有跳层机制的网络，如果我看到了会单独写一节来介绍。</p><h4>动态宽度（Dynamic Width）</h4><h5>动态通道数（Dynamic channel pruning in CNNs）</h5><p>动态宽度的动态神经网络，顾名思义，这种网络会根据输入动态调整网络的宽度。一种比较简单的思路是，动态调整通道数量：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509173020737.png" alt="image-20210509173020737"/></p><p>在上图中，也出现了一个被称为<code>Gating module</code>的模块，它根据输入的不同会向卷积层提供一个向量，该向量决定了卷积层跳过输出哪些通道。一种可能的方法是，该向量由0和1组成，卷积层将保留1代表的通道，而取消0代表的通道。</p><p>另一种可行的方法是使用多个不同宽度的网络对输入进行处理，当某个深度的网络的输出（例如<code>softmax</code>输出）达到某个阈值时，就不再加深网络了。</p><h5>专家子网络加权（Mixture of Experts，MOE）</h5><p>这种方法是通过将很多不同的网络的结果进行动态加权来提升网络性能的一种方法。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509174050883.png" alt="image-20210509174050883"/></p><p>上图是两种可行的思路：</p><ul><li>(a)是一种“软加权”，对多个网络的输出进行动态加权，每个子网络都会被执行完，通过调节加权达到更好的性能。</li><li>(b)是一种“硬加权”，通过一个<code>Gating Module</code>决定某个子网络是否参与决策。如果某个子网络不参与决策，则它根本不会被执行。</li></ul><p>请注意，这种方法会加大计算量和参数量。在后面的动态参数方法中，会介绍一种和该方法思路很类似的方法。</p><h5>动态全连接层大小（Dynamic width of fully-connected layers）</h5><p>自如起名，动态修改全连接层的大小，不需要任何额外模块和设计。这里不做详细介绍。</p><hr/><h4>动态路由（Dynamic Routing）</h4><p>上面介绍的动态深度和动态宽度的方法广义上实际上都能视为某种简单的动态路由方法。这里的动态路由单独拿出来，指具有更加复杂的超网络结构（超网络不再是简单地链式结构）的动态路由，这种结构甚至会给不同的样本以不同的计算图。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509114605362.jpg"/></p><p>这里有两个可能的设计：</p><ul><li>(c)是一种树状结构</li><li>(d)是一种多尺度的动态结构</li></ul><hr/><h3>动态参数（Dynamic Parameters）</h3><p>动态参数指的是网络会根据输入的不同使用不同的参数对输入进行运算。可能的动态参数方法有：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510083907081.png" alt="image-20210510083907081"/></p><ul><li>(a)使用一个动态参数加权调节（Parameter Adjustment）模块，根据输入产生一个影响运算参数的参数。</li><li>(b)使用一个动态参数产生（Parameter Generation）模块，根据输入的不同产生不同的参数比如新的卷积核，对输入进行运算。</li><li>(c)软注意力（Soft attention）方法</li></ul><h4>动态参数加权（Attention on weight）</h4><p>动态参数的设计能够提升模型的表达能力。下面举一个简单地例子进行说明：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510084536681.png" alt="image-20210510084536681"/></p><p>在上图中<code>+</code>表示加和，$\alpha_1$、$\alpha_2$、$\alpha_3$分别表示一个动态参数调节模块产生的权重。根据输入的不同，这三个提前设定的卷积核通过不同的权重加权形成新的卷积核。上图中的这种操作等效于：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510084826504.png" alt="image-20210510084826504"/></p><p>输入分别与三个不同的卷积核进行运算，并且通过$\alpha_1$、$\alpha_2$、$\alpha_3$三个权重加权形成输出。这种设计让人不禁想到在动态结构的设计中出现的专家子网络加权（Mixture of Experts，MOE）方法。不过之前的专家子网络加权方法在这种情形下要卷积三次，而动态参数加权的设计只卷积一次。</p><p>上面这两种等效的表达可以写为下列等式：
$$
(\sum<em>{n}a_n w_n)\cdot x = \sum</em>{n}a_n(w_n\cdot x)
$$</p><h4>动态卷积核形状（Kernel shape adaptation）</h4><p>动态卷积核形状的方法能根据输入的不同调节卷积核的形状，以此来获得不同的感受野。比较著名的相关工作是一篇叫做Deformable Convolutional Networks（可形变卷积网络）的论文。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510091338617.png" alt="image-20210510091338617"/></p><p>上图是论文Deformable Convolutional Networks中的示意图。</p><h4>动态参数和注意力的关系（Dynamic features or Dynamic weights）</h4><p>有一个很好的问题：</p><blockquote><p>The goal of dynamic parameters is generating dynamic features. So why not rescale features directly with attention?</p></blockquote><p>注意力机制也是为了动态产生特征的，为什么还要有动态参数的方法呢？例如，在较为出名的transformer中，是通过key和query的相似度对value进行动态调节；还有SENet（<a href="https://arxiv.org/abs/1709.01507">Squeeze-and-excitation networks</a>）中，对不同通道进行的动态调节。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510094907451.png" alt="image-20210510094907451"/></p><p>上图是SENet中提及的方法，输入经过正常的卷积运算产生一些channel，同时一个注意力模块（Attention Module）接收输入并产生一个注意力向量，作用于卷积产生的这些channel，使它们被乘以不同的权值。这种方法被称作通道注意力（Channel-wise attention）。</p><p>所以在某些角度上我们可以说：
$$
DynamicFeatures = DynamicWeight
$$
表示为公式就是：
$$
(x\times W)\otimes \alpha  =  x\times(W\otimes \alpha)<!-- -->\<!-- -->
$$
其中，符号$\otimes$是克罗内克积，可以查阅相关资料进行了解。</p><p>上述公式中，等号左侧是动态卷积，先使用$W$对输入的$x$完成卷积，再乘上参数$\alpha$；等号右侧是动态参数，先使用参数$\alpha$影响卷积参数$w$，再对输入的$x$进行卷积。它们在数学上是等价的。</p><hr/><h2>空间自适应的动态神经网络（Spatial-wise dynamic networks）</h2><p>传统的卷积神经网络存在一个问题：</p><blockquote><p>Most conventional networks perform the same computation across different spatial locations of an image.</p></blockquote><p>对于一张图片，在不同位置包含的信息量可能是不一样的。所以传统网络对图像中每个不同的位置使用相同的运算方法听上去会带来很多冗余的计算量。</p><p>在这篇论文中，作者将空间自适应方法大致分：</p><ul><li>像素级（Pixel Level）<ul><li>动态结构（Dynamic Architecture）</li><li>动态参数（Dynamic Parameter）</li></ul></li><li>区域级（Region Level）<ul><li>动态变换（Dynamic Transformation）</li><li>硬注意力（Hard Attention）</li></ul></li><li>分辨率级（Resolution Level）<ul><li>自适应缩放率（Adaptive Scaling Ratios）</li><li>多尺度架构（Multi-scalue Architecture）</li></ul></li></ul><h4>像素级自适应（Pixel Level）</h4><p>像素级的一种代表工作如下：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510144149172-1620628910961.png" alt="image-20210510144149172"/></p><p>在上图中，对于输入，经过一个轻量化的计算得到一个Mask代表哪些位置是重要的。根据生成的Mask进行一个稀疏的卷积，并得到一个稀疏的输出。对于没有被Mask覆盖的区域，可能通过Skip Connection等方式直接跳过运算，直接得到输出。</p><h4>区域级自适应（Region-Level）</h4><p>区域级空间自适应方法的一种可行的方法是：</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510145118821.png" alt="image-20210510145118821"/></p><p>将图片输入到一个选择器中，选择器选出其中比较重要的一部分（抠出一个小Patch），并只将这一部分输入进网络或运算模块。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510145427441.png" alt="image-20210510145427441"/></p><p>上图是一篇名为<a href="https://arxiv.org/abs/2010.05300">Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification</a>的相关论文中的一个示意图。对于较为简单的样本，在一个小分辨率上直接得到可信的预测；对于复杂的样本，在网络得到置信度很高的输出之前，不断从图片中选择“较为重要”的一部分继续推测。</p><p>这种方法除了实现了一种注意力机制之外，还实现了网络的早退。</p><h4>分辨率级自适应（Resolution-Level）</h4><p>分辨率自适应也是一种基于早退的方法，其基本构想是使用递进的网络深度处理不同分辨率的输入。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510150347394.png" alt="image-20210510150347394"/></p><p>上图截取自相关工作RANet（<a href="https://arxiv.org/abs/2003.07326">Resolution Adaptive Networks for Efficient Inference</a>），对于简单的输入，使用一个很小的子网络，若达到很好的置信度输出，则早退；对于复杂的输入，使用更深的网络进行推断。</p><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510150607144.png" alt="image-20210510150607144"/></p><p>上图是RANet的一种推断过程。图中各层<code>Conv Block</code>之间的蓝色箭头实现了特征复用。</p><h2>时间自适应（Temporal-wise dynamic network）</h2><p>对于序列的输入，例如视频或文本的输入，可以使用时间自适应的动态网络结构。</p><p>这篇论文将时间自适应的网络分为：</p><ul><li>用于处理文本（Text）<ul><li>动态更新隐藏态（Dynamic Update of Hidden States）</li><li>时间早退（Temporally Early Exiting）</li><li>动态跳跃（Dynamic Jumping）</li></ul></li><li>用于处理视频（Video）<ul><li>动态循环神经网络（Dynamic RNNs）<ul><li>动态更新隐藏态（Dynamic Update of Hidden States）</li><li>时间早退（Temporally Early Exiting）</li><li>动态跳跃（Dynamic Jumping）</li></ul></li><li>帧采样（Frame Sampling）</li></ul></li></ul><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510151837557.png" alt="image-20210510151837557"/></p><p>上图是常见的时间自适应的设计。 其中：</p><ul><li>(a)中的<code>Agent</code>接受时间戳为$t$的输入$x<em>t$，并判断该输入是否重要。若重要，则保留输入进网络并获得当前时间戳输出$h_t$；若不重要，则直接将上一个时间戳$h</em>{t-1}$作为输出，不进行任何计算。</li><li>(b)中通过更窄一点的RNN模块进行更新，对于$h_{t-1}$和$h_t$中下半部分灰色的部分也是直接copy的。</li><li>(d)的基本思路是采样输入中较为重要的位置，具体来说就是通过自适应让<code>RNN</code>模块跳过输入视频中的一部分帧。例如<a href="https://arxiv.org/abs/1804.00931">Dynamic Video Segmentation Network</a>。</li></ul><hr/><h2>训练和推理</h2><h3>推理（Inference）</h3><h4>基于置信度（Based on confidence）</h4><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509170120613.png" alt="image-20210509170120613"/></p><p>早退机制的常用方法。当网络在某个阶段输出的<code>softmax</code>置信度达到某个阈值时网络就会退出。该方法不需要特殊的运算和设计，仅需要设计一个阈值。</p><h4>基于政策网络（Based on Policy Networks）</h4><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210509172345469.png" alt="image-20210509172345469"/></p><p>该方法在跳层实现的动态神经网络中常见。这种<code>Policy Network</code>的设计往往是基于主干网络的结构的，比如其输出的长度可能等于主干网络的<code>Block</code>总数，因此需要专门设计，并不是很通用。</p><h4>基于门控函数（Based on Gating Functions）</h4><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510204008178.png" alt="image-20210510204008178"/></p><p>这种方法在很多动态神经网络中常见。通常<code>Gating Function</code>的职责可能是：</p><ul><li>决定哪些网络层被跳过</li><li>决定哪些专家网络（Expert Network）被执行</li><li>决定哪些通道（Channel）被忽略</li><li>决定图像中的哪些区域被采样和卷积的</li></ul><p><code>Gating Function</code>的计算量通常较小，并且设计上具有即插即用的特点，更加常用。但是这类方法也有一个缺点，就是更长的训练过程。</p><hr/><h3>训练（Training）</h3><h4>训练目标（Objectives）</h4><h5>早退网络的目标函数（Multi-exit Networks）</h5><p>一种比较普通的思路是：
$$
L(y,f(x;\theta)) = \sum<em>{i}\lambda_i CE(y,f_i(x;\theta_i))
$$
其中，$CE$是交叉熵损失函数，$\lambda_i$是第$i$个退出位置的权值。有的工作中甚至$\lambda_i$设置恒等于1。也有更加复杂的用于提升多个不同分类器组合训练效果的目标：
$$
L_i = \alpha CE_i + (1-\alpha)KLD_i<!-- -->\<!-- -->
KLD_i = -\sum</em>{c\in Y}p_k(c|x;\theta ,T)\log\frac{p_i(c|x;\theta ,T)}{p_k(c|x;\theta ,T)}
$$</p><h5>鼓励稀疏度的目标函数（Training Objectives for Encouraging Sparsity）</h5><p>这类目标函数主要是为了训练跳层或是忽略通道的动态神经网络而设计的。通常情况下，这类网络需要动态调用网络中不同的模块。如果不单独设计新的目标函数，可能会导致网络陷入“尽可能多地调用网络的模块”或是“网络中某个模块一次也没有被调用”。因此很多工作加入了一种称为鼓励稀疏度的目标函数：
$$
L = L<em>{task} + L</em>{sparse}
$$
其中，$L_{sparse}$可以但不限于表示：</p><ul><li><p>被激活或是被调用的网络模块的比率。很明显的是，当网络试图“尽可能使用全部模块”时，目标函数的值会增大。</p></li><li><p>动态调用的模型的复杂度，可以是计算量或是参数量的估算值（FLOPs）。</p></li></ul><h4>优化方法（Optimization）</h4><h5>梯度估计（Gradient Estimation）</h5><p>一种可行的方法是使用<code>Gumbel-Softmax</code>对离散变量再参数化，这种方法主要用于优化<code>Gating Function</code>，使其嫩能构成完整的端到端训练系统。坏消息就是这种方法又引入了一个称为<code>Gumbel Noise</code>的随机变量，需要更长的训练过程，并且容易对某些超参数敏感。</p><h5>强化学习（Reinforcement Learning）</h5><p>强化学习已经被应用于很多决策任务中。强化学习在动态神经网络中主要用于决定跳层、决定选择哪些Patch以及决定采样哪些帧。</p><hr/><h2>Whatever Disscussion</h2><p><img src="./src/Dynamic-Neural-Networks-A-Survey/image-20210510215257391.png" alt="image-20210510215257391"/></p><p>动态神经网络已经被应用于很多领域中（如上表）。</p><p>最后，这篇论文还对一些开放性问题进行了讨论：</p><ul><li><p>动态神经网络理论（Theories）</p><p>例如在多出口的网络里，早退机制的判断标准的阈值决策方法是没有很好的理论支撑的。因此，研究具有理论保障的动态决策方法也许可以进一步提升网络性能。</p></li><li><p>结构设计（Architecture Design）</p><p>多尺度的动态网络结构设计说明了特别的网络设计对于提升网络的效率是有很大帮助的：在链式的结构中，浅层的feature并不能很好的被用来分类，而使用多尺度的设计就能让分类器达到更好地效果。</p></li><li><p>在不同任务上的可用性（Applicability on more diverse tasks）</p><p>当前的很多动态神经网络都是为了分类任务而设计的。虽然很多思想（例如早退和跳层）理论上可以被应用于其他任务，但是大多数现存的动态网络设计并不能像ResNet、DenseNet一样被作为一个比较通用的主干网络，用于很多下游的任务。</p></li><li><p>理论和实际的差距（Gap between theoretical &amp; practical efficiency）</p><p>现在的一些动态神经网络受限于计算机硬件或是软件库的设计，并不能达到理论上的最优效果，比如上文中提到的一些空间自适应的设计，当前的图形处理器（GPU）及其运算库的设计对静态神经网络以及整张图片的卷积采样运算优化已经很好了，使用这种设计反而需要单独进行特殊设计。一个重要的议题是如何设计出在当前硬件条件下所带来的运算效率提升。</p></li><li><p>鲁棒性（Robustness）</p><p>已经有工作研究了多出口网络在对抗攻击方面的鲁棒性，也有一个很有意思的网络，不但对网络的准确性发起攻击，还对网络的效率发起攻击，这也是一个研究方向。</p></li><li><p>可解释性（Interpretability）</p><p>动态神经网络在时间和空间上的硬注意力机制和人的神经系统决策过程也是非常相似的。我们在看一张图片的时候也可能是先粗略地看一眼，然后再单独去看每个小Patch；人在看视频的时候也可能只需要看一些帧就能大概Get到这个视频在讲述什么。</p></li></ul>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Pyramid Networks for Object Detection]]></title>
        <id>Feature Pyramid Networks for Object Detection</id>
        <link href="https://ml.akasaki.space/blog/[09]Feature-Pyramid-Networks-for-Object-Detection"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这篇笔记的写作者是VisualDust。]]></summary>
        <content type="html"><![CDATA[<h3>这篇笔记的写作者是<a href="https://github.com/visualDust">VisualDust</a>。</h3><p>原论文<a href="https://arxiv.org/abs/1612.03144">Feature Pyramid Networks for Object Detection</a>。</p><p>这篇论文就是大家熟知的FPN了。FPN是<strong>比较早期的一份工作</strong>（请注意，这篇论文只是多尺度特征融合的一种方式。不过这篇论文提出的比较早（CVPR2017），在当时看来是非常先进的），在当时具有很多亮点：FPN主要解决的是物体检测中的多尺度问题，通过简单的网络连接改变，在基本不增加原有模型计算量情况下，大幅度提升了小物体检测的性能。</p><h2>Abstract（摘要）</h2><blockquote><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></blockquote><p>这篇论文对以后的许多网络设计产生了较大的影响，推荐你阅读<a href="https://arxiv.org/abs/1612.03144">原文</a>。这里只是对这篇论文的粗浅阅读笔记。</p><hr/><h2>介绍（Introduction）</h2><p>该论文提出，特征金字塔是识别系统中用于检测不同比例物体的基本组件，甚至号称手工特征设计时代的万金油：比如在OpenCV库的特征匹配Cascade分类器用于人脸识别中使用特征金字塔模型+AdaBoost提取不同尺度特征经行分类等。</p><p>原论文这样形容多尺度的好处：</p><blockquote><p>The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p></blockquote><p>在进入深度卷积网络之后，如何在卷积神经网络中更好地利用多尺度称为了一项挑战。一方面，仅使用深度卷积网络进行检测会导致小目标的漏检；另一方面，在每层不同级别的特征图上进行预测产生了多余的性能消耗，并且效果并没有想象的那么好。这篇论文就是为了解决这个问题的。</p><hr/><h2>相关工作（Related works）</h2><ul><li>手工设计的特征和早期神经网络（Hand-engineered features and early neural networks）</li><li>普通的深度卷积目标检测网络（Deep ConvNet object detectors）</li><li>融合了深度卷积网络的特征金字塔模型（Methods using multiple layers）</li></ul><p><img src="./src/Feature-Pyramid-Networks-for-Object-Detection/image-20210512231141905.png" alt="image-20210512231141905"/></p><p>上图为原论文中出现的示意图。其中：</p><ul><li><p>(a)是手工设计特征描述时代的常见模型，即对不同尺寸的图片提取特征，以满足不同尺度目标的检测要求，提高模型性能。</p></li><li><p>(b)是普通的深度卷积网络模式，通过下采样扩大感受野，提取语义信息。</p></li><li><p>(c)是融合了深度卷积网络的特征金字塔模型。深度卷积网络在卷积过程中每层都会产生不同尺度的特征图，所以其本身就天然具有金字塔结构。截止到这篇论文的写作时间为止，有很多在COCO和ImageNet上表现优秀的网络都是pyramid representations的，即让不同层预测不同尺度的物体。但是其对小目标的检测效果仍然不够好，原因在于低尺度的特征图包含的语义信息还不够深刻（说俗话就是欠卷了），原文是这样说的：</p><blockquote><p>This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p></blockquote><p>SSD就是第一批采用这种方法的深度卷积网络之一。不过SSD为了避免使用语义信息不足的特征图，SSD并没能很好地复用已有的特征图，这让它对小目标的检测效果仍然不够好。卷积神经网络的深度往往和每一步的卷积的步长参数是一个很矛盾的东西。展开来说，网络更深的时候，就不得不通过将步长改大以平衡更深的网络带来的参数量上涨问题。但同时这导致了另外一个问题，就是步长很大的时候，甚至可以大过输入图像中一些物体的大小，使得一些目标丢失。</p></li><li><p>(d)是这篇论文要提出的FPN网络结构。该网络在设计时的一个目标就是避免(c)中出现的问题，让每一个尺度的特征图都会包含足够丰富的语义信息。</p></li></ul><hr/><h2>特征金字塔网络（Feature Pyramid Networks）</h2><p>原论文这样描述这篇论文的目的：</p><blockquote><p>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.</p></blockquote><p>也就是说，这篇论文的目的就是利用深度卷积网络天然存在的金字塔特征层次结构：该层次结构利用自底向上的逐层的卷积获得丰富的语义信息，并在整个过程中构建具有高层语义的特征金字塔。</p><p>这篇论文的网络结构设计主要包含了两个部分：</p><ul><li>自底向上（Bottom-up pathway）</li><li>自顶向下（Top-down pathway and lateral connections）</li></ul><p>在接下来的说明中，会经常使用<code>stage</code>的概念。在这里进行提前定义说明：在这篇论文中，网络中输出的feature map大小相同的层被称为是同个stage的。不同stage会产生不同大小的特征图，这篇论文为每个stage定义一个金字塔层级（pyramid level）。</p><p><img src="./src/Feature-Pyramid-Networks-for-Object-Detection/image-20210513105442041.png" alt="image-20210513105442041"/></p><p>例如上图是VGG-16的网络结构图，图中用紫色的文字标出了5个不同的stage。当然，不挨在一起但是输出特征图大小相同的层也是属于同一个stage的。</p><h3>自底向上（Bottom-up pathway）</h3><p>自底向上就是普通深度卷积网络前向传播的过程。对于提及的金字塔（pyramid）特征，这篇论文选择每个stage最后一层的输出作为一个特征图，这个选择听上去很合理，因为越上层（每个stage的最后一个层）的特征就包含更多的语义信息。</p><p><img src="./src/Feature-Pyramid-Networks-for-Object-Detection/image-20210513105825996.png" alt="image-20210513105825996"/></p><p>例如，在上图中，这篇论文的方法选择右侧的特征图输出作为一个stage的特征图输出而不是左侧的。</p><h3>自顶向下（Top-down pathway and lateral connections）</h3><p>自顶向下的结构在直观感受上是和深度卷积网络下采样的过程相反的：它将高层stage输出的空间信息很粗劣（毕竟之前一直在下采样...）但是语义信息很丰富的特征图进行不断上采样，上采样到和其对应的stage的上一个stage的特征图大小，再通过图中这种侧边的连接（lateral connections），使用低一级stage的特征图来增强它。</p><p><img src="./src/Feature-Pyramid-Networks-for-Object-Detection/image-20210513110511093.png" alt="image-20210513110511093"/></p><p>上图是论文中侧边连接（lateral connections）的方式。每个侧边连接将同个stage的自底向上的特征图和自顶向下的特征图相加。注意，该论文中特指了每两个stage之间的空间分辨率之差是2倍，也就是下采样率是2倍。所以上图的示意中靠近顶层的特征图经过2倍上采样后使用侧边连接和上一级特征图相加组成增强后的新特征；技术上，在侧边连接的过程中，这篇论文的方法使用$1\times 1$卷积的方式使需要相加的特征图通道数保持一致。</p><hr/><h2>应用（Applications）</h2><p>有空就写。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review on Deep Learning Techniques Applied to Semantic Segmentation]]></title>
        <id>A Review on Deep Learning Techniques Applied to Semantic Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[10]Overview-Of-Semantic-Segmentation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇关于综述论文的解读。原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）]]></summary>
        <content type="html"><![CDATA[<p>这是一篇关于综述论文的解读。<a href="https://arxiv.org/pdf/1704.06857.pdf">原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）</a></p><p>摘要：</p><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="https://arxiv.org/pdf/1704.06857.pdf">原作</a>。本文只是对原作阅读的粗浅笔记。</p><h2>介绍分割</h2><p>对图像进行分割主要有：语义分割（Semantic segmentation）和实例分割（Instance segmentation）。它们的区别一目了然：</p><p><img src="./src/overview-of-semantic-segmentation/image-20210427154733807.png" alt="image-20210427154733807"/></p><p>左图：原图；中图：语义分割；右图：实例分割。</p><p>很明显，语义分割希望将不同类别的物体所在位置的像素分开来，但是对于相同类别的不同物体并不敏感；而实例分割不但需要分开每一个位置上像素属于哪一类，还要分出它具体属于哪一个对象。</p><p>我们知道一个图像只不过是许多像素的集合。图像分割分类是对图像中属于特定类别的像素进行分类的过程，因此<strong>图像分割可以认为是按像素进行分类的问题</strong>。</p><p>如果你对离散数学以及softmax很敏感的化，肯定第一时间会产生这样的联想：</p><p><img src="./src/overview-of-semantic-segmentation/image-20210427222245438.png" alt="image-20210427222245438"/></p><p>这张图实际上是这样的：</p><p><img src="./src/overview-of-semantic-segmentation/image-20210427222340602.png" alt="image-20210427222340602"/></p><p>当然，对于实际应用中通道数量的具体数字可根据实际需求选择。例如，在前景分割中，仅需分割出前景和背景，因此只需要一个通道。而全景分割中，如果使用类one-hot编码，则需要有和对象数目+1一样多的通道数。</p><h2>分割的技术</h2><p>在深度学习方法流行之前，TextonForest和基于随机森林分类器等语义分割方法是用得比较多的方法。但是本文章的背景是基于深度学习方法的计算机视觉，所以不做过多讨论。</p><p>深度学习技术在各个计算机领域获得了巨大的成功，其解决语义分割问题可以概括为几种思路：</p><ul><li>块分类（Patch classification）</li><li>全卷积方法（基于FCN）</li><li>编码器-解码器结构（encoder-decoder，本质基于FCN）</li><li>跨层连接的encoder-decoder结构</li></ul><h3>块分类（Patch classification）</h3><p>块分类算得上是一类最古老的方法。</p><p>如其名，把图像分成小块塞给网络进行分类。分成指定大小的小块是因为全连接网络只接受指定大小的输入。这大概是最初的基于深度学习的分割方法了（吧）。</p><h3>全卷积方法（基于FCN）</h3><p>全卷积方法在块分类之后，优势是使用全卷积代替了块分类中的全连接。</p><p>用于代替全连接的全卷积方法除了在其他视觉方法里很出名，也很快用到了分割算法中。2014年，全卷积网络（FCN）横空出世，FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，而且速度比Patch classification方法快很多。（我用简单分类模型实测了一下也是，全连接真的是太烂了，又慢又重，但是作为多层感知机到全卷积网路中间的过度组件，还是功不可没的。）</p><h4>插值法实现的上采样</h4><p>在全卷积方法中，为了使输出和输入大小相同，在卷积导致特征图变小后还需要经过上采样使特征图变为原来大小。</p><p><img src="./src/overview-of-semantic-segmentation/deconv01.gif" alt="deconv01"/></p><p>上图：一种反卷积的示意。其中蓝色较小的特征图是输入，通过在它周围填充，使其变为较大的特征图后，再进行卷积。得到的结果是绿色的特征图。</p><p><img src="./src/overview-of-semantic-segmentation/deconv02.gif" alt="deconv02"/></p><p>上图：另一种反卷积的示意。其中蓝色较小的特征图经过某种填充方法进行填充，变为较大的特征图后再进行卷积。</p><p>反卷积的常见思路是通过一些填充的方法将较小的特征图变大，然后通过卷积获得比原来的小特征图更大的特征图。较为常用的填充方法是插值法。</p><p>插值的方法主要可以分为两类，一类是线性图像插值方法：</p><ul><li>最近邻插值(Nearest neighbor interpolation)</li><li>双线性插值(Bi-Linear interpolation)</li><li>双立方插值(Bi-Cubic interpolation)</li></ul><p>另一类是非线性图像插值方法：</p><ul><li>基于小波变换的插值算法</li><li>基于边缘信息的插值算法。</li></ul><p>以上的这些方法都是一些插值方法，需要我们在决定网络结构的时候进行挑选。这些方法就像是人工特征工程一样，并没有给神经网络学习的余地，神经网络不能自己学习如何更好地进行插值，这个显然是不够理想的。</p><h4>转置卷积实现的上采样</h4><p>在上采样的方法中，比较出名的是转置卷积，因为它允许我们使用可学习的上采样过程。</p><p>典型的转置卷积运算将采用滤波器视图中当前值的点积并作为相应的输出位置产生的单个值，而转置卷积的过程基本想法。对于转置卷积，我们从低分辨率特征图中获取单个值，并将滤波器中的所有权重乘以该值，将加权值输出到更大的特征图。</p><p><img src="./src/overview-of-semantic-segmentation/image-20210427223356560.png" alt="image-20210427223356560"/></p><p>上图：转置卷积的一种示意。</p><blockquote><p>Tips：神经网络中的解卷积层也被称作：转置卷积(Transposed Convolution)、上卷积（upconvolution）、完全卷积（full convolution）、转置卷积（transposed convolution）、微步卷积（fractionally-strided convolution）。</p><p>转置卷积常常在一些文献中也称之为反卷积(Deconvolution)和部分跨越卷积(Fractionally-strided Convolution)，因为称之为反卷积容易让人以为和数字信号处理中反卷积混起来，造成不必要的误解，因此下文都将称为转置卷积，并且建议各位不要采用反卷积这个称呼。</p></blockquote><h3>编码器-解码器结构（encoder-decoder，本质基于FCN）</h3><p>encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。</p><p><img src="./src/overview-of-semantic-segmentation/image-20210428220457279.png" alt="image-20210428220457279"/></p><p>实际上，符合下采样提取特征，再上采样恢复原大小的都可以称为encoder-decoder结构。</p><h4>跨层连接的encoder-decoder结构</h4><p>通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接，其思想我猜是从VGG跨层连接出现的思想）。</p><p><img src="./src/overview-of-semantic-segmentation/image-20210427221642324.png" alt="image-20210427221642324"/></p><p>上图是带有跨层连接的encoder-decoder的代表之一：UNet的结构。</p><h4>高低层特征融合</h4><p>由于池化操作造成的信息损失，上采样（即使采用解卷积操作）只能生成粗略的分割结果图。因此，论文从高分辨率的特征图中引入跳跃连接（shortcut/skip connection）操作改善上采样的精细程度（感觉像是从ResNet开始出现的思想）：</p><p><img src="./src/overview-of-semantic-segmentation/FCN-2.png" alt="FCN-2"/></p><p>实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。可以看到如上三行的对应的结果：</p><p><img src="./src/overview-of-semantic-segmentation/FCN-3.png" alt="FCN-3"/></p><h3>空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）</h3><p>尽管FCN及encoder-decoder结构中移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作。这里使用池化的下采样为例：pooling操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于pooling下采样操作，使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。</p><p>Dilated/Atrous Convolution（空洞卷积），这种结构代替了池化，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息（我觉得这个设计很有意思，原图的大小完全不会改变，也不需要上采样了）。</p><p><img src="./src/overview-of-semantic-segmentation/image-20210427221923919.png" alt="image-20210427221923919"/></p><p>上图：在某篇论文中出现的空洞卷积示意图。</p><p><img src="./src/overview-of-semantic-segmentation/Atrous_conv.png" alt="Atrous_conv"/></p><p>上图：另一张空洞卷积的示意图。</p><h3>条件随机场</h3><p>在使用全卷积网络的分割方法中，有一个很常用的基本框架：</p><p><img src="./src/overview-of-semantic-segmentation/CRF01.jpg" alt="img"/></p><p>其中， FCN 表示各种全卷积网络，CRF 为条件随机场，MRF 为马尔科夫随机场。其大致思路就是前端使用 FCN 进行特征粗提取，后端使用 CRF/MRF 优化前端的输出，最后得到分割图。</p><p><a href="https://arxiv.org/pdf/1210.5644.pdf">条件随机场（Conditional Random Field，CRF）</a> 后处理操作通常用于进一步改善分割的效果。CRFs 是一种基于底层图像的像素强度进行“平滑”分割（‘smooth’ segmentation）的图模型，其工作原理是相似强度的像素更可能标记为同一类别。CRFs 一般能够提升 1-2% 的精度。</p><p><img src="./src/overview-of-semantic-segmentation/CRF.png" alt="CRF"/></p><p>上图为CRF示意图。（b）一元分类结合CRF;（c, d, e）是CRF的变体，其中(e)是广泛使用的一种CRF。</p><hr/><h2>分割的数据集</h2><p>截止到原综述写作时间为止时较为流行的数据集：</p><p><img src="./src/overview-of-semantic-segmentation/image-20210428094548476.png" alt="image-20210428094548476"/></p><p>还没看完，看完就写。</p><hr/><h2>领域知名论文</h2><h3>基于深度学习的分割方法</h3><p><img src="./src/overview-of-semantic-segmentation/image-20210428094705161.png" alt="image-20210428094705161"/></p><ol><li><p>FCN</p><p>主要贡献：使端对端的卷积语义分割网络变得流行起来；通过deconvolutional layers进行上采样；通过skip connection改善了上采样的粗糙度。</p></li><li><p>SegNet</p><p>主要贡献：使用Maxpooling indices来增强位置信息。</p></li><li><p>Dilated Convolutions</p><p>主要贡献：使用空洞卷积用来进行稠密预测（dense prediction）；提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。</p></li><li><p>DeepLab (v1 &amp; v2)</p><p>主要贡献：使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率；提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息；使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。</p></li><li><p>RefineNet</p><p>主要贡献：精心设计了encoder-decoder架构中的decoder部分，使得性能提升；整个网络的设计都遵循residual connections，网络表达能力更强，梯度更容易反向传播。</p></li><li><p>PSPNet</p><p>主要贡献：使用pyramid pooling整合context；使用auxiliary loss。</p></li><li><p>Large Kernel Matters</p><p>主要贡献：提出一种具有非常大的内核卷积的编码器-解码器体系结构。</p></li><li><p>DeepLab v3</p><p>主要贡献：改进的无孔空间金字塔池化（ASPP）；级联使用atrous卷积的模块。</p></li></ol><h3>上述方法的关系</h3><p><img src="./src/overview-of-semantic-segmentation/image-20210428094839526.png" alt="image-20210428094839526"/></p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobileNetV2 - Inverted Residuals and Linear Bottlenecks]]></title>
        <id>MobileNetV2 - Inverted Residuals and Linear Bottlenecks</id>
        <link href="https://ml.akasaki.space/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇讲解一种轻量级主干网络的论文。原论文（MobileNetV2: Inverted Residuals and Linear Bottlenecks）。]]></summary>
        <content type="html"><![CDATA[<p>这是一篇讲解一种轻量级主干网络的论文。<a href="https://arxiv.org/abs/1801.04381">原论文（MobileNetV2: Inverted Residuals and Linear Bottlenecks）</a>。</p><ul><li>本文主要针对轻量特征提取网络中结构上的三个修改提高了网络性能。</li><li>本文总思路：使用低维度的张量得到足够多的特征</li></ul><p>摘要:</p><blockquote><p>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and bench- marks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottle- neck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demon- strate that this improves performance and provide an in- tuition that led to this design. Finally, our approach allows decoupling of the in- put/output domains from the expressiveness of the trans- formation, which provides a convenient framework for further analysis. We measure our performance on ImageNet classification, COCO object detection <!-- -->[2]<!-- -->, VOC image segmentation <!-- -->[3]<!-- -->. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.</p></blockquote><h2>Introduction</h2><p>本文所介绍的网络是针对于计算资源受限的情况下仍然使用深度学习做实际落地应用等方面内容。一般来说，提高准确率很大程度与需要依赖算力成本，追求越高的准确率算力成本开销就越，使得很多移动设备和嵌入式应用无法使用这种类型的算法。</p><p>针对这个问题本文提出了一种新的神经网络结构，专门针对移动设备或者资源首先的环境下仍然能够使用深度学习，但在降低参数量和操作数量的同时，又要保证网络的准确率。</p><p>我们主要的贡献是一种新型的模块，使用linear bottleneck模块的inverted residual模块。Inverted Residual模块输入的是一个经过压缩之后的低维度的特征，这个特征首先是先扩展到高维特征，然后在经过轻量depthwise卷积之后得到的.</p><p>本文的方法是针对于移动端进行涉及，能够减少在推理阶段的内存开销，减少了对内存访问的需求，使得该模块能够在嵌入式硬件上使用。</p><h2>Related Work</h2><p>针对目前现有的网络，许多团队都是手动对结构进行优化、改进取得了很好的结果，如AlexNet,VGGNet,GoogleNet,ResNet等网络。对其进行了超参数的各种优化。针对连接的方式也做了大量的工作，目前很多都是修改卷积块内部的连接结构，像ShuffleNet。像近期的一些研究虽然能够达到很好的效果，但是网络结构很复杂，无法达到预期目标，所以我们的网络基于<a href="https://arxiv.org/abs/1704.04861">MobileNetV1</a>进行改进，在保证轻量、简单的同时又不需要额外特殊的操作就能提高准确率，在移动端上有一个良好的表现。</p><h2>创新点</h2><h3>Depthwise Separable Convolutions</h3><p>本文网络结构中关键的卷积模块是DSC模块(深度可分离卷积，以下简称DSC)，对于大多数高效的神经网络来说使用这种类型的卷积是关键。</p><h4>基本思想</h4><ul><li>常规卷积：</li></ul><p>假设输入层为一个大小为64×64像素、三通道彩色图片。经过一个包含4个Filter的卷积层，最终输出4个Feature Map，且尺寸与输入层相同。</p><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210516170258393.png" alt="image-20210516170258393"/></p><p>此时，卷积层共4个Filter，每个Filter包含了3个Kernel，每个Kernel的大小为3×3。因此卷积层的参数数量可以用如下公式来计算：</p><pre><code class="language-python">N_std = 4 × 3 × 3 × 3 = 108
</code></pre><ul><li><p>DSC</p><p>为了取代传统卷积操作，我们通过将图片分解成两个单独的图层。</p><p>第一层做深度卷积(DW)，通过使用对每一个通道使用轻量级的卷积进行滤波。对每一个通道使用卷积，在二维平面上进行，运算完之后生成3个特征图，此时通道数并没有扩大，没有有效利用不同通道上相同位置的特征信息。所以需要接下来的点卷积(PW)</p></li></ul><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210516165901837.png" alt="image-20210516165901837"/></p><pre><code class="language-python"># 参数量
N_depthwise = 3 × 3 × 3 = 27
</code></pre><p>第二层是一个$1\times 1$的卷积，叫做点卷积，负责通过计算输入通道的线性组合来建立新的特征。</p><ul><li><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210516165910834.png" alt="image-20210516165910834"/></li></ul><pre><code class="language-python"># 参数量
N_pointwise = 1 × 1 × 3 × 4 = 12

# 总和
N_sum = N_depthwise + N_pointwise = 39
</code></pre><ul><li><p>计算消耗</p><p>标准的卷积模块，对于一个$h\times w\times d$的图片来说，为了得到$h\times w\times d&#x27;$，使用一个$k\in R^{k\times k\times d\times d&#x27;}$的卷积核，需要进行$h\times w\times d\times d&#x27;\times k^2$次运算。</p><p>DSC模块，能够达到跟标准卷积同样的效果但是只需要$h\times w\times d\times(k^2+d&#x27;)$</p><p>为什么深度可分离卷积性能好，因为他是先做一次3x3的卷积(对3个通道分别做卷积)，分成三个通道后在做1x1的卷积提取特征，计算量比传统的减少了</p></li></ul><h3>Linear Bottleneck</h3><blockquote><p>这部分主要介绍了为什么使用线性分类器来取代激活函数，通过一系列例子来证明，先是将卷积和激活函数抽象成一个兴趣流形，然后根据兴趣流形在不同输出维度的表现来说明<strong>ReLU在低维空间会丢失大量的信息，但在高维的时候却不会丢失</strong>。这部分主要讲<strong>线性变换取代ReLU激活函数</strong>，代替原本非线性激活变换。所以在后面介绍Inverted Risdual的时候使用LinearBottleneck就可以捕获兴趣流形并且防止非线性破坏太多信息。</p></blockquote><ul><li><p>兴趣流形manifold of interest</p><p>对于一个n个$L_i$层的神经网络来说，每一层输出的张量是$h_i\times w_i\times d_i$的大小。其中包含了$h_i\times w_i$个像素和$d_i$个维度。manifold of interest是指在神经网络中嵌入低维子空间，也就是说，将每个在深度卷积层的通道通过manifold编码为各种信息，这些信息又嵌入低维子空间中。我们人为<strong>一连串的卷积核激活函数就形成了一个manifold of interest(兴趣流形)</strong></p><p>神经网络中的兴趣流形可以嵌入到低维子空间，也就是我们查看单个d通道的像素时，这些值中存在多种编码信息，我们通过变换可以进一步嵌入到下一个低维的子空间中(通过$1\times1$的卷积变换，转换兴趣流形的空间维度)</p></li><li><p>实际例子</p><p>如果当前激活空间内兴趣流形完整度较高，经过ReLU，可能会让激活空间坍塌，不可避免的会丢失信息。</p><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210516162727671.png" alt="image-20210516162727671"/></p><p>如图所示，输入的是一个二维的数据，兴趣流形就是蓝色的螺旋线，本例使用矩阵$T$将数据嵌入到n维空间中，后接ReLU，再使用$T^{-1}$将其投影回2D平面。可以看到设置$n=2,3$时信息丢失严重，中心点坍塌掉了。当$n=15..30$之间，恢复的信息明显多了。</p><p>如果输入对应的输出是非零的，那么输入和输出之间其实对应的就是一个线性映射关系。</p><blockquote><p>因为这个网络的目的是为了把网络轻量化，那么就不能够使用高维度的特征图，维度越高计算量越大，但是如果维度太低导致特征不明显就会导致准确率下降，所以本文的<strong>目的是尽可能使用一些维度比较低的卷积。</strong></p><p>但是如果<strong>对比较低维度的特征使用ReLU激活函数会丢失掉很多信息</strong>。因为ReLU是一个线性映射，所以我们用线性分类器就会减少维度信息的丢失，同时又可以设计出维度比较低的层。</p></blockquote></li><li><p>总结</p><p>如果manifold of interest 始终保持非零在经过ReLU变换之后，那么就是对于线性的变换。</p><p>ReLU能够保留输入的manifold的信息，但是只有在输入空间中低维度空间中的manifold才能保留完整信息。假设manifold of interest是低维度的，我们可以通过线性瓶颈层插入卷积块来捕获，使用线性层是为了防止非线性层摧毁太多信息，从传统的残差块中去除掉非线性的部分。</p></li></ul><h3>Inverted Residuals</h3><blockquote><p>设计思路: 先通过Expansion layer来<strong>扩展维度</strong>，之后在<strong>用深度可分离卷积来提取特征</strong>，之后使用Projection layer来压缩数据，<strong>将高维度的数据映射回低维数据</strong>，让网络从新变小。因为Expansion layer 和 Projection layer都是有可以学习的参数，所以整个网络结构可以学习到如何更好的扩展数据和从新压缩数据。</p><p>先将张量维度扩展到高维，对高维tensor提取特征，然后再映射回低维度空间。</p></blockquote><p>网络结构如下:</p><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210516160202078.png" alt="image-20210516160202078"/></p><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210517095545070.png" alt="image-20210517095545070"/></p><p>设计思路主要还是深度可分离卷积模块的堆叠，在v1的基础上，除了使用DSC模块之外，添加了<strong>Projection layer</strong>和<strong>Expansion layer</strong>。在提取特征的时候使用高维tensor(高维信息多)，在处理特征的时候使用低维tensor.</p><p><strong>Projection layer</strong>也是使用$1\times 1$的网络结构，他的目的是希望把<strong>高维特征映射到低维空间</strong>去。使用 $1\times 1$的网络结构将高维空间映射到低维空间的设计有的时候我们也称之为<strong>Bottleneck layer。</strong></p><p><strong>Expansion layer</strong>的功能正相反，使用$1\times 1$的网络结构，目的是将<strong>低维空间映射到高维空间</strong>。这里<strong>Expansion factor</strong>有一个超参数(Expansion Factor)是维度扩展几倍。可以根据实际情况来做调整的，默认值是6，也就是扩展6倍。与传统的残差连接不同，对于一个块大小为$h\times w$，扩展因子为$t$和卷积核大小为$k$，输入通道为$d&#x27;$，输出通道为$d&#x27;&#x27;$总共需要进行$h\times w\times d&#x27;\times t(d&#x27;+k^2+d&#x27;&#x27;)$次乘法。通过一个额外的$1\times 1$卷积使得网络能够利用更小的输入和输出。从而实现降低维度大小，也就是降低乘法计算量的方法.</p><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210516161042196.png" alt="image-20210516161042196"/></p><p>此图更详细的展示了整个模块的结构。我们输入是24维，最后输出也是24维。但这个过程中，我们扩展了6倍，然后应用深度可分离卷积进行处理。</p><p><strong>bottleneck residual block(ResNet)</strong>是中间窄两头胖<strong>(中间是维度小的，两边是维度大的)</strong>，在MobileNetV2中正好反了过来<strong>(中间是维度大的，两边是维度小的)</strong>，所以，在MobileNetV2的论文中我们称这样的网络结构为<strong>Inverted residuals</strong>。需要注意的是residual connection是在输入和输出的部分进行连接。另外，我们之前已经讲的<strong>Linear Bottleneck</strong>在这里使用，因为从高维向低维转换，使用ReLU激活函数可能会造成信息丢失或破坏（不使用非线性激活数数）。所以在projection convolution这一部分，<strong>我们不再使用ReLU激活函数而是使用线性激活函数。</strong>（这也是为什么前面要提到linear bottleneck）</p><p>与传统的Residuals不同，传统的ResNet网络结构如下：</p><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210516161633202.png" alt="image-20210516161633202"/></p><p>可以注意到，对比<strong>Inverted</strong>版本的，在残差求和之前，他们都是维度比较小的，经过一个$1\times 1$卷积之后变回原来的通道数（高维-&gt;低维）。而Inverted版本的是输入较低维度的，在残差求和之前通过Expansion layer使用$1\times 1$的网络架构将低维空间映射到高维空间。（低维-&gt;高维）</p><h2>Supplements</h2><p>我们知道，如果tensor维度越低，卷积层的乘法计算量就越小。那么如果整个网络都是低维的tensor，那么整体计算速度就会很快。</p><p>然而，如果只是使用低维的tensor效果并不会好。如果卷积层的过滤器都是使用低维的tensor来提取特征的话，那么就没有办法提取到整体的足够多的信息。所以，如果提取特征数据的话，我们可能更希望有高维的tensor来做这个事情。V2就设计这样一个结构来达到平衡。</p><p><img src="./src/MobileNetV2-Inverted-Residuals-and-Linear-bottleneck/image-20210517095431302.png" alt="image-20210517095431302"/></p><p>先通过Expansion layer来扩展维度，之后在用深度可分离卷积来提取特征，之后使用Projection layer来压缩数据，让网络从新变小。因为Expansion layer 和 Projection layer都是有可以学习的参数，所以整个网络结构可以学习到如何更好的扩展数据和从新压缩数据。</p><p>使用 $1\times 1$的网络结构将高维空间映射到低维空间的设计有的时候我们也称之为<strong>Bottleneck layer。</strong></p>]]></content>
        <author>
            <name>PommesPeter</name>
            <uri>https://blog.pommespeter.com/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast-SCNN - Fast Semantic Segmentation Network]]></title>
        <id>Fast-SCNN - Fast Semantic Segmentation Network</id>
        <link href="https://ml.akasaki.space/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇讲解一种快速语义分割的论文。论文名 Fast Semantic Segmentation Network]]></summary>
        <content type="html"><![CDATA[<p>这是一篇讲解一种快速语义分割的论文。论文名:<a href="https://arxiv.org/abs/1902.04502">Fast-SCNN: Fast Semantic Segmentation Network</a></p><ul><li>主要是采用双流模型的架构设计这个网络</li><li>本文总思路：减少冗余的卷积过程，从而提高速度</li></ul><p>摘要：</p><blockquote><p>The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024 × 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our ‘learning to downsample’ module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.</p></blockquote><h2>摘要</h2><p>主要贡献：</p><ol><li>提出了一个有竞争性(68.0%miou)，并且能在高分辨率(1024x2048)的图片实现实时(123.5FPS)语义分割的算法Fast-SCNN.</li><li>采用了离线型DCNNs中流行的<strong>跳跃连接(skip connection)</strong>，并提出了一种浅层学习的下采样模块<strong>learning to Down-sample</strong>,以此更加快速高效地进行多分支低级特征提取。</li><li>将Fast-SCNN设计为轻量型(low capacity)，并证实了无论是使用ImageNet数据集的训练模型多训练几代，还是在添加的粗糙数据中多训练几代的结果是等效的。</li></ol><h2>DCNNs的效率</h2><p>高效DCNNs（Diffusion-Convolutional Neural Networks ）的常见技术为：</p><h3>深度可分离卷积(Depthwise Separable Convolutions):</h3><p>MoblieNet将标准的Conv分解为<strong>深度卷积（depthwise convolutions）</strong>和$1 \times 1$<strong>点式卷积(pointwise convolution)</strong>通过这样的方式，减少了浮点运算和卷积参数，减少了模型的计算成本和内存需求。</p><h3>DCNNs的高效重新设计</h3><p>MobileNet-V2使用<strong>倒置的瓶颈残差块(inverted bottleneck residual blocks)</strong>以分类任务构建有效的DCNN。</p><p>ContextNeto使用能够倒置瓶颈残差块设计了一个1两分支网络，以进行有效的实时语义分割。</p><h3>网络压缩</h3><p>使用剪枝减小预训练网络的大小，从而实现更块的运行时间，更小的参数集和更小的内存占用空间。</p><p>Fast-SCNN严重依赖与深度可分离卷积和残差瓶颈块，还引入了一个两分支模型，该模型将学习内容整合到下采样的模块中，从而允许在多个分辨率级别上进行共享特征提取。网络量化和网络压缩可以正交应用，留待后面的工作。</p><h2>Fast-SCNN</h2><p>网络结构图：</p><p><img src="./src/Fast-SCNN-Fast-Semantic-Segmentation-Network/20210512115432image-20210512115431363.png" alt="image-20210512115431363"/></p><p>在定义网络的BN层时使用类各种类型的BN层,但是默认都是使用普通的BN层</p><p>常规的BN，SyncBN（跨卡BN），FrozenBN（测试阶段使用的BN），GN（Group Normalization）</p><p><img src="./src/Fast-SCNN-Fast-Semantic-Segmentation-Network/20210516102348image-20210516102346560.png" alt="image-20210516102346560"/></p><h3>下采样学习模块(learning to down-sample)</h3><p>在该模块中使用了三层卷积，第一层是普通的卷积(Conv2D)，其余两层是可分离卷积(DSConv)，因为图像刚刚输入只有三个通道，使用DSConv的优势并不明显所以，采用普通卷积层。</p><p>在下采样学习模块中，使用的步长均为2，然后进行BN和ReLU。卷积核和深度可分离卷积核均为$3\times3$.</p><pre><code class="language-python"># 在主网络中的定义
self.learning_to_downsample = LearningToDownsample(32, 48, 64, norm_layer=self.norm_layer) # norm_layerh是普通的BN

# 下采样学习模块的定义
class LearningToDownsample(nn.Module):
    &quot;&quot;&quot;Learning to downsample module&quot;&quot;&quot;

    def __init__(self, dw_channels1=32, dw_channels2=48, out_channels=64, norm_layer=nn.BatchNorm2d):
        super(LearningToDownsample, self).__init__()
        self.conv = _ConvBNReLU(3, dw_channels1, 3, 2) # 这个就是单纯的CONV+BN+ReLU
        # 深度可分离卷积：一个深度卷积，一个点卷积的组合
        self.dsconv1 = SeparableConv2d(dw_channels1, dw_channels2, stride=2, relu_first=False, norm_layer=norm_layer)
        self.dsconv2 = SeparableConv2d(dw_channels2, out_channels, stride=2, relu_first=False, norm_layer=norm_layer)

    def forward(self, x):
        x = self.conv(x) #  普通卷积
        x = self.dsconv1(x) # 可分离卷积
        x = self.dsconv2(x)
        return x
</code></pre><pre><code class="language-python"># 深度可分离卷积
class SeparableConv2d(nn.Module):
    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, relu_first=True,
                 bias=False, norm_layer=nn.BatchNorm2d):
        super().__init__()
        # 深度卷积，卷积核为3步长1，padding1，空洞1的卷积层
        depthwise = nn.Conv2d(inplanes, inplanes, kernel_size,
                              stride=stride, padding=dilation,
                              dilation=dilation, groups=inplanes, bias=bias)
        # 对应的BN
        bn_depth = norm_layer(inplanes)
        # 点卷积，就是普通的1x1卷积
        pointwise = nn.Conv2d(inplanes, planes, 1, bias=bias)
        # 对应的BN
        bn_point = norm_layer(planes)
        # 是否使用激活函数
        if relu_first:
            self.block = nn.Sequential(OrderedDict([(&#x27;relu&#x27;, nn.ReLU()),
                                                    (&#x27;depthwise&#x27;, depthwise),
                                                    (&#x27;bn_depth&#x27;, bn_depth),
                                                    (&#x27;pointwise&#x27;, pointwise),
                                                    (&#x27;bn_point&#x27;, bn_point)
                                                    ]))
        else:
            self.block = nn.Sequential(OrderedDict([(&#x27;depthwise&#x27;, depthwise),
                                                    (&#x27;bn_depth&#x27;, bn_depth),
                                                    (&#x27;relu1&#x27;, nn.ReLU(inplace=True)),
                                                    (&#x27;pointwise&#x27;, pointwise),
                                                    (&#x27;bn_point&#x27;, bn_point),
                                                    (&#x27;relu2&#x27;, nn.ReLU(inplace=True))
                                                    ]))

    def forward(self, x):
        return self.block(x)
</code></pre><h3>全局特征提取器(Global Feature Extrator)</h3><p>全局特征提取器模块的目的在于捕获分割图像的全局上下文信息。该模块直接将下采样学习模块的结果(分辨率为原图的$\frac 18$)作为而输入。该模块引入了MobileNet-V2中提出的有效的瓶<strong>颈残差网络(efficient bottleneck residual blocks)</strong>。当输入的图像和输出的图像尺寸相同时，使用残差连接链接瓶颈残差块。</p><p>在瓶颈残差块中使用了有效的深度可分离卷积，从而减少了参数量和浮点数运算。最后还添加了一个金字塔池化模块(pyramid pooling module 简称PPM)，用于汇总基于不同区域的上下文信息。</p><p>在各层的详细参数如下表：</p><p><img src="./src/Fast-SCNN-Fast-Semantic-Segmentation-Network/20210512135254image-20210512135253006.png" alt="image-20210512135253006"/></p><p>每一条横线分别表示，下采样学习模块，全局特征提取器，特征融合，分类四个总体模块</p><p>其中t,c,n,s分别表示瓶颈块的拓展因子，输入通道数，使用该层的次数，步长</p><p>瓶颈块的参数表：</p><p><img src="./src/Fast-SCNN-Fast-Semantic-Segmentation-Network/20210512135639image-20210512135638073.png" alt="image-20210512135638073"/></p><p>瓶颈残差块将输入为c的图像转化为具有拓展因子t的c`</p><p>最后的点卷积不适用非线性函数f</p><pre><code class="language-python"># 主网络声明
self.global_feature_extractor = GlobalFeatureExtractor(64, [64, 96, 128], 128, 6, [3, 3, 3],norm_layer=self.norm_layer)

# 全局特征提取器对应的模块类
class GlobalFeatureExtractor(nn.Module):
    &quot;&quot;&quot;Global feature extractor module&quot;&quot;&quot;
    # 输入的通道数，每一层的通道数，输出的通道数，拓展因子t，块在每一层的数量
    def __init__(self, in_channels=64, block_channels=(64, 96, 128), out_channels=128,
                 t=6, num_blocks=(3, 3, 3), norm_layer=nn.BatchNorm2d):
        super(GlobalFeatureExtractor, self).__init__()
        # 创建瓶颈残差块，这里使用的InvertedResidual叫做反向残差。
        # 只有步长为1并且输入通道和输出通道相同的情况下这各个反向残差才会使用残差连接
        self.bottleneck1 = self._make_layer(InvertedResidual, in_channels, block_channels[0], num_blocks[0],
                                            t, 2, norm_layer=norm_layer)
        self.bottleneck2 = self._make_layer(InvertedResidual, block_channels[0], block_channels[1],
                                            num_blocks[1], t, 2, norm_layer=norm_layer)
        self.bottleneck3 = self._make_layer(InvertedResidual, block_channels[1], block_channels[2],
                                            num_blocks[2], t, 1, norm_layer=norm_layer)
        # 做一个金字塔池化
        self.ppm = PyramidPooling(block_channels[2], norm_layer=norm_layer)
        # 最后使用1x1卷积输出成对应的通道，进行输出
        self.out = _ConvBNReLU(block_channels[2] * 2, out_channels, 1, norm_layer=norm_layer)

    def _make_layer(self, block, inplanes, planes, blocks, t=6, stride=1, norm_layer=nn.BatchNorm2d):
        # 使用的模块，输入的通道数，输出的通道数，块的数量，拓展因子t，步长
        # 初始化一个容器
        layers = []
        # 将块中的信息加入
        layers.append(block(inplanes, planes, stride, t, norm_layer=norm_layer))
        # 重复这个块对应次
        for i in range(1, blocks):
            layers.append(block(planes, planes, 1, t, norm_layer=norm_layer))
        #  将对应的内容放入Sequential容器中
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.bottleneck1(x)
        x = self.bottleneck2(x)
        x = self.bottleneck3(x)
        x = self.ppm(x)
        x = self.out(x)
        return x
</code></pre><pre><code class="language-python"># 反向卷积块
class InvertedResidual(nn.Module):
    def __init__(self, in_channels, out_channels, stride, expand_ratio, dilation=1, norm_layer=nn.BatchNorm2d):
        # 参数：输入通道，输出通道，步长，拓展因子，空洞卷积，
        super(InvertedResidual, self).__init__()
        assert stride in [1, 2]
        # 是否使用残差连接
        self.use_res_connect = stride == 1 and in_channels == out_channels

        layers = list()
        # 中间的通道数，使用拓展因子*输入的通道数
        inter_channels = int(round(in_channels * expand_ratio))
        if expand_ratio != 1:
            # pw
            # 先做一个标准卷积嘛，使用中间通道数作为输出,1x1卷积
            layers.append(_ConvBNReLU(in_channels, inter_channels, 1, relu6=True, norm_layer=norm_layer))
        layers.extend([
            # dw 这里使用了分组卷积，但是实际上和普通的卷积没有什么区别，如果groups整好是输入通道数的一个因素，则输入的通道会被分成对应的组进行卷积
            _ConvBNReLU(inter_channels, inter_channels, 3, stride, dilation, dilation,
                        groups=inter_channels, relu6=True, norm_layer=norm_layer),
            # pw-linear
            # 使用1x1卷积将中间通道数转化成最终的通道数
            nn.Conv2d(inter_channels, out_channels, 1, bias=False),
            norm_layer(out_channels)])
        self.conv = nn.Sequential(*layers)

    def forward(self, x):
        # 残差连接
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)
</code></pre><pre><code class="language-python"># PPM(金字塔池化模块)
class PyramidPooling(nn.Module):
    def __init__(self, in_channels, sizes=(1, 2, 3, 6), norm_layer=nn.BatchNorm2d, **kwargs):
        super(PyramidPooling, self).__init__()
        # 定义输出为输入的四分之一
        out_channels = int(in_channels / 4)
        # 创建平均池化和卷积层模块列表
        self.avgpools = nn.ModuleList()
        self.convs = nn.ModuleList()
        # 遍历平均池化的尺寸
        for size in sizes:
            # 使用自适应平均池化，这里的参数，表示经过自适应平均池化的特征图输入为c X h X w，出来的结果为c X size X size
            self.avgpools.append(nn.AdaptiveAvgPool2d(size))
            # 使用普通卷积层进行卷积1x1的卷积核
            self.convs.append(_ConvBNReLU(in_channels, out_channels, 1, norm_layer=norm_layer, **kwargs))

    def forward(self, x):
        size = x.size()[2:]
        feats = [x]
        for (avgpool, conv) in zip(self.avgpools, self.convs):
            # 没记错的话interpolate应该是上采样到size的大小
            feats.append(F.interpolate(conv(avgpool(x)), size, mode=&#x27;bilinear&#x27;, align_corners=True))
        # 记录完平均池化的结果后，就进行拼接
        return torch.cat(feats, dim=1)
</code></pre><h3>特征融合模块(Feature Fusion Module)</h3><p><img src="./src/Fast-SCNN-Fast-Semantic-Segmentation-Network/20210512140504image-20210512140501709.png" alt="image-20210512140501709"/></p><p>先前下采样学习模块计算的特征图（表的左边）只经过一个$1 \times 1$的卷积即可，在x次下采样后的结果(经过全局特征提取模块的特征图，表的右边)，上采样X次，使用可分离卷积和一个非线性函数，再使用一个$1 \times1$的卷积，最后将两个特征图加起来，再使用非线性激活函数f</p><pre><code class="language-python"># 在主类中的声明：
self.feature_fusion = FeatureFusionModule(64, 128, 128, norm_layer=self.norm_layer)

# 特征融合模块
class FeatureFusionModule(nn.Module):
    &quot;&quot;&quot;Feature fusion module&quot;&quot;&quot;
    # 输入的参数为高输入的通道数，低输入的通道数，输出的通道数
    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, norm_layer=nn.BatchNorm2d):
        super(FeatureFusionModule, self).__init__()
        # 设置规模
        self.scale_factor = scale_factor
        # 使用普通卷积将低通道数转化成输出的通道数
        self.dwconv = _ConvBNReLU(lower_in_channels, out_channels, 1, norm_layer=norm_layer)
        # 再对低维卷积的将诶过再做一个1x1卷积，但是不激活
        self.conv_lower_res = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 1),
            norm_layer(out_channels)
        )
        # 对高维度的卷积，只使用1x1卷积，不使用激活函数
        self.conv_higher_res = nn.Sequential(
            nn.Conv2d(highter_in_channels, out_channels, 1),
            norm_layer(out_channels)
        )
        self.relu = nn.ReLU(True)

    def forward(self, higher_res_feature, lower_res_feature):
        # 先将低维特征图上采样到现在的4倍
        lower_res_feature = F.interpolate(lower_res_feature, scale_factor=4, mode=&#x27;bilinear&#x27;, align_corners=True)
        # 将低纬度的通道数转化成输出的通道数
        lower_res_feature = self.dwconv(lower_res_feature)
        # 再做一次1x1卷积，但是不激活
        lower_res_feature = self.conv_lower_res(lower_res_feature)
        # 对高纬度进行1x1卷积，但是不激活
        higher_res_feature = self.conv_higher_res(higher_res_feature)
        # 将低纬度和高纬度加起来
        out = higher_res_feature + lower_res_feature
        # 最后激活他就行
        return self.relu(out)
</code></pre><h3>分类模块(classifier)</h3><p>在分类模块中采用两个深度可分离卷积(DSConv)和一个普通卷积(Conv2D，纠正一下，之前说过的点卷积是Conv2D)。</p><p>为了适应梯度下降，所以在训练中使用了Softmax激活函数，在推理过程中,由于argmax和sorftmax都是单调递增的函数，所以使用argmax代替softmax减小计算开销。</p><p>如果需要Fast-SCNN的概率模型，才在推理时使用softmax。</p><pre><code class="language-python"># 在主类中的声明：
self.classifier = Classifer(128, self.nclass, norm_layer=self.norm_layer)

# 分类模块
class Classifer(nn.Module):
    &quot;&quot;&quot;Classifer&quot;&quot;&quot;

    def __init__(self, dw_channels, num_classes, stride=1, norm_layer=nn.BatchNorm2d):
        # 参数：输入的通道数，分类数，步长，BN
        super(Classifer, self).__init__()
        # 使用2个深度分离卷积
        self.dsconv1 = SeparableConv2d(dw_channels, dw_channels, stride=stride, relu_first=False,
                                       norm_layer=norm_layer)
        self.dsconv2 = SeparableConv2d(dw_channels, dw_channels, stride=stride, relu_first=False,
                                       norm_layer=norm_layer)
        # 设置随机失活(dropout2d)，然后进行卷积，不适用BN不使用，激活，使用1x1卷积
        self.conv = nn.Sequential(
            nn.Dropout2d(0.1),
            nn.Conv2d(dw_channels, num_classes, 1)
        )

    def forward(self, x):
        x = self.dsconv1(x)
        x = self.dsconv2(x)
        x = self.conv(x)
        return x

</code></pre>]]></content>
        <author>
            <name>Zerorains</name>
            <uri>https://github.com/zeroRains</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications]]></title>
        <id>MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</id>
        <link href="https://ml.akasaki.space/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[这是一篇讲解一种轻量级主干网络的论文。原论文（MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications）。]]></summary>
        <content type="html"><![CDATA[<p>这是一篇讲解一种轻量级主干网络的论文。<a href="https://arxiv.org/abs/1704.04861">原论文（MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications）</a>。</p><ul><li>本文提出了一种应用于移动或者嵌入式设备的高效神经网络</li><li>本文提出了一种操作数较小的卷积模块深度可分离卷积(Depthwise Separable Convolution，以下称DSC)</li></ul><p>摘要:</p><blockquote><p>We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.</p></blockquote><h1>Introduction</h1><p>卷积神经网络已经普遍应用于计算机视觉领域，但是随着神经网络网络的发展，各种类型的识别精度不断在提高，但与此同时导致速度方面却很慢，提高准确率不一定会让速度或大小更小。所以我们需要通过一种有效的网络结构和两种超参数：通道长度调整系数(width multiplier)和分辨率调整系数(resolution multiplier)</p><h1>Realted Work</h1><p>现在针对轻量网络方面的研究也再不断增加，但很多还是没有注意速度，只是关注模型大小，不考虑速度。而本文算法主要是用了深度可分离卷积(depthwise separable convolution)进行卷积操作.最近DSC用于Inception网络来减少计算量。在Factorized Network中介绍了一种类似的分离式卷积。另一个小型网络是SqueezeNet，它使用瓶颈方法设计一个非常小的网络。目前还有的办法是通过hash，剪枝，蒸馏，矢量量化，哈夫曼编码等方式进行优化。</p><h3>MobileNet Architecture</h3><blockquote><p>depthwise卷积：对每个输入通道单独使用一个卷积核处理。</p><p>pointwise卷积：$1\times1$卷积，用于将depthwise卷积的输出组合起来。</p></blockquote><ul><li><p>Depthwise Separable Convolution</p><p>深度可分离卷积是一个把标准的卷积分解成深度卷积(depthwise convolution)和一个$1\times1$的卷积，被称为点卷积(pointwise convolution).用单个卷积对输入图片每一个通道都进行一次深度卷积DW，然后再对深度卷积的输出结果使用$1\times1$的卷积。</p><p>对于标准的卷积来说，是一步就将两个filter和输入组合成一个新的输出，而DSC则是分成两层，一层是做滤波，另一层是做组合拼接。</p><ul><li><p>计算分析</p><blockquote><p>对于DW</p><ul><li><p>输入: $D_F\times D_F\times M$，输出:$D_F\times D_F\times M$，卷积核:$D_K\times D_K$</p></li><li><p>计算量: $D_K\times D_K\times M\times D_F\times D_F$</p></li><li><p>理解: 将输入的特征图($$D_F\times D_F\times M$$)看做是M个$D_F\times D_F\times1$的特征图，对这M个特征图分别做卷积，因为输入通道数为1，输出通道数也为1。实质上，这就是对卷积的通道数进行分组，然后对每组的特征图分别进行卷积，是组卷积（group convolution）的一种扩展，每组只有一个特征图。</p></li></ul><p>对于PW:</p><ul><li>输入: $D_F\times D_F\times M$，输出:$D_F\times D_F\times N$，卷积核:$1\times1\times M\times N$</li><li>计算量: $1\times1\times M\times N\times D_F\times D_F$</li><li>理解: 就是$1\times1$卷积，是普通的卷积操作。</li></ul></blockquote><p>标准卷积处理一张$D_F\times D_F\times M$的特征图$\bold F$，得到一张$D_G\times G_F\times N$的特征图$\bold G$，其中$D_F$是空间的长和宽，$M$是输入通道数，$D_G$是指输出的特征图的长宽，$N$是输出的特征图的通道数。标准卷积在做运算的时候使用一个$D_K\times D_K$的卷积核进行运算，单次计算的计算消耗为$D_K\times D_K\times M\times N$，其中$D_K$是指卷积核的大小，一共需要总共得到一张输出的特征图总计算量消耗是$D_K\times D_K\times M\times N\times D_F\times D_F$通过该式子可以得到计算消耗取绝于特征图大小、卷积核大小、输出通道数等因素。标准卷积通过卷积核和特征的组合能产生新的特征(提取特征的本质)。</p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518103843290.png" alt="image-20210518103843290"/></p><p>而对于DSC来说，因为卷积的过程可以被分成两步，一步是滤波，另一步是组合，将这两步分离的卷积就是本文提出的DSC。下面对其计算进行分析。DSC主要分成两层，深度卷积层(Depthwise Convolution,以下简称DW)和点卷积层(Pointwise Convolution,以下简称PW)。用单个卷积对输入图片每一个通道都进行一次深度卷积DW，然后再对深度卷积的输出结果使用$1\times1$的卷积，对DW之后的层进行线性组合。经过计算，进行DW时的计算量为$D_K\times D_K\times M\times D_F\times D_F$，因为对第 $m$ 个通道进行DW之后生产特征图 $\bold F$ 中对应第 $m$ 个通道的特征图$\bold G$。但这样目前只是把通道进行滤波，但还没有进行组合产生新的特征，对DW产生的特征图使用PW，一个$1\times1$的卷积产生新的特征。</p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518105644144.png" alt="image-20210518105644144"/></p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518105705131.png" alt="image-20210518105705131"/></p><p>PW的计算消耗为$M\times N\times D_F\times D_F$.所以对于DSC来说总的计算量为$(N+D_K\times D_K)\times M\times D_F\times D_F$.通过两式相除可以得到这两种计算量上的差距
$$
\frac{(N+D_K\times D_K)\times M\times D_F\times D_F}{D_K\times D_K\times M\times N\times D_F\times D_F}<!-- -->\<!-- --> = \frac{1}{N} + \frac{1}{D_K^2}
$$</p></li><li><p>总结</p><p>所以相对来说，DSC减少了很多计算量。</p></li></ul></li><li><p>Network Structure</p><p>对于MobileNet，就是使用了DSC结构来堆叠神经网络模块，最后得到的网络结构图如下：</p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518105953216.png" alt="image-20210518105953216"/></p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518110406332.png" alt="image-20210518110406332"/></p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518112758044.png" alt="image-20210518112758044"/></p></li><li><p>Hyper-Parameters</p><ul><li><p>Width Multiplier</p><p>这是一个超参数，为了使得网络更加小和轻量，通过一个通道长度系数$\alpha$来调整DSC的通道长度，对输出的通道数乘上$\alpha$来调整DSC输出的通道长度，计算消耗的表达式为$D_K\times D_K\times\alpha M\times D_F\times D_F+\alpha M\times\alpha N\times D_F\times D_F,(\alpha\in <!-- -->[0,1]<!-- -->)$.一般来说，$\alpha$会调整成$1,0.75,0.5,0.25$，当$\alpha=1$的时候是原始的mobilenet，$\alpha&lt;1$时，是压缩过的mobilnet。可以定义一个更小的模型。</p></li><li><p>Resolution Multiplier</p><p>另一个超参数，叫做分辨率大小系数$\rho,(\rho\in<!-- -->[0,1]<!-- -->)$，即对输入的图片和每一层的特征图大小乘一个系数，这个系数用来调整输入图像和特征图的大小，从而进一步减少参数量。图像输入一般的取值为$224,192,160,128$，当$\rho=1$的时候是原始的mobilenet，$\rho&lt;1$时，是压缩过的mobilnet。可以定义一个更小的模型，可以减少$\rho^2$倍的参数。</p></li><li><p>总结</p><p>综合上面两个超参数，最终的参数量为$D_K\times D_K\times\alpha M\times\rho D_F\times\rho D_F+\alpha M\times\alpha N\times\rho D_F\times\rho D_F,(\alpha\in<!-- -->[0,1]<!-- -->,\rho\in<!-- -->[0,1]<!-- -->)$</p></li></ul></li></ul><h3>Experiments</h3><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518111850667.png" alt="image-20210518111850667"/></p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518111920979.png" alt="image-20210518111920979"/></p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518111929583.png" alt="image-20210518111929583"/></p><p><img src="./src/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/image-20210518111942245.png" alt="image-20210518111942245"/></p>]]></content>
        <author>
            <name>PommesPeter</name>
            <uri>https://blog.pommespeter.com/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gated Channel Transformation for Visual Recognition]]></title>
        <id>Gated Channel Transformation for Visual Recognition</id>
        <link href="https://ml.akasaki.space/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：Gated Channel Transformation for Visual Recognition]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/abs/1909.11519">Gated Channel Transformation for Visual Recognition</a></p><p>作者：Zongxin Yang, Linchao Zhu, Y u Wu, and Yi Yang</p><p>Code：<a href="https://github.com/z-x-yang/GCT">https://github.com/z-x-yang/GCT</a></p></blockquote><h2>摘要</h2><ul><li>GCT模块是一个普遍适用的门控转换单元，可与网络权重一起优化。</li><li>不同于SEnet通过全连接的隐式学习，其使用可解释的变量显式地建模通道间的关系，决定是竞争或是合作。</li></ul><p><strong>关键词：可解释性、显式关系、门控</strong></p><h2>介绍</h2><ul><li>单个卷积层只对Feature Map中每个空间位置的临近局部上下文进行操作，这可能会导致局部歧义。通常有两种方法解决这种问题：一是增加网络的深度，如VGG，Resnet，二是增加网络的宽度来获得更多的全局信息，如GEnet大量使用领域嵌入，SEnet通过全局嵌入信息来建模通道关系。</li><li>然而SEnet中使用fc层会出现两个问题：<ol><li>由于使用了fc层，出于节省参数的考虑，无法在所有层上使用</li><li>fc层的参数较为复杂，难以分析不同通道间的关联性，这实际上是一种<strong>隐式</strong>学习</li><li>放在某些层之后会出现问题</li></ol></li></ul><h2>相关工作</h2><h3>门控机制</h3><p>门控机制已经成功地应用于一些循环神经网络结构中。LSTM引入了输入门、输出门和遗忘门，用于调节模块的进出信息流。基于门控机制，一些注意力方法侧重于将计算资源集中于特征信息最丰富的部分。</p><h3>归一化层</h3><p>近年来，归一化层被广泛应用于深度网络中。局部响应归一化（LRN）为每个像素计算通道间一个小邻域内的统计信息；批量归一化（BN）利用批维度上的全局空间信息；层归一化（LN）沿着通道维度而不是批处理维度计算；组归一化（GN）以不同的方式将通道划分为组，并在每个组内计算均值和方差来进行归一化。</p><h2>GCT</h2><p>设计思路：</p><ol><li>通过p-norm嵌入全局上下文信息</li><li>通过嵌入信息与可训练参数来进行通道归一化</li><li>通过门控权重与偏置来实现通道门控注意力机制</li></ol><h3>整体结构</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210513113146461.png" alt="image-20210513113146461"/></p><p>GCT模块主要包括三个部分——<strong>全局上下文嵌入</strong>、<strong>通道归一化</strong>、和<strong>门控自适应</strong>。其中，归一化操作时<strong>无参</strong>的。</p><p>同时，为了使GCT<strong>可学习</strong>，引入了三个权重——$\alpha、\gamma、\beta$，$\alpha$负责自适应嵌入输出。门控权重$\gamma$和偏置$\beta$负责控制门的激活。</p><p>另外，<strong>GCT的参数复杂度为$O(C)$,而SEnet 的复杂度为$O(C^2)$。</strong></p><p>则GCT模块的激活特征为：
$$
\hat x=F(x|\alpha,\gamma,\beta),\alpha,\gamma,\beta \in \mathbb{R}^c
$$</p><h3>全局上下文嵌入</h3><p>较大的感受野可以避免局部语义歧义，因此设计了一个全局上下文嵌入模块来<strong>聚合每个通道中的全局上下文信息。</strong></p><p><strong>GAP（全局平均池化）在某些情况下会失效</strong>，如将SE模块部署在LN层之后，因为LN固定了每个通道的平均数，对于任意输入，GAP的输出都是恒定的。</p><p>这里<strong>选用了p-norm来进行全局上下文嵌入</strong>，2-norm的效果最好，1-norm的效果与其十分接近，但是注意，当p=1时，对于非负输入（如部署在ReLU之后），将等价于GAP</p><p>其中参数$\alpha$定义为$\alpha=<!-- -->[\alpha_1...\alpha_C]<!-- -->$，当$\alpha_n$接近0时，该通道将不参与通道归一化</p><p>该模块定义为：
$$
s<em>c=\alpha||x_c||_p=\alpha<!-- -->{<!-- -->[\sum</em>{i=1}^H\sum_{i=1}^W(x_c^{i,j})^p]+\varepsilon<!-- -->}<!-- -->^{\frac{1}{p}}
$$
其中$\varepsilon$为一个极小的常数避免了零点处求导问题。</p><h3>通道归一化</h3><p>归一化方法可以在神经元(或通道)之间建立<strong>竞争关系</strong>，使得其中通道响应较大的值变得相对更大，并抑制其他反馈较小的通道（该说法最早可能在<a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">LRN论文</a>中提出，但是该论文并没有给出任何解释，或许当<strong>$\frac{\sqrt{C}}{||s_c||_2}&gt;1$</strong>大于1时会起到建立竞争关系的作用），这里使用$l_2$正则化来进行通道归一化。</p><p>类似于LRN，其定义如下：
$$
\hat{s<em>c}=\frac{\sqrt{C}s_c}{||s||_2}=\frac{\sqrt{C}s_c}{[(\sum</em>{i=1}^{C}s_c^2)+\varepsilon]^{\frac{1}{2}}}
$$</p><h3>门控自适应</h3><p>定义如下：
$$
\hat{x_c}=x_c<!-- -->[1+\tanh(\gamma_c\hat{s_c}+\beta_c)]<!-- -->
$$
当一个通道的门控权重被积极激活时，GCT促进该通道与其他通道竞争。当门控权重被消极激活时，GCT鼓励该通道与其他通道合作。</p><p>此外，当门控权重和门控偏置为0时，允许原始特征传递到下一层：
$$
\hat{x}=F(x|\alpha,0,0)=x
$$
该特性可以有效解决深层网络退化问题，ResNet也从该思想中受益。</p><p>因此建议在GCT层初始化中将γ和β初始化为0。这样，训练过程的初始步骤会更加稳定，GCT的最终表现也会更好。</p><h3>代码</h3><pre><code class="language-python">class GCT(nn.Module):

    def __init__(self, num_channels, epsilon=1e-5, mode=&#x27;l2&#x27;, after_relu=False):
        super(GCT, self).__init__()

        self.alpha = nn.Parameter(torch.ones(1, num_channels, 1, 1))
        self.gamma = nn.Parameter(torch.zeros(1, num_channels, 1, 1))
        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))
        self.epsilon = epsilon
        self.mode = mode
        self.after_relu = after_relu

    def forward(self, x):

        if self.mode == &#x27;l2&#x27;:
            embedding = (x.pow(2).sum((2, 3), keepdim=True) +
                         self.epsilon).pow(0.5) * self.alpha #[B,C,1,1]
            norm = self.gamma / \ 
                (embedding.pow(2).mean(dim=1, keepdim=True) + self.epsilon).pow(0.5)
            # [B,1,1,1],公式中的根号C在mean中体现
        elif self.mode == &#x27;l1&#x27;:
            if not self.after_relu:
                _x = torch.abs(x)
            else:
                _x = x
            embedding = _x.sum((2, 3), keepdim=True) * self.alpha
            norm = self.gamma / \
                (torch.abs(embedding).mean(dim=1, keepdim=True) + self.epsilon)
        else:
            print(&#x27;Unknown mode!&#x27;)
            sys.exit()

        gate = 1. + torch.tanh(embedding * norm + self.beta)
        # 这里的1+tanh就相当于乘加操作
        return x * gate
        
</code></pre><h2>可解释性</h2><h3>门控权重</h3><p>将门控权重在ResNet-50上的分布进行可视化：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210520220347.png" alt="image-20210520220336157"/></p><ul><li>value代表权重的值</li><li>index of layers表示权重所在网络层数的维数，越大说明其位置越靠近输出</li><li>density of params表示参数的密度，这里做了$\log(1+z)$的处理来放缩$z$轴</li></ul><p>计算门控权重在不同位置的均值和方差：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210520222730.png" alt="image-20210520220709174"/></p><p>可以看到：</p><ul><li>网络浅层，门控权重的均值小于0，GCT模块倾向于减小通道差异性，鼓励通道之间合作。</li><li>网络深层，门控权重的均值大于0，GCT模块倾向于增加通道差异性，鼓励通道之间竞争。</li></ul><h3>对于合作和竞争的解释</h3><ul><li>在网络浅层，主要学习低级特征，如纹理，边缘等，对于这些基础特征，我们需要通道之间进行合作，以更加广泛地提取特征。</li><li>在网络深层，主要学习高级特征，它们之间的差异往往很大，而且与任务有直接关系，我们需要通道之间进行竞争，来获得更有价值的特征信息。</li></ul><h2>溶解研究</h2><p>本文并没有对<strong>GCT各部分块的有效性进行探索</strong>，而是对各个部分中的p-norm方法以及激活函数进行对比。</p><p>文中仅仅给出了门控权重的有效性，并没有具体分析门控偏置和嵌入权重的作用。</p><h2>补充</h2><h3>训练</h3><p>将GCT模块添加进已有的模型时，可以先冻结网络的其他参数而只训练GCT模块中的参数，之后再将网络解冻一起训练。</p><p>也可以将GCT从一开始就加入网络之中，从头开始训练。</p><h3>思考</h3><p>通过5.1中的权重分布图可以发现，有相当大的一部分权重集中在0左右，是否可以说明GCT存在一定的冗余？</p><p>可以探索更多全局信息嵌入的方法以及归一化的方法。</p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Block Attention Module]]></title>
        <id>Convolutional Block Attention Module</id>
        <link href="https://ml.akasaki.space/blog/[16]Convolutional-Block-Attention-Module"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：CBAM: Convolutional Block Attention Module]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/abs/1807.06521">CBAM: Convolutional Block Attention Module</a></p><p>作者：Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon，Korea Advanced Institute of Science and Technology, Daejeon, Korea</p></blockquote><h2>摘要</h2><ul><li>CBAM（Convolutional Block Attention Moudule)是一种简单有效的<a href="https://www.cnblogs.com/samshare/p/11801806.html">前馈</a>卷积神经网络注意力模块。 </li><li>该模块为混合域注意力机制（）从通道和空间两个方面依次推断attention map。</li><li>CBAM是一个轻量级的通用模块，可以无缝集成到任何CNN中。</li></ul><p><strong>关键词:物体识别，注意机制，门控卷积</strong></p><h2>介绍</h2><ul><li>卷积神经网络(CNNs)基于其丰富的表达能力显著提高了视觉任务的性能，目前的主要关注网络的三个重要因素：<strong>深度，宽度和基数</strong>（Cardinality）。</li><li>从LeNet到残差网络，网络变的更加深入，表达形式更加丰富；GoogLeNet表明宽度是提高模型性能的另一个重要因素；Xception和ResNext则通过增加网络的<strong>基数</strong>，在节省参数的同时，来获得比深度、宽度更强的表达能力（引用于ResNext论文）。</li><li>除了这些因素之外，本文考察了与网络结构设计不同的方面——注意力。</li></ul><h2>注意力机制</h2><ul><li>注意（attention）在人类感知中起着重要的作用。人类视觉系统的一个重要特性是，它不会试图一次性处理整个场景，而是利用一系列的局部一瞥（a sequence of partial glimpses）来获得对显著部分的关注。</li><li>近年来有一些尝试将注意力机制加入CNN中，如使用Endcoder-Decoder结构注意力模块的Residual Attention Network，使用“Squeeze-Excitation“模块的SEnet。</li><li>具体可见<a href="https://asthestarsfalll.icu/2021/05/12/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a></li></ul><h2>CBAM</h2><h3>整体结构</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512124618161.png" alt="image-20210512124618161"/></p><ul><li><p>CBAM在混合域（通道域、空间域）上引入注意力机制，拥有更强的表达能力；</p></li><li><p>整个过程可以被概括为：
$$
F’=M_c(F)\otimes F<!-- -->\<!-- -->
F&#x27;&#x27;=M_s(F&#x27;)\otimes F&#x27;
$$
其中$F$为模块的输入，$M_c、M_s$表示通道注意力图和空间注意力图，$\otimes$表示element-wise multiply，在具体的实现过程中会相应的进行广播。</p></li></ul><h3>Channel attention module</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512130136918.png" alt="image-20210512130136918"/></p><p>利用通道之间的关系生成通道注意图（channel attention map），具体可见<a href="https://asthestarsfalll.icu/2021/05/12/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a></p><p>通道注意力主要关注的是图像的“什么”更有意义。</p><p>实现过程：</p><ol><li>对input进行<strong>全局平均池化</strong>和<strong>全局最大池化</strong>来聚集空间内的信息;</li><li>通过一个<strong>共享的</strong>MLP(多层感知机)(具体实现可用1X1卷积层),为了减少参数隐藏层的通道数被设置为$\frac{C}{R}$，在第一层之后设置了ReLU函数来引入非线性（类似于SENet，这种结构出现在各种网络之中，作用之一是为了减少参数和计算量，作用之二是为了获得更多的非线性）；</li><li>对应求和之后经过一个ReLU层得到最终的Channel attention map</li><li>将其与input相乘（会自动进行广播）。</li></ol><p>代码复现：</p><pre><code class="language-python">class Channel_module(nn.Module):
    def __init__(self,  in_ch, ratio=16):
        super(Channel_module, self).__init__()
        self.global_avg = nn.AdaptiveAvgPool2d(1)
        self.global_max = nn.AdaptiveMaxPool2d(1)
        self.fc1 = nn.Conv2d(in_ch, in_ch//ratio, 1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(in_ch//ratio, in_ch, 1, bias=False)

    def forward(self, x):
        a = self.fc2(self.relu(self.fc1(self.global_avg(x))))
        m = self.fc2(self.relu(self.fc1(self.global_avg(x))))
        attention = F.sigmoid(a+m)
        return x*attention
</code></pre><h3>Spatial attention module</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512133601683.png" alt="image-20210512133601683"/></p><p>利用空间之间的关系来生成空间注意力图（spatial attention map）</p><p>空间注意力主要关注“哪里”有重要信息，与通道注意力相辅相成。</p><p>实现过程：</p><ol><li>在通道维度上分别进行平均池化和最大池化，然后进行concat；</li><li>经过一个7X7的卷积层，将通道数降为1；</li><li>Sigmoid函数；</li><li>与inputs也就是上一层的Channel-refined feature对应相乘。</li></ol><p>代码复现：</p><pre><code class="language-python">class Spatial_module(nn.Module):
    def __init__(self, kernel_size=7):
        super(Spatial_module, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=3,
                              bias=False)  # 使用padding保持大小不变

    def forward(self, x):
        a = torch.mean(x, dim=1, keepdim=True)  # 沿着channel维度计算均值和最大值
        m, _ = torch.max(x, dim=1, keepdim=True)
        cx = torch.cat([a, m], dim=1)
        cx = F.sigmiod(self.conv(cx))
        return cx*x
</code></pre><h3>Arrangement of attention modules.</h3><p>以上两个注意力模块计算互补的注意力，考虑到这一点，这两个模块可以并行或是顺序排列。实验表明，顺序排列比并行排列效果更好，其中通道优先顺序略好于空间优先顺序。</p><h2>Ablation studies（消融研究）</h2><p>作者团队首先寻找计算通道注意的有效方法，然后是空间注意。最后，我们考虑如何结合通道关注模块和空间关注模块。</p><h3>Channel attention</h3><p>作者团队比较了3种不同的通道注意力:平均池化、最大池化和两种池化的联合使用。</p><p><img src="https://i.loli.net/2021/05/12/kFItTg6yD1u5joJ.png" alt="image-20210512160839407"/></p><p>可以看到，最大池化与平均池化同样重要，而SENet忽略了最大池化的重要性。</p><p><strong>对显著部分进行编码的最大池化特征可以补偿对全局信息软编码的平均池化特征</strong>。</p><p>在空间注意力的研究当中，将直接使用最大池化特征和平均池化特征，并将R设置为16。</p><h3>Spatial attention</h3><p>作者团队考虑了两种空间注意力的方案：<strong>一是使用通道维度上的平均池化和最大池化，二是使用1X1卷积进行降维</strong>。此外还研究了3X3和7X7卷积核的影响。在实验当中，将空间注意力模块置于通道注意力模块之后。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512162421957.png" alt="image-20210512162421957"/></p><p>可以看到，通道池化的效果更好，同时，使用较大的核会产生更好的精度，这意味着需要一个更大的感受野来决定空间上的重要区域。</p><h3>Arrangement of the channel and spatial attention.</h3><p>作者团队考虑了三种不同的模块安排方案：通道优先，空间优先和并行。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512163932604.png" alt="image-20210512163932604"/></p><p>可以看到，通道优先的效果更好。</p><h3>最终效果</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512164218849.png" alt="image-20210512164218849"/></p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boundary IoU - Improving Object-Centric Image Segmentation Evaluation]]></title>
        <id>Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</id>
        <link href="https://ml.akasaki.space/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：Boundary IoU: Improving Object-Centric Image Segmentation Evaluation]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/abs/2103.16562">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></p><p>作者：Bowen Cheng，Ross Girshick，Piotr Dollár，Alexander C. Berg，Alexander Kirillov</p><p>Code：<a href="https://github.com/bowenc0221/boundary-iou-api">https://github.com/bowenc0221/boundary-iou-api</a></p></blockquote><p>写在前面：</p><p>​	<strong>正如它的名字，Boundary IoU就是边界轮廓之间的IoU。</strong></p><p>​	重点为3.4节、5.1节，其他基本都是对比实验。</p><h1>摘要</h1><ul><li>提出了一种新的基于边界质量的分割评价方法——Boundary IoU；</li><li>Boundary IoU对大对象的边界误差比标准掩码IoU测量明显更敏感，并且不会过分惩罚较小对象的误差；</li><li>比其他方法更适合作为评价分割的指标。</li></ul><h1>介绍</h1><ul><li><p>对于分割任务，不同的评估指标对不同类型错误的敏感性不同，网络可以轻易解决对应敏感的类型，而其他错误类型的效果则不尽人意；</p></li><li><p>mask的边界质量是图像分割的一个重要指标，各种下游任务直接受益于更精确的目标分割；</p></li><li><p>目前的分割网络的预测不够保真，边缘也很粗糙，<strong>这种情况说明目前的评估指标可能对目标边界的预测误差具有有限的敏感性</strong>；</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210508214210.png" alt="image-20210508214206239"/></p></li><li><p>在大量的论文中，AP最高可达到八九十，而很少有论文会提及他们mask的边界质量。</p></li><li><p>对于实例分割，本文提出<strong>Boundary Average Precision</strong> (Boundary AP)，对于全景分割，提出<strong>Boundary Panop-tic Quality</strong> (Boundary PQ)。</p></li></ul><h1>相关指标</h1><p>各种相关指标如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210508223158.png" alt="image-20210508222750295"/></p><p>首先解释几个名词：</p><ol><li><p>对称（Symmetric）：GT（GroundTruth）和Pred（prediction）的交换是否改变测量值</p></li><li><p>倾向（Preference）：衡量方法是否偏向某一类型的预测。</p></li><li><p>不灵敏度（Insensitivity）：测量不太敏感的误差类型。</p></li><li><p>三分图（Trimap）：对给定图像的一种粗略划分将给定图像划分为前景、背景和待求未知区域。</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509151808.png" alt="img"/></li><li><p>Mask-based Measure：考虑物体的所有像素</p></li><li><p>Boundary-based Measure：衡量预测边界的分割质量，不同于Mask-based Measure，该方法只评估边界及其邻近的像素。</p></li><li><p>d：边界窄带的像素宽度</p></li></ol><p>通过分析各种相关指标的缺点，我们得出Boundary IoU应该拥有的特性：<strong>同时考虑分类、定位和分割质量。</strong></p><h2>Mask IoU和Pixel Accuracy</h2><p>所有像素对指标的贡献都是相同的，而物体内部的像素呈二次型增长，其边界仅会线性增长，因此<strong>对较大物体的边界不够敏感</strong>。</p><p>Mask IoU计算方式示意图：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510161356.png" alt="image-20210510161350211"/><h2>Trimap IoU</h2><p>基于边界的分割指标，其计算距离GT和pred边界d像素窄带内的IoU，计算方式示意图如下（方便起见，简化为矩形且只显示边界部分）：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509163907.png" alt="image-20210509163853163"/><p><strong>需要注意分母的</strong>$G_d\cap G$。</p><h2>Feature Measure</h2><p>F-Measure最初被提出用于边缘检测，但它也被用于评价分割质量。在最初的公式中，使用二分图匹配来进行计算，对于高分辨率的图像来说计算成本很大；因此提出了一种允许重复匹配的近似算法，<strong>precision为pred轮廓中 \ 距离GT轮廓中像素 \ 在d个像素以内的 \ 像素 \ 所占pred的比例</strong>（已断句），recall同理。不是很理解，原文如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510151207.png" alt="image-20210510151147870"/></p><p>Precision和Recall计算方式示意图如下（可能）：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510153516.png" alt="image-20210510152547915"/><h2>Boundary  IoU</h2><p>Boundary IoU对大物体边界误差更加敏感，并且不会过分惩罚小物体。</p><p>直观上就是GT和Pred轮廓的交集除以并集，但是<strong>这里的轮廓是在对象内部的</strong>$G_d、P_d$，不包括在对象外面的部分，详细请看9.1。</p><p>虽然看起来和Trimap IoU很相似，但个人认为它是Mask IoU的边界升级版本，去除了对象内部巨量像素对整体的影响（见5.1Mask IoU的分析），使其拥有更优秀的性质。 </p><p>完整的论文中给出的示意图如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510154511.png" alt="image-20210510153535293"/></p><p>我画的：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510154514.png" alt="image-20210510154509338"/><h1>敏感性分析</h1><p>为了进行系统的比较，本文对GT进行处理形成伪预测，通过<strong>模拟</strong>不同的误差类型来尽可能的模拟真实误差类型。</p><h2>尺度误差</h2><p>通过对GT进行膨胀和腐蚀操作，误差严重程度由运算核半径控制。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185613.png" alt="image-20210509185608432"/></p><h2>边界定位误差</h2><p>将随机高斯噪声添加到GT上每一个多边形顶点的<strong>坐标</strong>上，误差严重程度由高斯噪声的标准差确定。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185617.png" alt="image-20210509185545908"/></p><h2>物体定位误差</h2><p>将GT中的对象随机偏移一些像素，误差严重程度由位移像素长度控制。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185622.png" alt="image-20210509185530435"/></p><h2>边界近似误差</h2><p>利用Sharply的简化公式来删除多边形顶点，同时保持简化多边形尽可能接近原始图像，误差严重程度由函数的容错参数控制。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185624.png" alt="image-20210509185108649"/></p><h2>内部掩码错误</h2><p>向GT中添加随机性形状的孔，虽然这种误差类型并不常见，但是本文将其包含进来，用以评估内部掩膜误差的影响。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185630.png" alt="image-20210509185508685"/></p><h2>实现细节</h2><p><strong>数据集</strong>：作者从LVIS V0.5验证集中随机抽取实例掩码，因为该数据集拥有高质量的注释。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509190212.png"/></p><p><strong>实现过程</strong>：通过改变误差类型和误差的严重程度，记录每种类型的平均值和标准差，此外，还通过划分不同的区域，来比较对不同大小物体的指标评价。</p><p>其中d设置为图像对角线的2%。</p><h1>现有方法分析</h1><h2>Mask IoU</h2><h3>理论分析</h3><p><strong>尺度不变性</strong>（自己取的）：即对于一个<strong>固定</strong>的Mask IoU值，分割对象面积越大，则其错误像素越多，二者之间的变化关系成正比，其比例即为Mask IoU的值。</p><p><strong>惩罚差异性</strong>（自己取的）：然而，当缩放一个对象时，内部像素数量呈二次增长，边界像素仅为线性增长，二者不同的增长率导致Mask IoU容忍更大的对象边界上的更多错误分类。</p><h3>实证分析</h3><p><strong>尺度不变性</strong>基于一个假设，即GT标注中的边界误差也随着对象的大小而增长。</p><p>然而已有研究表明，不论物体大小，被不同标注器标记的同一个对象的两个轮廓之间的像素距离很少超过图像对角线的1%。（就叫它<strong>标注相似性</strong>吧）</p><p>本文通过研究LVIS提供的双标注图像来证实这一点，如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509203230.png" alt="image-20210509201406961"/></p><p>其中冰箱的面积是机翼面积的100倍，但在相同分辨率的区域内，注释之间的差异在视觉上十分相似。</p><p>两者的两个轮廓的Mask IoU分别为0.97,0.81，而它们的Boundary IoU则更为接近，分别为0.87，0.81。说明Mask IoU<strong>对小尺寸图片的“惩罚”更大</strong>。</p><p><strong>实验</strong>：通过严重程度相同的膨胀/腐蚀来模拟<strong>尺度误差</strong>，其显著降低了小物体的Mask IoU，而Mask IoU随物体大小的增加而增加，见下图：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510093856.png" alt="image-20210510093853486"/></p><h3>总结</h3><ul><li>Mask IoU的主要不足在于对大物体边界的不敏感性。</li><li>相比之下，Boundary IoU更注重物体的边界。</li></ul><h2>Trimap IoU</h2><p>Trimap IoU是不对称的，交换GT和Pred将会得到不同的值。下图显示了其更倾向于比GT更大的pred：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510095941.png" alt="image-20210510095821668"/></p><p>可以看到：</p><ul><li>不论膨胀的严重程度是多少，其值总会大于某个正值，对小物体的“惩罚”依然过大。</li><li>腐蚀则会下降到零。</li></ul><p>简单的证明：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114830.png" alt="image-20210510165235885"/></p><p>蓝色部分为pseudo-predictions （伪预测），红色方框为GT轮廓，可以看到，当pseudo-predictions 完全包含了GT时，其值不会再改变</p><p>同理，当伪预测完全被GT所包含，分子为0，最终值为0。</p><h2>F-measure</h2><p>F-measure完全忽略了小的轮廓误差，但是表现效果很差，会在很短的严重程度中快速下降到0：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114828.png" alt="image-20210510170006064"/></p><h2>总结</h2><p>综上可知，F-measure和Trimap IoU都不能代替Mask IoU，而Mask IoU也有着不能忽视的缺陷，因此，本文提出Boundary IoU。</p><h1>Boundary IoU</h1><h2>公式</h2><p>一个简化的IoU公式
$$
IoU = \frac{G_d\cap P_d}{G_d\cup P_d}
$$
该公式直接使用$G_d、P_d$,丢失了边缘的尖锐部分的信息</p><p>Boundary IoU公式如下：
$$
Boudary-IoU(G,P)=\frac{|(G_d\cap G)\cap(P_d\cap P)|}{|(G_d\cap G)\cup(P_d\cap P)|}
$$
其中参数d控制了测量的灵敏性，当d足够大时，Boundary IoU就相当于Mask IoU;若使用较小的d，Boundary IoU则会忽略内部像素，使其对边界像素更加敏感。</p><p>此外，对于较小的对象，Boundary IoU十分接近甚至等价于Mask IoU，这主要取决于参数d。</p><h2>Mask IoU vs Boundary IoU：敏感性分析</h2><p>本文对比了Mask IoU和Boundary IoU在面积大于$96^2$的物体的不同误差类型下的表现：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181123.png" alt="image-20210510173824215"/></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181124.png" alt="image-20210510173839905"/></p><p>对于每种误差类型，Boundary IoU都能更好的利用0-1的范围</p><p>使用的固定的误差严重程度，对大小不同的对象使用伪预测，以$16^2$为增量划分区域，二者表现如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181127.png" alt="image-20210510181102929"/></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181129.png" alt="image-20210510181118302"/></p><p>可以看到：</p><ul><li>对于较大的对象，Boundary IoU在相同严重程度下保持平缓，而Mask IoU则明显的偏向于大物体；</li><li>对于较小的对象，二者拥有相似的指标，说明他们都没有对其进行过度惩罚。</li></ul><h2>Boundary IoU vs  Trimap IoU</h2><p>二者具有一定的相似性，Boundary IoU将Pred和GT边缘上的像素都考虑了进来，这个简单的改进改变了Trimap IoU两点不足，一是不对称，二见4.2。</p><h2>Boundary IoU vs F-measure</h2><p>F-measure对轮廓之间使用了硬预测——如果轮廓之间的像素在距离d内那么Precision和Recall都是完美的，然而当它们都位于d之外，则不会发生任何匹配（见4.3 ，其值会很快的降为0）。</p><p>而Boundary IoU使用一种软分割，变化平缓。</p><p>在附录中将会进行详细分析。</p><h2>像素距离参数d</h2><p>上文提过，当d足够大时，Boundary IoU等价于Mask IoU，当d过小，Boundary IoU则会出现严重惩罚的情况。</p><p>为了选择合适的参数d，本文在COCO和ASE20K两个数据集（它们拥有相似的分辨率）上进行实验，发现当d为图像<strong>对角线的2%（大约为15个像素）</strong>时，两数据集的Boundary IoU的中位数超过0.9。</p><p>对于Cityscapes中更大分辨率的图像，作者也建议使用相同的像素距离（15个左右），设置d为对角线的0.5%</p><p>对于其他数据集，作者建议考虑两个因素（<strong>没看懂</strong>：</p><ol><li>将注释一致性将下界设为d</li><li>D应根据当前方法的性能选择，并随着性能的提高而降低。</li></ol><h2>Boundary IoU的局限</h2><p>Boundary IoU不评估距离轮廓超过d的像素，例如一个圆形Mask和一个环形Mask：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114822.png" alt="image-20210510190200706"/></p><p>显然，其Boundary Iou值极高为1</p><p>为了惩罚这种情况，作者建议组合Boundary IoU和Mask IoU，并取他们的最小值。</p><p>此外，在实验中还发现，99.9%的情况Boundary IoU都是小于等于Mask IoU的，极少数情况如上图会出现Boundary IoU大于Mask IoU。</p><h1>应用</h1><p>如上文所说，作者将两种IoU组合，取其最小。</p><h2>Boundary AP for instance segmentation</h2><p>实例分割任务的目标是用像素级掩码描绘每个对象，其评估指标是同时评估多个方面，如分类、定位和分割质量。</p><p>本文通过（Synthetic predictions，Synthetic，综合的；合成的，人造的，结合上下文个人感觉应该取“人造”之意） 合成预测与真实模型来进行实验。</p><h3>合成预测</h3><blockquote><p>综合预测允许我们单独的评估分割质量。</p></blockquote><ul><li><p><strong>具体方法</strong>：</p><p>使用COCO数据集，将GT缩小为28X28的连续值掩码，使用双线性插值upscale it back，最后将其二值化。如下图所示</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510230303.png" alt="image-20210510230301360"/></p><p>这种合成Mask十分接近GT，但这种差异随着物体大小的增大而增大，因此越大的物体经过处理后的IoU值应该越低。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122838.png" alt="image-20210510223226154"/></p><p>下标表示物体的大小，可以看到，对于越大的物体，Boundary IoU的值越低，而Mask IoU的值则维持在高水平，<strong>这进一步显示了Boundary IoU对于大物体边界的敏感性</strong>。</p></li><li><p>实验结果：在Mask RCNN、PointRend、以及BMask RCNN模型上进行实验，结果如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122836.png" alt="image-20210510224102719"/></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122835.png" alt="image-20210510224120918"/></p><p>众所周知，Mask RCNN对大物体的分割表现不尽人意（我不知道），从上表可以看出Boundary Ap的优越性</p><p>此外，上表还证明了相较于BMask RCNN，PointRend对较大对象的表现更好。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122833.png" alt="image-20210510224713604"/></p><p>上表显示了更深的主干网络并不能带来分割质量的显著提升。</p></li></ul><h3>真实预测</h3><blockquote><p>利用现有的分割模型得到的真实预测进一步实验，可以进一步了解Boundary IoU在实例分割任务各个方面的表现。</p></blockquote><ul><li><p><strong>具体方法</strong>：</p><p>为了将分割质量与分类和定位错误分离开，作者为这些方法提供了Ground Truth Box，并为其分配随机置信度。</p></li><li><p><strong>实验结果</strong>：</p><p>模型在COCO数据集上训练，在LVIS v0.5上验证</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115505.png" alt="image-20210511115502161"/></p><p>模型在Cityscapes上训练和验证</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115657.png" alt="image-20210511115655795"/></p></li></ul><h2>Boundary  PQ</h2><p>下图为标准PQ的公式</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115040.png" alt="image-20210511115032369"/><p>将其中的Mask IoU替换为Mask IoU与Boundary IoU的组合，取其最小值。</p><h3>合成预测</h3><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122826.png" alt="image-20210511120047274"/><h3>真实预测</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122824.png" alt="image-20210511120131765"/></p><h1>总结</h1><p>​		不同于Mask IoU，Boundary IoU提供了一个明确的，定量的梯度，奖励改善边界分割质量。作者希望Boundary IoU可以鼓励更多人开发高保真Mask预测新方法。此外，Boundary  IoU允许对复杂的任务(如实例和全景分割)的分割相关错误进行更细粒度的分析。在性能分析工具(如TIDE<!-- -->[2]<!-- -->)中结合度量可以更好地洞察实例分段模型的特定错误类型。（<strong>直接翻译的</strong>）</p><h1>补充</h1><h2>$G_d$和$G_d\cap G$</h2><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511121300.png" alt="image-20210511121230297"/></p><h2>代码复现</h2><p>对于二分类图像的Boundary Iou</p><pre><code class="language-python"># 将二值Mask转化为Boundary mask
def mask_to_boundary(mask, dilation_ratio=0.01):
    h, w = mask.shape
    img_diag = np.sqrt(h ** 2 + w ** 2)
    dilation = int(round(dilation_ratio * img_diag))
    if dilation &lt; 1:
        dilation = 1
    # Pad image so mask truncated by the image border is also considered as boundary.
    # 将mask使用0填充一圈，防止dilation为1时
    new_mask = cv2.copyMakeBorder(
        mask, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0)
    kernel = np.ones((3, 3), dtype=np.uint8)
    # 对mask进行腐蚀操作
    new_mask_erode = cv2.erode(new_mask, kernel, iterations=dilation)
    mask_erode = new_mask_erode[1: h + 1, 1: w + 1]
    # G_d intersects G
    return mask - mask_erode

def boundary_iou(mask, pred):
    intersect = mask*pred
    ite = np.sum(intersect == 1)
    un = mask+pred
    union = np.sum(un &gt;= 1)
    return ite/union
</code></pre><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091830.png" alt="image-20210519091807762"/></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091840.png" alt="image-20210519091815466"/></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091833.png" alt="image-20210519091826066"/></p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Involution - Inverting the Inherence of Convolution for Visual Recognition]]></title>
        <id>Involution - Inverting the Inherence of Convolution for Visual Recognition</id>
        <link href="https://ml.akasaki.space/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：Involution: Inverting the Inherence of Convolution for Visual Recognition]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/abs/2103.06255"><em>Involution: Inverting the Inherence of Convolution for Visual Recognition</em></a></p><p>作者：Duo Li， Jie Hu， Changhu Wang， Xiangtai Li， Qi She， Lei Zhu， Tong Zhang， Qifeng Chen， The Hong Kong University of Science and Technology， ByteDance AI Lab， Peking University， Beijing University of Posts and Telecommunications</p></blockquote><h1>Convolution</h1><ol><li><a href="https://arxiv.org/abs/1805.12177">空间无关性(spatial agnostic)</a>：same kernel for different position<ul><li>优点：参数共享，平移等变</li><li>缺点：不能灵活改变参数，卷积核尺寸不能过大，只能通过堆叠来扩大感受野、捕捉长距离关系</li></ul></li><li>通道特异性(channel specific)：different kernels for different channels<ul><li>优点：充分提取不同通道上的信息</li><li>缺点：有冗余</li></ul></li></ol><p>Convolution kernel 尺寸为 B,C_out,C_in,K,K</p><h1>Involution</h1><p>与convolution不同，involution拥有<strong>完全相反</strong>的性质：</p><ol><li>空间特异性：kernel privatized for different position</li><li>通道不变性：kernel shared across different channels</li></ol><p>involution kernel 的尺寸为B,G,KK,H,W.</p><h1>how to generate involution kernels</h1><p>kernel generated based on input featrue map(self-attention的一种体现？) to ensure kernel size aligned with the input tensor size</p><p>一种简单的kernel generation function,方便起见以一个像素为例</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210507124123.png" alt="image-20210426192156487"/></p><ol><li>inputs为维度1×1×C；</li><li>线性变换：$W_0$：通道压缩，节省计算量；$W_1$：首先变为1×1×(K×K×G)，再拆分为G组，最后变换为K×K×G；（其生成的卷积核包含了所有通道上的信息，对不同通道之间的信息交换有一定的作用）</li><li>生成的kernel与(i,j)像素领域进行乘加操作，因为维度不同，需要进行广播，得到大小为k×k×C；</li><li>最后进行聚合，输出大小为1×1×C。</li></ol><pre><code class="language-python">class Involution(nn.Module):
    def __init__(self, channel, group, kernel, s):
        super(Involution, self).__init__()
        self.channel = channel
        self.group = group
        self.kernel_size = kernel
        ratio=4

        self.o = nn.AvgPool2d(s, s) if s &gt; 1 else nn.Identity()
        self.reduce = nn.Sequential(
            nn.Conv2d(channel, channel//ratio, 1),
            nn.BatchNorm2d(channel//ratio),
            nn.ReLU(inplace=True)
        )
        self.span = nn.Conv2d(channel//ratio, kernel**2*group, 1)
        # 从一个Batch中提取出卷积滑动的局部区域块，较难理解，建议自行百度
        # 普通的卷积操作实际上就相当于将feature map unfold与conv kernel乘加之后再fold
        self.unfold = nn.Unfold(
            kernel_size=kernel, padding=(kernel-1)//2, stride=s)

    def forward(self, x):
        kernel = self.span(self.reduce(self.o(x)))  # B,KKG,H,W
        B, _, H, W = kernel.shape
        kernel = kernel.view(B, self.group, self.kernel_size **
                             2, H, W).unsqueeze(2)  # B,G,1,kk,H,W，unsqueeze：增加一个维度用于广播

        x_unfolded = self.unfold(x)  # B,CKK,HW
        x_unfolded = x_unfolded.view(
            B, self.group, self.channel//self.group, self.kernel_size**2, H, W)# B,G,C/G,KK,H,W

        out = (kernel*x_unfolded).sum(dim=3)  # B,G,C/G,H,W
        out = out.view(B, self.channel, H, W) # B,C,H,w
        return out

</code></pre><p>更多：</p><ol><li>对Involution kernel的生成方式进行更多的探索；</li><li>进一步探索Convolution-Involution的混合结构。</li></ol><h1>Involution	vs	Convolution</h1><p>优点：</p><ol><li><p>参数量和计算量都很少</p><ul><li><p>对于Convolution，其参数量为：
$$
K^2C<em>{in}C</em>{out}
$$
计算量大约为：
$$
HWK^2C<em>{in}C</em>{out}
$$</p></li><li><p>对于Involution，其参数量为：
$$
\frac{C^2+CGK^2}{r}
$$
计算量大约为：
$$
HWK^2C
$$</p></li></ul><p>可以看到，involution的计算量与通道数呈线性关系。</p></li><li><p>能有效建模长距离关系</p><p>相较于Convolution，involution kernel可以使用更大的卷积核而不过多增加其参数量，其感受野也就越大。</p></li><li><p>involution是动态的，而convolution是静态的。</p></li></ol><p>缺点：</p><ol><li><p>通道间的信息交换在一定程度上受到影响</p><p>虽然同一组内共享同一个kernel，但是不同组通道间的信息交换还是会受到影响。</p></li><li><p>速度相较于Convolution没有优势</p></li></ol><h1>Relation to Self-Attention</h1><blockquote><p>self-attention可以看作广义involution的一种实例</p></blockquote><p>可以看到与self-attention之间的相似性：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210507124139.png" alt="image-20210427212347084"/></p><p>相似：</p><ol><li>其中H可以类比为involution中的G；</li><li>self-attention中每个位置的关系矩阵可以类比为involution中每个位置的kernel。</li></ol><p>不同：</p><ol><li>相比于self-attention，Involution潜在编码了位置信息，而self-attention需要position encoding来区分位置信息.</li><li>不在需要使用Q-K，仅依靠单个像素生成kernel，而非依靠像素间的关系生成attention map.</li></ol><p>总结：self-attention是Involution的一种实例化，且Involution的表达更为宽泛和简洁。</p><h1>Ablantion    Analysis</h1><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210507124142.png" alt="image-20210427213135224"/></p><p>可以看到：</p><ul><li>Involution能在不显著提升参数量和计算量的前提下，增大kernel的感受野，提升网络性能;</li><li>在显著降低计算量和参数量的情况下，准确度损失却不大。</li></ul><h1>其他</h1><ol><li><p>关于卷积的可替代性</p><p>特征在空间位置上差异明显，我们更需要注意长距离关系时，involution或许是个好的选择。</p></li><li><p>训练与优化</p><p>不同于convolution，involution实际上是二阶优化，需要优化的并不是kernel，而是kernel生成函数里的参数，这就会造成很多问题（最近的transformer优化过程也有很多问题），作者建议对于某些网络需要使用gradient clipping等方法来进行更好的优化。</p></li><li><p>硬件支持</p><p>involution的优化并没有convolution好，也没有相应硬件的支持，因此虽然参数量和计算量都减小了，但是实际并没有convolution快，作者建议使用CUDA编写Involution。</p></li></ol>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PointRend - Image Segmentation as Rendering]]></title>
        <id>PointRend - Image Segmentation as Rendering</id>
        <link href="https://ml.akasaki.space/blog/[19]PointRend-Image-Segmentation-as-Rendering"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[image-20210601121147760]]></summary>
        <content type="html"><![CDATA[<p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601121147760.png" alt="image-20210601121147760"/></p><blockquote><p>“我们希望预测分割图的边界区域更加准确，我们就不应该使用均匀采样，而应该更加倾向于图像边界区域。”</p></blockquote><p>这是一篇用于改善图像分割问题中边缘分割效果的方法的论文的阅读笔记。该方法“将分割问题看作渲染问题”，达到了较好的效果。论文原文：<a href="https://arxiv.org/abs/1912.08193">PointRend: Image Segmentation as Rendering</a>。在阅读这篇笔记之前，请确保先了解图像分割技术。对分割的技术进行简要的了解，可以参考<a href="./%5B10%5DOverview-Of-Semantic-Segmentation">另一篇笔记</a>。</p><h2>Abstract（摘要）</h2><blockquote><p>We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend&#x27;s efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend">this https URL</a>.</p></blockquote><h2>介绍（Introduction）</h2><p>我们希望预测分割图的边界区域更加准确，我们就不应该使用均匀采样，而应该更加倾向于图像边界区域。这种类似的采样问题在计算机图形学中已经被研究了几十年了，图像渲染，将一个模型（比如，3D网格）映射为一个rasterized image（即一个像素的规则网格），虽然输出是规则的，但是计算的时候却不是根据网格上均匀采样来计算。常见的策略就是在图像平面内自适应地采样一些点产生不规则的子集，再来进行计算。</p><p>这篇文章的中心思想就是将图像分割问题视作图像渲染问题，使用来自于计算机图像学中的经典思想设计到神经网络中，渲染出更高质量的分割图。这个设计的核心内容是 PointRend（基于点的渲染）神经网络模块：“一个基于迭代细分算法在自适应选择的位置执行基于点的分割预测的模块”。该模块使用细分策略自适应地选择一组非均匀点来计算标签，容易理解的说法是该模块自动选取边缘上难以正确归类的点进行再次分类。 它的细分策略使用比直接密集计算少一个数量级的浮点运算来有效地计算高分辨率分割图。</p><p>请注意，PointRend是一个网络模块而不是独立的网络。该模块接受一个或多个CNN输出的feature map，并产生比输入更高分辨率的预测。</p><blockquote><p>PointRend is a general module that admits many possible implementations.</p></blockquote><p>PointRend 可以合并到流行的元架构中，用于实例分割（例如，Mask R-CNN）和语义分割（例如FCN）。</p><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601154033700.png" alt="image-20210601154033700"/></p><p>上图是一种可能的PointRend结构模式图：选择一些分割困难的点，CNN产生的特征图被输入到MLP中，和粗分割结果融合，最终输出分割困难点的预测结果。</p><p>实际上，这篇论文的创新在于重新选择边缘困难点的种类。本质上这篇论文其实是一个新型上采样方法，针对物体边缘的图像分割进行优化，使其在难以分割的物体边缘部分有更好的表现。在整个PointRend设计的过程中有类似于渲染的思想，但请不要对“渲染”过度理解。</p><hr/><h2>方法（Method）</h2><h3>PointRend模块</h3><p>类似于在计算机图形学中，屏幕上某个位置的像素是从一个曲线、模型等通过被称为渲染的映射产生，在计算机视觉中，我们可以将图像分割视为底层连续实体的类别图，而分割输出，即预测标签的像素集合，是从中“渲染”出来的。</p><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601161409751.png" alt="image-20210601161409751"/></p><p>上图是这个模式的简图。其中，PointRend的关键步骤就是训练一个“解码器”（或者叫“渲染器”），从channel中“解码”（或者“渲染”）出预测困难的像素所属的类别。</p><p>PointRend模块接受一个或多个典型的$C$通道的CNN特征图（$f\in \R^{C\times H\times W}$）作为输入，这些输入特征图往往比需要预测的图像的实际尺寸要小4~16倍。PointRend模块会输出一个大小为$p\in \R^{k\times H&#x27; \times W&#x27;}$的对K个类别的预测。通常输出的大小会大于输入的大小。</p><p>（如果读不懂了请先参考<a href="./%5B10%5DOverview-Of-Semantic-Segmentation">另一篇笔记</a>了解图像的分割技术）</p><h3>PointRend模块的组成</h3><p>PointRend模块由单个主要的部分组成：</p><ol><li>一个<strong>点的选择策略</strong>。PointRend模块并不会对整幅图片上的所有点进行预测（这样会产生巨大的开销），而是选择其中的一部分看上去“难以预测的实值点”进行预测（实值点的特征是通过输入$f$的双线性插值计算的）。</li><li>一个<strong>特征提取器</strong>（或者叫“解码器”，或是“渲染器”）。对于每个选定的点，在输入中相关的部分可能是$i\in \R^{C\times 1\times 1}$的一个长长的通道。对于每个选定的点，特征提取器提取该点的特征表示。</li><li>一个<strong>分类器</strong>（原文中称之为“point head”）。这个分类器是一个很小的神经网络，它被训练来从这个逐点特征表示中预测一个标签。对于这部分来说，每个点都是独立的。</li></ol><p>这几个组成部分将会在下文中进行详细介绍。</p><p>PointRend 架构可应用于实例分割（例如，在 Mask R-CNN上）和语义分割（例如，在 FCNs上）任务。</p><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601154033700.png" alt="image-20210601154033700"/></p><p>如上图，PointRend 模块应用于每个区域。它通过对一组选定点进行预测，以从粗到细的方式计算掩码。对于语义分割，可以将整个图像视为单个区域，因此不失一般性。</p><h3>点的选择策略（Point selection for Inference and Training）</h3><p>PointRend方法的核心是灵活和自适应地选择图像平面中预测分割标签的点。直觉上，这些点应该更密集地靠近高频区域，例如物体边界，类似于光线追踪中的抗锯齿问题。</p><p>PointRend的推理选择策略受到计算机图形学中自适应细分的经典技术的启发。该技术用于通过仅在值与其邻居显著不同的位置进行计算来高效地渲染高分辨率图像。对于其他位置，这些值是通过插入已经计算出的输出值获得的。下面是一段解释：</p><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601170513086.png" alt="image-20210601170513086"/></p><p>在渲染问题中，例如，在一块分辨率为1080p的屏幕上显示一张2k的图片时，往往显示设备不需要对2k图片的所有像素都进行处理，仅处理一部分即可显示出足够的分辨率。但当用户放大图片，在1080p的显示屏上显示一张2k图片中的一角时，显示设备就需要对这张图片进行更精细的处理让它们出现在屏幕上显示更多细节。通过观察我们发现，1和2部分和原来的像素差距不大，不需要从图片文件渲染，处理时仅需同上文中的“这些值是通过插入已经计算出的输出值获得的”，从放大前的图上插值即可得到。需要重新处理的，是1和2的边缘部分，也就是上面提到的“值与其邻居显著不同的位置”。</p><blockquote><p> 类比渲染问题中的细分过程和分割问题中的上采样过程后我们发现，在分割中的上采样过程中时我们需要关注边缘，对边缘进行细化即可。</p></blockquote><p>所以点的选择策略，就是在上采样过程中选择“不确定的点”，或“边界的点”。</p><h4>前向传播过程（Inference）</h4><blockquote><p>对于每个区域，我们以粗到细的方式迭代地“渲染”输出掩码。</p></blockquote><p>在网络的前向传播过程到达PointRend模块时，会总体经历以下步骤：</p><ol><li>使用双线性插值对其先前预测的分割进行上采样。</li><li>在这个上采样的特征图上选择 N 个最不确定的点（例如，对于二进制掩码，可以选择概率最接近 0.5 的那些点）。</li><li>为这 N 个点中的每一个计算逐点特征表示（在第 3.2 节中简要描述）并预测它们的标签。</li><li>重复这个过程，直到分割被上采样到所需的分辨率。</li></ol><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601200340192.png" alt="image-20210601200340192"/></p><p>上图是这个过程的放在整个网络中的大致流程示意图，下面是这个过程的局部流程图。</p><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601205123801.png" alt="image-20210601205123801"/></p><p>整个过程就是比双线性插值的上采样多了一个选择不确定点和预测其种类的过程。整个前向传播的过程简单易懂。</p><p>输入大小为 $M_0 \times M_0$ 并且输出大小为 $M\times M$ 的PointRend模块一次运算所需要预测的像素总量不会超过 $N \log_2 {\frac{M}{M_0}}$ ，这比 $M\times M$ 小得多，并使PointRend 更有效地进行高分辨率预测。例如，如果 $M_0$ 为 7 并且所需分辨率为 $M =224$ ，则执行 5 个细分步骤。如果我们在每一步选择 $N =28^2$ 个点，PointRend 只对 $ 28^2 · 4.25 $ 个点进行预测，这比 $224^2$ 小 15 倍。</p><h4>训练时的“不确定”点选取（Training）</h4><p>在之前的一些步骤中，我们提到PointRend模块中需要选择一些“不确定”的点进行分类。那么，怎么选择“不确定”的点呢？我们在上面提到:</p><blockquote><p>在这个上采样的特征图上选择 N 个最不确定的点</p></blockquote><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601154033700-1622553357332.png" alt="image-20210601154033700"/></p><p>采样策略在特征图上选择N个点，这N个点应该是“不确定”的。PointRend在选择点时使用三个原则使选择到的点是“不确定”的：</p><ol><li><p>过度生成：平均从输入中选择$kN$个点（其中$k&gt;1$）。</p></li><li><p>重要性采样：根据粗预测（上方图片中的“coarse prediction”）结果判断每个点的不确定性，从中选择“不确定粗略预测的点”，从刚才选取的$kN$的点中选出最不确定的$\beta N$个点（其中$\beta \in <!-- -->[0,1]<!-- -->$）。</p></li><li><p>Coverage （说实话暂时没看懂这一个是干啥的）：剩余的$(1-\beta)N$个点是从均匀分布中采样。如下图中采用了不同的$k$和$\beta$来说明这个过程。左侧是均匀分布的示意图，右侧是采用不同的$k$和$\beta$的示意图。</p><p><img src="./src/PointRend-Image-Segmentation-as-Rendering/image-20210601211914002.png" alt="image-20210601211914002"/></p></li></ol><p><code>原则上，这里的选择点的策略可以与上文中推理时点的细分策略类似。**然而，细分的策略会引入对神经网络的反向传播算法不太友好的运算步骤**，所以在选择点的时候这篇论文中选择了非迭代的随机采样策略。</code></p><h3>点的特征提取和分类（Point-wise Representation and Point Head）</h3><p>在特征提取部分，PointRend通过融合粗特征和细粒度特征获得更合适的特征，然后通过分类得到该点的类别。</p><h4>细粒度特征（Fine-grained features）</h4><blockquote><p>细粒度特征就是各种分割网络中用于稠密预测的直接特征。</p></blockquote><p>为了让 PointRend 呈现精细的分割细节，Point Rend方法中从前序CNN输出的特征图中的每个像素位置提取一个特征向量，然后进行双线性插值上采样作为这个位置像素的细粒度特征。可以从单个特征映射中提取这个特征（例如ResNet中的$res_2$）；或者，也可以从多个特征图融合得到（例如$res_2$到$res_5$，或是从它们的特征金字塔对应层的输出提取）。</p><h4>粗预测特征（Coarse prediction features）</h4><blockquote><p>粗粒度让特征“更具空间性”。</p></blockquote><p>单纯使用细粒度特征容易导致先天性的不足，那就是和周围较大范围内的像素很难产生关联，也就是所谓的空间信息丢失。这是前序CNN下采样导致的结果。例如，同时处在两个不同实体边界上的点只具有一份相同的细粒度特征，但是一个点只能被分给一个实体（也换句话说就是具有相同细粒度特征的点在不同的实体区域内应该被分类为不同标签），这就需要该点的额外的区域信息。</p><p>还有，根据产生细粒度特征所使用的特征图的不同，可能会出现细粒度特征内只包含相对低级的信息的情况。在这种情况下，具有更多上下文和语义信息的特征源会对分割的精度产生很大的帮助。</p><p>基于这些考虑，第二种特征类型是来自网络的粗分割预测，即区域（框）中每个点的 K 维向量代表 K 类预测。</p><h4>分类器（“点头”，Point Head）</h4><p>点头。给定每个选定点的逐点特征表示，PointRend 使用简单的多层感知器 (MLP) 进行逐点分割预测。这个 MLP 在所有点（和所有区域）上共享权重，类似于图卷积或 PointNet。由于 MLP 预测每个点的分割标签，它可以通过标准的特定于任务的分割损失进行训练。</p><h2>实验（Experiments）</h2><p>请参考原文。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer - Attention is all you need]]></title>
        <id>Transformer - Attention is all you need</id>
        <link href="https://ml.akasaki.space/blog/[20]Transformer-Attention-is-all-you-need"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：Attention Is All you Need]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All you Need</a></p><p>作者：Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N. Gomez，Łukasz Kaiser，Illia Polosukhin</p><p>code：<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py">https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py</a></p></blockquote><h2>前言</h2><p>基于RNN或CNN的Encoder-Decoder模型在NLP领域占据大壁江山，然而她们也并非是完美无缺的：</p><ul><li>LSTM，GRU等RNN模型受限于固有的循环顺序结构，无法实现<strong>并行计算</strong>，在序列较长时，计算效率尤其低下，虽然最近的工作如<a href="http://arxiv.org/abs/1703.10722">因子分解技巧</a><sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>，<a href="https://arxiv.org/abs/1701.06538">条件计算</a><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>在一定程度上提高了计算效率和性能，但是顺序计算的限制依然存在；</li><li>Extended Neural GPU<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>,<a href="https://arxiv.org/abs/1610.10099">ByteNet</a><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>,和<a href="https://arxiv.org/abs/1705.03122">ConvS2S</a><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup> 等CNN模型虽然可以进行并行计算，但是学习任意两个位置的信号的长距离关系依旧比较困难，其计算复杂度随距离线性或对数增长。</li></ul><p>而谷歌选择抛弃了主流模型固有的结构，提出了<strong>完全</strong>基于注意力机制的Transformer，拥有其他模型无法比拟的优势：</p><ul><li>Transformer可以高效的并行训练，因此速度十分快，在8个GPU上训练了3.5天；</li><li>对于长距离关系的学习，Transformer将时间复杂度降低到了常数，并且使用多头注意力来抵消位置信息的平均加权造成的有效分辨率降低</li><li>Transform是一种自编码（Auto-Encoding）模型，能够同时利用上下文</li></ul><h2>整体结构</h2><p>Transfromer的整体结构是一个Encoder-Decoder，自编码模型主要应用于语意理解，对于生成任务还是自回归模型更有优势</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210605151335569.png" alt="image-20210605151335569"/></p><p>我们可以将其分为四个部分：输入，编码块，解码块与输出</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/photo_2021-06-05_15-27-55.jpg"/></p><p>接下来让我们按照顺序来了解整个结构，希望在阅读下文前你可以仔细观察这幅图，阅读时也请参考该图</p><h3>输入</h3><p>使用<code>nn.Embedding</code>进行Word Embedding，论文中嵌入维度$d_{model}=512$</p><p>在嵌入时，左右两部分的权重会共享</p><p>得到词嵌入向量后需要乘以$\sqrt{d_{model}}$，其原因可能是为了相对减小位置编码的影响</p><p>同时会将上一层的输出加入进来，网络的第一层则会直接使用Inputs充当“上一层”</p><p>在输入之后会进行<strong>位置编码</strong>，使得Transformer拥有捕捉序列顺序的能力</p><h3>Encoder-Decoder</h3><p>整体结构如图</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210606010226377.png" alt="image-20210606010226377"/></p><p>Encoder-Decoder的内部结构如下图：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210606010346416.png" alt="image-20210606010346416"/></p><ul><li><p><strong>Encoder</strong>：编码块是由6个完全相同的layer组成的，每个layer有两个子层</p><p>第一层包括一个$Multi-Head Self-Attention$、$Layer-Normalization$和残差连接</p><p>第二层包括一个二层的全连接前馈层：$FFN(x)=max(0,xW_1+b_1)W_2+b_2$,中间层的维度为2048；同样包含$Layer-Normalization$和残差连接</p></li><li><p><strong>Decoder</strong>：解码块同样由6个完全相同的layer组成，每个子层同样有残差连接和$Layer-Normalization$</p><p>额外添加了第三个子层——$Masked-Multi-Head-Attention$，这是针对于上一层输出的，将在下文详细解读</p><p>此外，还修改了子注意力子层（如上图，由原来的Self-Attention变Encoder-Decoder Attention）</p></li></ul><p><strong>Layer Normalization</strong>：NLP任务中主要使用$Layer-Norm$而不是$Batch-Norm$，因为在批次上进行归一化会混乱不同语句之间的信息，我们需要在每个语句之中进行归一化。</p><h3>输出</h3><p>对解码器的输出使用普通的线性变化与$Softmax$，作为下一层的输入</p><h2>注意力机制</h2><h3>Self-Attention</h3><p>具体内容可参考我的另一篇博客——<a href="https://asthestarsfalll.icu/2021/05/12/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html">注意力机制</a></p><h3>缩放点积注意力</h3><p>缩放点积注意力，图式如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210606005714922.png" alt="image-20210606005714922"/></p><p>其公式为
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
缩放点积指的是其中的打分函数
$$
\frac{QK^T}{\sqrt d}
$$
常见的注意力模型有加性模型和点积模型，点积模型相较于加性模型效率更高，但是当输入向量维度过高，点积模型通常有较大的方差，从而导致softmax函数<strong>梯度很小</strong>，而缩放点积模型可以很好地解决这个问题。</p><p>另为，Transformer在实现过程中使用了残差连接</p><p><strong><a href="https://www.zhihu.com/search?type=content&amp;q=Transformer%20softmax%20scaled">Softmax梯度问题</a></strong>：
$$
S<em>i=\frac{e^{x_i}}{\sum</em>{j=1}^Ne^{x_j}}
$$
我们知道，$Softmax$的作用是拉大数据之间的差距</p><p> 对于一组数据$<!-- -->[x,x,2x]<!-- -->$，让我们给其赋不同的值，来观察方差和$S_3$的变化</p><pre><code class="language-python">import numpy as np
x = np.array([np.exp([i, i, 2*i]) for i in [1, 10, 100]])
print(np.square(np.linalg.norm(x, axis=1, ord=2)))  # 方差S
print(x[:, 2]/x.sum(axis=1).T) # S3
</code></pre><p>$$
\begin{cases}
x=1\quad S=6.938\quad S_3=0.576 <!-- -->\<!-- -->
x=10 \quad S=2.253e17\quad S_3=0.999<!-- -->\<!-- -->
x=100\quad S=5.221e173\quad S_3=1.0
\end{cases}
$$</p><p>即使数据之间成比例，<strong>在数量级较大时，Softmax将几乎全部的概率分布都分配给了最大的那个数</strong></p><p>Softmax的梯度为
$$
\frac{\partial S(x)}{\partial x}=
\left<!-- -->[\begin{array}<br/>
<!-- -->{c}y_1&amp;0&amp;\cdots&amp;0<!-- -->\<!-- -->
0&amp;y_2&amp;\cdots&amp;0<!-- -->\<!-- -->
\vdots&amp;\vdots&amp;\ddots&amp;\vdots<!-- -->\<!-- -->
0&amp;0&amp;\cdots&amp;y_d
\end{array}\right]<!-- -->-
\left<!-- -->[\begin{array}
{}
y_1^2&amp;y_1y_2&amp;\cdots&amp;y_1y_d<!-- -->\<!-- -->
y_2y_1&amp;y_2^2&amp;\cdots&amp;y_2y_d<!-- -->\<!-- -->
\vdots&amp;\vdots&amp;\ddots&amp;\vdots<!-- -->\<!-- -->
y_dy_1&amp;y_dy_2&amp;\cdots&amp;y_d^2
\end{array}\right]<!-- -->
$$
当出现上述的情况时，softmax会输出一个近似one-hot的向量$<!-- -->[1,0,0,\cdots,0]<!-- -->$,此时梯度为
$$
\frac{\partial S(x)}{\partial x}=
\left<!-- -->[\begin{array}<br/>
<!-- -->{c}1&amp;0&amp;\cdots&amp;0<!-- -->\<!-- -->
0&amp;0&amp;\cdots&amp;0<!-- -->\<!-- -->
\vdots&amp;\vdots&amp;\ddots&amp;\vdots<!-- -->\<!-- -->
0&amp;0&amp;\cdots&amp;0
\end{array}\right]<!-- -->-
\left<!-- -->[\begin{array}
{}
1^2&amp;0&amp;\cdots&amp;0<!-- -->\<!-- -->
0&amp;0&amp;\cdots&amp;0<!-- -->\<!-- -->
\vdots&amp;\vdots&amp;\ddots&amp;\vdots<!-- -->\<!-- -->
0&amp;0&amp;\cdots&amp;0
\end{array}\right]<!-- -->=0
$$
<strong>缩放点积为什么有效？</strong></p><p>在论文的注脚中给出了如下假设：</p><p><strong>假设向量 $Q$和$K$ 的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积$QK$的均值是0，方差是 $d_k$</strong></p><p>具体推理过程可参考我的另一片博客<a href="https://asthestarsfalll.icu/2021/05/07/%E6%A6%82%E7%8E%87%E8%AE%BA/index.html">概率论</a>2.3.5和2.3.6节</p><p>我们在高二就学过方差的一个基本性质，对于随机变量$Y=aX+b$
$$
\sigma_Y^2=a^2\sigma_X^2
$$
所以除以$\sqrt{d_k}$可以将方差控制为1，从而有效地解决梯度消失的情况</p><h3>Multi-Head Attention</h3><p>多头注意力，图式如下</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210606012114964.png" alt="image-20210606012114964"/></p><p>相比于使用$d_{model}$维数（此处为512维）的$Q、K、V$来执行一个$Attention$，使用不同的线性映射得到多个$Q、K、V$来并行得执行$Attention$效果更佳，原因如下：</p><ul><li>其增强了模型专注于不同信息的能力</li><li>为注意力层提供了多个“表示子空间”</li></ul><p><strong>具体操作</strong>：</p><p>对于每一个头，我们使用一套单独的权重矩阵$W<em>Q、W_K、W_V$，并且将其维度降至$d</em>{model}/H$</p><p>生成H个不同的注意力矩阵，将其拼接在一起</p><p>最后使用一个单独的权重矩阵$W^O$得到最终的注意力权重
$$
MutiHead(Q,K,V)=Concat(head_1,head_2,\cdots,head_h)W^O<!-- -->\<!-- -->where\quad head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)
$$</p><p>由于维度做了缩放，多头注意力的总代价和仅使用一个注意力的代价相近</p><p><strong>与卷积的关系</strong>：</p><p>我们可以发现，多头注意力实际上与卷积有着异曲同工之妙</p><p>正如多个头可以注意不同的信息，不同的卷积核可以提取图像中不同的特征</p><p>同样，正如特征图多个通道内的信息冗余，多头注意力也存在着<strong>信息冗余</strong></p><h2>位置编码</h2><p><a href="https://blog.csdn.net/muyuu/article/details/110925334?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-.first_rank_v2_pc_rank_v29">主要参考</a></p><h3>为什么需要位置编码？</h3><p>上文已经提到，Transformer是一种并行计算，为了让模型能够捕捉到序列的顺序关系，引入了位置编码，来获得单词之间的<strong>相对距离</strong>。</p><h3>正余弦位置编码</h3><p>$$
PE(pos,2i) = \sin(\frac{pos}{10000^{\frac{2i}{d<em>{model}}}})<!-- -->\<!-- -->
PE(pos,2i+1) = \cos(\frac{pos}{10000^{\frac{2i}{d</em>{model}}}})<!-- -->\<!-- -->
$$</p><p>对于奇数位置使用余弦函数进行编码</p><p>对于偶数位置使用正弦函数进行编码</p><p><strong>注意</strong>：这里的位置指的是一个词向量里数据的位置，pos指的才是单词在语句中的位置</p><p>例如某个单词在语句中的位置为Pos=5，$d_{model}=512$，则其位置编码向量为
$$
\left<!-- -->[\begin{array}{c}sin(\frac{5}{10000^{\frac{0}{512}}})<!-- -->\<!-- -->cos(\frac{5}{10000^{\frac{0}{512}}})<!-- -->\<!-- -->sin(\frac{5}{10000^{\frac{2}{512}}})<!-- -->\<!-- -->cos(\frac{5}{10000^{\frac{2}{512}}})<!-- -->\<!-- -->\vdots<!-- -->\<!-- -->sin(\frac{5}{10000^{\frac{512}{512}}})\end{array}\right]<!-- -->
$$
可以看到，$2i、2i+1$仅仅决定使用的是$sin$还是$cos$，对于同一个$i$，内部是相同的</p><p>得到位置编码之后，将其与词向量相加，作为最终的输入</p><p>这里的直觉是，将位置编码添加到词向量，它们投影到$Q/K/V$并且进行点积时，会提供有意义的距离信息</p><h3>为什么位置编码是有效的？</h3><p>我们在小学二年级就学过三角函数的诱导公式：
$$
\sin(\alpha+\beta)=sin(\alpha)cos(\beta)+cos(\alpha)sin(\beta)<!-- -->\<!-- -->
cos(\alpha+\beta)=cos(\alpha)cos(\beta)-sin(\alpha)sin(\beta)
$$</p><p>可以得到：
$$
PE(pos+k,2i)=PE(pos,2i)PE(k,2i+1)+PE(pos,2i+1)PE(k,2i)<!-- -->\<!-- -->
PE(pos+k,2i+1)=PE(pos,2i+1)PE(k,2i+1)-PE(pos,2i)PE(k,2i)
$$
我们令$u(k)=PE(k,2i)、v(k)=PE(k,2i+1)$，得：
$$
\left<!-- -->[\begin{array}
{c}PE(pos+k,2i)<!-- -->\<!-- --> PE(pos+k,2i+1)
\end{array} \right]<!-- -->=
\left<!-- -->[\begin{array}
{c}v(k)&amp;u(k)<!-- -->\<!-- -->-u(k)&amp;v(k)
\end{array} \right]<!-- -->
\left<!-- -->[\begin{array}
{c}PE(pos,2i)<!-- -->\<!-- -->PE(pos,2i+1)
\end{array}\right]<!-- -->
$$
给定相对距离$k$，$PE(pos+k)$与$PE(pos)$之间具有<strong>线性关系</strong></p><p>因此模型可以通过<strong>绝对位置</strong>的编码来更好地捕捉单词的<strong>相对位置</strong>关系</p><h3>更多</h3><p>毫无疑问，位置编码在整个Transformer中的作用是巨大的</p><p>没有位置编码的Tranformer就是一个巨型词袋</p><p>接下来让我们看看正余弦位置编码的局限</p><h4>相对距离的方向性</h4><p>我们知道，<strong>点积</strong>可以表示相对距离，注意力机制中就使用点积作为打分函数来获取$Q、K$的相似度，让我们看看两个相对距离为k的位置编码的距离</p><p>对于$PE<em>{pos}$，令$c_i=\frac{1}{10000^{\frac{2i}{d}}}$：
$$
\begin{align}
PE</em>{pos}&amp;=
\left<!-- -->[\begin{array}
{c}PE(pos,0)<!-- -->\<!-- -->PE(pos,1)<!-- -->\<!-- -->PE(pos,2)<!-- -->\<!-- --> \vdots<!-- -->\<!-- -->PE(pos,d)
\end{array}\right]<!-- -->\<!-- -->
&amp;=
\left<!-- -->[\begin{array}
{c}sin(c<em>0pos)<!-- -->\<!-- -->cos(c_0pos)<!-- -->\<!-- -->sin(c_1pos)<!-- -->\<!-- -->\vdots<!-- -->\<!-- -->cos(c</em>{\frac{d}{2}-1}pos)
\end{array}\right]<!-- -->
\end{align}
$$
内积可得：
$$
\begin{align}PE<em>{pos}^TPE</em>{pos+k}&amp;=
\sum<em>{i=0}^{\frac{d}{2}-1}{sin(c_ipos)sin(c_i(pos+k))+cos(c_ipos)cos(c_i(pos+k))}<!-- -->\<!-- -->
&amp;=\sum</em>{i=0}^{\frac{d}{2}-1}
{cos(c<em>i(pos+k-pos))}<!-- -->\<!-- -->
&amp;=\sum</em>{i=0}^{\frac{d}{2}-1}cos(c_ik)
\end{align}
$$
而余弦函数是一个<strong>偶函数</strong>，因此正余弦位置编码仅能捕捉到两单词之间的距离关系，而无法判断其距离关系</p><h4>自注意力对位置编码的影响</h4><p>在Transfromer中，位置编码之后会进行自注意力的计算，公式如下：
$$
score(x_i)=\frac{(x_iW_Q)(x_iW_K)^T}{\sqrt{d}}=\frac{((x_i^{position}+x_i^{word})W_Q)((x_i^{position}+x_i^{word})W_K)^T}{\sqrt{d}}
$$</p><p>可以看到，经过自注意力的计算，模型实际上是无法保留单词之间的位置信息</p><p>那么Transformer是如何work的呢？</p><p><strong>题外话</strong>：在bert中直接使用了Learned Position Embedding而非Sinusoidal position encoding</p><h2>解码块</h2><h3>Masked Multi-Head Attention</h3><p>在机器翻译中，解码过程是一个顺序操作的过程，也就是当解码第$k$个特征向量时，我们只能看到第$k-1$及其之前的解码结果，因此使用了添加了Mask，将当前词之后的词全都盖住</p><h3>Endcoder-Decoder Attention</h3><p>设计了一种解码块与编码块的交互模式</p><p>解码块最终的输入会生成不同的$K，V$（在代码中是这样体现的），输入给所有的解码器，而$Q$则是来自于Masked Multi-Head Attention</p><h2>代码讲解</h2><h3>位置编码</h3><pre><code class="language-python">class PositionalEncoding(nn.Module):

    def __init__(self, d_hid, n_position=200):
        super(PositionalEncoding, self).__init__()

        # Not a parameter
        # 在内存中定义一个常量，同时，模型保存和加载的时候可以写入和读出。
        self.register_buffer(
            &#x27;pos_table&#x27;, self._get_sinusoid_encoding_table(n_position, d_hid))

    def _get_sinusoid_encoding_table(self, n_position, d_hid):
        &#x27;&#x27;&#x27; Sinusoid position encoding table &#x27;&#x27;&#x27;
        # TODO: make it with torch instead of numpy
        # 某个词向量的“角度”
        def get_position_angle_vec(position):
            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]
        # 所有词向量的“角度”
        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])
        # 获得位置编码
        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i
        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1

        return torch.FloatTensor(sinusoid_table).unsqueeze(0) # 去除第一个维度
    
    def forward(self, x):
        return x + self.pos_table[:, :x.size(1)].clone().detach() # 与词向量相加
</code></pre><h3>前馈层</h3><pre><code class="language-python">class PositionwiseFeedForward(nn.Module):
    &#x27;&#x27;&#x27; A two-feed-forward-layer module &#x27;&#x27;&#x27;

    def __init__(self, d_in, d_hid=2048, dropout=0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_in, d_hid) # position-wise
        self.w_2 = nn.Linear(d_hid, d_in) # position-wise
        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):

        residual = x

        x = self.w_2(torch.relu(self.w_1(x)))
        x = self.dropout(x)
        x += residual

        x = self.layer_norm(x)

        return x
</code></pre><h3>缩放点积和多头自注意力</h3><pre><code class="language-python">class ScaledDotProductAttention(nn.Module):
    &#x27;&#x27;&#x27; Scaled Dot-Product Attention &#x27;&#x27;&#x27;

    def __init__(self, temperature, attn_dropout=0.1):
        super().__init__()
        self.temperature = temperature # 根号d_k,用来放缩
        self.dropout = nn.Dropout(attn_dropout)

    def forward(self, q, k, v, mask=None):

        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))

        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9) #让mask中是0的部分变为-1e9

        attn = self.dropout(F.softmax(attn, dim=-1))
        output = torch.matmul(attn, v)

        return output, attn
    
    
class MultiHeadAttention(nn.Module):
    &#x27;&#x27;&#x27; Multi-Head Attention module &#x27;&#x27;&#x27;

    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1): # d_model=512
        super().__init__()

        self.n_head = n_head #头数
        self.d_k = d_k # 维度
        self.d_v = d_v
        
        # 直接获得所有头的权重矩阵，d_q=d_k
        # 使用全连接层初始化和训练权重矩阵
        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False) 
        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)
        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)
        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)

        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)

    def forward(self, q, k, v, mask=None):

        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head
        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)

        residual = q # 使用残差连接

        # Pass through the pre-attention projection: b x lq x (n*dv)
        # Separate different heads: b x lq x n x dv
        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)

        # Transpose for attention dot product: b x n x lq x dv
        # 将n换到第二个维度，类似于特征图当中的通道
        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)

        if mask is not None:
            mask = mask.unsqueeze(1)   # For head axis broadcasting.

        q, attn = self.attention(q, k, v, mask=mask)

        # Transpose to move the head dimension back: b x lq x n x dv
        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1) # 自行百度contiguous()
        q = self.dropout(self.fc(q)) # 相当于上面说的WO
        q += residual

        q = self.layer_norm(q)

        return q, attn
</code></pre><h3>编码器和解码器</h3><pre><code class="language-python">class EncoderLayer(nn.Module):
    &#x27;&#x27;&#x27; Compose with two layers &#x27;&#x27;&#x27;

    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.slf_attn = MultiHeadAttention(
            n_head, d_model, d_k, d_v, dropout=dropout) # 多头自注意力
        self.pos_ffn = PositionwiseFeedForward(
            d_model, d_inner, dropout=dropout) # 前馈层

    def forward(self, enc_input, slf_attn_mask=None):
        
        enc_output, enc_slf_attn = self.slf_attn(
            enc_input, enc_input, enc_input, mask=slf_attn_mask) # q attention
        enc_output = self.pos_ffn(enc_output) # 相当对两个输出同时做了softmax,attention已经做过了
        
        return enc_output, enc_slf_attn


class DecoderLayer(nn.Module):
    &#x27;&#x27;&#x27; Compose with three layers &#x27;&#x27;&#x27;

    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):
        super(DecoderLayer, self).__init__()
        self.slf_attn = MultiHeadAttention(
            n_head, d_model, d_k, d_v, dropout=dropout)
        self.enc_attn = MultiHeadAttention(
            n_head, d_model, d_k, d_v, dropout=dropout)
        self.pos_ffn = PositionwiseFeedForward(
            d_model, d_inner, dropout=dropout)

    def forward(self, dec_input, enc_output,slf_attn_mask=None, dec_enc_attn_mask=None):
        # masked-multi-head-attention
        dec_output, dec_slf_attn = self.slf_attn(
            dec_input, dec_input, dec_input, mask=slf_attn_mask) # q,attention
        # encoder-decoder-attention
        # 使用decoder的输出q生成该层的q，encoder的输出q生成该层的k,v
        dec_output, dec_enc_attn = self.enc_attn(
            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask) # mask=none
        dec_output = self.pos_ffn(dec_output)
        
        return dec_output, dec_slf_attn, dec_enc_attn
    


</code></pre><h3>编码块和解码块</h3><pre><code class="language-python">class Encoder(nn.Module):
    &#x27;&#x27;&#x27; A encoder model with self attention mechanism. &#x27;&#x27;&#x27;

    def __init__(
            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,
            d_model, d_inner, pad_idx, dropout=0.1, n_position=200, scale_emb=False):
        &#x27;&#x27;&#x27;
        n_src_vocab：词典大小
        d_word_vec：嵌入的维度
        n_layer：编码块层数
        n——head：头数
        d_k，d_v：k，q和v的维度
        d_model：词向量维度
        d_inner：前馈层中间层维度
        pad_idx：填充长度
        scale_emb：是否进行缩放
        &#x27;&#x27;&#x27;

        super().__init__()

        self.src_word_emb = nn.Embedding(
            n_src_vocab, d_word_vec, padding_idx=pad_idx) # 进行嵌入
        
        self.position_enc = PositionalEncoding(
            d_word_vec, n_position=n_position) # 位置嵌入
        
        self.dropout = nn.Dropout(p=dropout)
        
        self.layer_stack = nn.ModuleList([
            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)
            for _ in range(n_layers)]) # 定义Encoder stack
        
        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) # 层归一化
        
        self.scale_emb = scale_emb 
        self.d_model = d_model

    def forward(self, src_seq, src_mask, return_attns=False):

        enc_slf_attn_list = []

        # -- Forward
        enc_output = self.src_word_emb(src_seq) # 词嵌入
        if self.scale_emb:
            enc_output *= self.d_model ** 0.5
        enc_output = self.dropout(self.position_enc(enc_output)) # 位置编码和Drop out
        enc_output = self.layer_norm(enc_output) # 层归一化

        for enc_layer in self.layer_stack:
            enc_output, enc_slf_attn = enc_layer(
                enc_output, slf_attn_mask=src_mask)
            enc_slf_attn_list += [enc_slf_attn] if return_attns else []

        if return_attns:
            return enc_output, enc_slf_attn_list
        return enc_output,


class Decoder(nn.Module):
    &#x27;&#x27;&#x27; A decoder model with self attention mechanism. &#x27;&#x27;&#x27;

    def __init__(self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,
            d_model, d_inner, pad_idx, n_position=200, dropout=0.1, scale_emb=False):

        super().__init__()

        self.trg_word_emb = nn.Embedding(
            n_trg_vocab, d_word_vec, padding_idx=pad_idx)
        self.position_enc = PositionalEncoding(
            d_word_vec, n_position=n_position)
        self.dropout = nn.Dropout(p=dropout)
        self.layer_stack = nn.ModuleList([
            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)
            for _ in range(n_layers)])
        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)
        self.scale_emb = scale_emb
        self.d_model = d_model

    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):

        dec_slf_attn_list, dec_enc_attn_list = [], []

        # -- Forward
        dec_output = self.trg_word_emb(trg_seq)
        if self.scale_emb:
            dec_output *= self.d_model ** 0.5
        dec_output = self.dropout(self.position_enc(dec_output))
        dec_output = self.layer_norm(dec_output)

        for dec_layer in self.layer_stack:
            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(
                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)
            dec_slf_attn_list += [dec_slf_attn] if return_attns else []
            dec_enc_attn_list += [dec_enc_attn] if return_attns else []

        if return_attns:
            return dec_output, dec_slf_attn_list, dec_enc_attn_list
        return dec_output,
</code></pre><h3>组建Transformer</h3><pre><code class="language-python">class Transformer(nn.Module):
    &#x27;&#x27;&#x27; A sequence to sequence model with attention mechanism. &#x27;&#x27;&#x27;

    def __init__(
            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,
            d_word_vec=512, d_model=512, d_inner=2048,
            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,
            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True,
            scale_emb_or_prj=&#x27;prj&#x27;):

        super().__init__()

        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx

        # In section 3.4 of paper &quot;Attention Is All You Need&quot;, there is such detail:
        # &quot;In our model, we share the same weight matrix between the two
        # embedding layers and the pre-softmax linear transformation...
        # In the embedding layers, we multiply those weights by \sqrt{d_model}&quot;.
        #
        # Options here:
        #   &#x27;emb&#x27;: multiply \sqrt{d_model} to embedding output
        #   &#x27;prj&#x27;: multiply (\sqrt{d_model} ^ -1) to linear projection output
        #   &#x27;none&#x27;: no multiplication

        assert scale_emb_or_prj in [&#x27;emb&#x27;, &#x27;prj&#x27;, &#x27;none&#x27;]
        scale_emb = (scale_emb_or_prj ==
                     &#x27;emb&#x27;) if trg_emb_prj_weight_sharing else False
        self.scale_prj = (scale_emb_or_prj ==
                          &#x27;prj&#x27;) if trg_emb_prj_weight_sharing else False
        self.d_model = d_model

        self.encoder = Encoder(
            n_src_vocab=n_src_vocab, n_position=n_position,
            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,
            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,
            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)

        self.decoder = Decoder(
            n_trg_vocab=n_trg_vocab, n_position=n_position,
            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,
            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,
            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)

        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)

        for p in self.parameters(): # 初始化方法
            if p.dim() &gt; 1:
                nn.init.xavier_uniform_(p)

        assert d_model == d_word_vec, \
            &#x27;To facilitate the residual connections, \
         the dimensions of all module outputs shall be the same.&#x27;

        if trg_emb_prj_weight_sharing:
            # Share the weight between target word embedding &amp; last dense layer
            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight # 共享权重

        if emb_src_trg_weight_sharing:
            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight

    def forward(self, src_seq, trg_seq):

        src_mask = get_pad_mask(src_seq, self.src_pad_idx)
        trg_mask = get_pad_mask(
            trg_seq, self.trg_pad_idx) &amp; get_subsequent_mask(trg_seq)

        enc_output, *_ = self.encoder(src_seq, src_mask) # 只接受第一个
        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)
        seq_logit = self.trg_word_prj(dec_output)
        if self.scale_prj:
            seq_logit *= self.d_model ** -0.5

        return seq_logit.view(-1, seq_logit.size(2))
    
    
    
    
def get_pad_mask(seq, pad_idx):
    return (seq != pad_idx).unsqueeze(-2)

def get_subsequent_mask(seq):
    &#x27;&#x27;&#x27; For masking out the subsequent info. &#x27;&#x27;&#x27;
    &#x27;&#x27;&#x27;会生成一个如下的bool矩阵：
    tensor([[[ True, False, False],
             [ True,  True, False],
             [ True,  True,  True]]])
    &#x27;&#x27;&#x27;
    sz_b, len_s = seq.size()
    subsequent_mask = (1 - torch.triu(
        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()
    # 可以用subsequent_mask = torch.triu(torch.ones((1, len_s, len_s))).transpose(2,1).bool()代替
    return subsequent_mask

</code></pre><h2>附录</h2><div class="footnotes"><hr/><ol><li id="fn-1">Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2">Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3">Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems<a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4">Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neuralmachine translation in linear time<a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5">Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning<a href="#fnref-5" class="footnote-backref">↩</a></li></ol></div>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features]]></title>
        <id>RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</id>
        <link href="https://ml.akasaki.space/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/abs/2104.08569">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+G">Gang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu%2C+X">Xin Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan%2C+J">Jingru Tan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J">Jianmin Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhaoxiang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+Q">Quanquan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu%2C+X">Xiaolin Hu</a></p><p>期刊：CVPR2021</p><p>代码：<a href="https://github.com/zhanggang001/RefineMask">https://github.com/zhanggang001/RefineMask</a></p></blockquote><h2>原文摘要</h2><blockquote><p>The  two-stage  methods  for  instance  segmentation,  e.g.Mask  R-CNN,  have  achieved  excellent  performance  re-cently.  However, the segmented masks are still very coarsedue  to  the  downsampling  operations  in  both  the  featurepyramid and the instance-wise pooling process, especiallyfor large objects.  In this work, we propose a new methodcalled  RefineMask  for  high-quality  instance  segmentationof objects and scenes, which incorporates fine-grained fea-tures  during  the  instance-wise  segmenting  process  in  amulti-stage manner. Through fusing more detailed informa-tion stage by stage, RefineMask is able to refine high-qualitymasks  consistently.    RefineMask  succeeds  in  segmentinghard  cases  such  as  bent  parts  of  objects  that  are  over-smoothed by most previous methods and outputs accurateboundaries.  Without bells and whistles, RefineMask yieldssignificant gains of 2.6, 3.4, 3.8 AP over Mask R-CNN onCOCO, LVIS, and Cityscapes benchmarks respectively at asmall  amount  of  additional  computational  cost.   Further-more, our single-model result outperforms the winner of theLVIS Challenge 2020 by 1.3 points on the LVIS test-dev setand establishes a new state-of-the-art.</p></blockquote><h2>摘要</h2><p>即使如Mask R-CNN这样二阶段的实例分割网路已经有了优秀的表现，但因为在特征金字塔和实例池化过程中使用了下采样操作，使得分割掩码仍然非常粗糙，尤其是对于大型物体。</p><p>在本文中，提出了RefineMask方法，用于对象和场景的高质量实例分割，它在实分割的过程中以多阶段的方式结合了细粒度特征。通过逐步融合更细节的信息，RefineMask能够始终如一地提炼出高质量的mask。</p><h2>介绍</h2><p>Mask R-CNN丢失了高质量实例分割任务必不可少的图像细节。导致这个问题的主要有两个原因</p><p>第一，池化过程中的特征来自于特征金字塔的多个层次，高级特征通常会使得空间分辨率更加粗糙，对于高级特征，在mask的预测过程中很难保留细节信息。</p><p>第二，池化操作本身y进一步将特征的空间尺寸减小到更小的尺寸，这也会导致信息丢失。</p><p>由于语义分割不需要区分个体，所以其可以充分利用高分辨率的特征。因此使用语义分割的方法，利用高分辨率特征来生成高质量的语义，并成功分割清洗的对象边缘。 于二阶段的实例分割方法相比，这些方法对对象边界区域的预测精度更高，同时该方法没有必要使用任何实例池化操作。</p><p>本文的主要思想是通过保持当前二阶段的强大能力来区分实例分割过程中用细粒度特征补充丢失的细节来执行实例分割。</p><p><strong>RefineMask</strong>在特征金字塔中的最高分辨率特征图上<strong>构建新的语义头</strong>，以生成细粒度的语义特征这些细粒度特征用于补充实例分割过程中的细节头丢失。于Mask R-CNN不同，RefineMask在mask head中使用了<strong>多阶段的细化策略</strong>。在ROI-Align操作之后，他逐渐放大预测大小并结合细粒度特征，以减轻高质量的实例掩码预测的损失细节。同时RefineMkas使用边界感知细化策略关注边界区域，以预测更加准确的边界。通过迭代融合更细粒度的特征并明确关注边界区域，RefindMask能够持续改进更高质量的掩码。在对象边缘的难点区域，RiefneMask的表现比Mask R-CNN的分割精度高得多。</p><h2>RefineMask</h2><p><img src="./src/RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features/20210612174037image-20210612174033296.png" alt="image-20210612174033296"/></p><p>基于FPN网络，引入一个与检测头平行的<strong>mask head</strong>，以多阶段的方式进行实例分割，并添加一个<strong>语义头</strong>，生成细粒度特征，每个阶段都有一个<strong>语义融合模块(SFM)</strong>来融合从前一个阶段获得的实例特征和从语义头的输出中汇集的语义特征，同时接收实例掩码和语义掩码作为指导。还使用了<strong>边界感知细化(BAR)</strong>策略来关注边界区域，以便在后期预测更准确的边界。</p><h3>语义头(Semantic Head)</h3><p><strong>语义头</strong>是一个<strong>FCN网</strong>络(全卷积神经网络)，附着在P2（<strong>FPN网络的最高分辨率的特征图</strong>）上。由四个卷积层组成，用于提取整个图像的语义特征，以及一个二元分类器，用于预测每个像素属于前景的概率。在<strong>二元交叉熵损失</strong>(binary cross-entropy loss)的监督下，预测整个图像的高分辨率语义掩码。将<strong>细粒度特征</strong>进一步用于补充<strong>Mask head</strong>丢失的细节部分，以进行高质量的掩码预测。</p><h3>掩码头(Mask Head)</h3><p><strong>掩码头</strong>是一个全卷积实分割分支，在掩码头中，通过14x14的<strong>RoIAlign</strong>操作提取的特征首先被送入两个3x3卷积层以生成实例特征。之后，一个1x1的卷积层被用来预测<strong>实例掩码</strong>(instance mask)，但是mask的空间大小只有14x14。这个粗糙的掩码用作后期细化阶段的初始掩码。</p><h3>多阶段细化(Multi-stage refinement)</h3><p>经过上述的过程，可以得到一个粗略的实例掩码。接下来，提出了一种多阶段细化过程以迭代方式细化掩码，每个阶段的输入由四部分组成，即从其前一阶段获得的<strong>实例特征和实例掩码</strong>，从语义头的输出中汇集的<strong>语义特征和语义掩码</strong>。提出了<strong>语义融合模块(SFM)</strong>来n疾驰而过这些输入，然后将融合的特征放大到更高的空间尺寸。<strong>Mask head</strong>迭代地运行此细化程序，并输入分辨率高达112x112的高质量实例掩码。</p><p>在放大到更高的空间尺寸之前，SFM中的融合特征用1x1卷积压缩以将其通道减半。因此，尽管特征的空间尺寸ua越来越大，但引入的额外计算成本却很低。</p><h3>语义融合模块(Semantic Fusion Module)</h3><p>为了更好的融合细粒度特征，设计了一个简单的融合模块，称为<strong>语义融合模块（SFM）</strong>，以确保掩码头部中的每个神经元感知其周围的上下文。如下图所示，他连接了每个阶段的<strong>四个输入部分</strong>，遵循<strong>1x1卷积层</strong>来融合这些特征并减少通道维度，之后，<strong>三个平行的具有不同空洞(dilations)的3x3卷积</strong>层用于融合单个神经元周围的信息，同时保留局部细节。最后<strong>实例掩码和语义掩码再次与融合特征连接起来</strong>，作为后续预测的指导。</p><p><img src="./src/RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features/20210613142007image-20210613142005975.png" alt="image-20210613142005975"/></p><h3>边界感知细化(Boundary-Aware Refinement)</h3><h4><strong>边界区域的定义</strong>：</h4><p>令$M^k$表示第k阶段二元实例分割的掩码，掩码空间大小可以表示为：$14 \cdot2^k\times14\cdot2^k$,其中$k = 1,2,3$。$M^k$的边界区域被定义为由于mask轮廓的距离小于$\hat d$的像素组成。引入一个二值掩码$B^k$来表示$M^k$的边界区域，$B^k$可以用下面的公式表示：
$$
B^{k}(i, j)=\left<!-- -->{<!-- -->\begin{array}{ll}
1, &amp; \text { if } d<em>{i j} \leq \hat{d} <!-- -->\<!-- -->
0, &amp; \text { otherwise }
\end{array}\right.
$$
其中(i,j)表示$M^k$的像素点$p</em>{ij}$的位置，$d<em>{ij}$表示$p</em>{ij}$到mask的轮廓上最近的像素的欧几里得距离。</p><p>边界如下图所示</p><p><img src="./src/RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features/20210613143808image-20210613143807751.png" alt="image-20210613143807751"/></p><p>为了有效地实现这个模块，设计了一个卷积算子来近似边界区域的计算。由于对象具有不同的比例，首先将实例掩码调整为固定大小，如第1阶段28x28和第二阶段56x56，然后再计算边界掩码。</p><h4><strong>训练过程</strong>：</h4><p>第一阶段大小为28x28的完整实例掩码,在输出大小分别为56x56和112x112的两个后续阶段中，仅使用监督信号训练某些边界区域，这些区域由ground-truth mask和其前一阶段的预测掩码决定。
$$
R^{k}=f<em>{\text {up }}\left(B</em>{G}^{k-1} \vee B<em>{P}^{k-1}\right)
$$
其中：$f</em>{up}$表示比例因子为2的上采样操作，$B^{k-1}<em>G$表示ground-truth在k-1阶段的边界区域，$B^{k-1}_P$表示预测掩码在k-1阶段的边界区域，$\vee$表示两个边界区域的并集。输出大小为$S_k\times S_k$的第k阶段(k=2,3)的损失函数$L_k$可定义为如下公式：
$$
\mathcal{L}^{k}=\frac{1}{\delta</em>{n}} \sum<em>{n=0}^{N-1} \sum</em>{i=0}^{S<em>{k}-1} \sum</em>{j=0}^{S<em>{k}-1} R</em>{n i j}^{k} \cdot l<em>{n i j}<!-- -->\<!-- -->
\delta</em>{n}=\sum<em>{n=0}^{N-1} \sum</em>{i=0}^{S<em>{k}-1} \sum</em>{j=0}^{S<em>{k}-1} R</em>{n i j}
$$
其中：N表示实例的数量，$l_{nij}$表示第n个实例位置为(i,j)处的二元交叉熵损失</p><h4><strong>推理过程</strong>：</h4><p>对于每个实例，第一阶段输出一个大小为28x28的粗略完整掩码$M^1$，同时生成其边界掩码$B<em>p^1$生成更精细和完整的实例掩码$M^{\prime K}$(第k阶段的最终输出)的规则，可以用下面的公式进行表示：
$$
\begin{array}{c}
M^{\prime 1}=M^{1} <!-- -->\<!-- -->
M^{\prime k}=f</em>{\text {up }}\left(B<em>{P}^{k-1}\right) \otimes M^{k}+\left(1-f</em>{\text {up }}\left(B<em>{P}^{k-1}\right)\right) \otimes f</em>{\text {up }}\left(M^{\prime k-1}\right)
\end{array}
$$
其中：$\otimes$表示逐像素乘法，下图展示了第二阶段的推理过程。通过重复这个过程直到得到最好的mask</p><p><img src="./src/RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features/20210613145811image-20210613145810117.png" alt="image-20210613145810117"/></p><p>$M^{\prime1},M^{\prime2}$是BAR模块的输入，$B_p^1$是$M^{\prime1}$的边界区域，$M^{\prime2}$是i第二阶段的输入掩码。带有$\times$和$+$的两个运算符分别表示逐像素乘法和逐像素加法。</p>]]></content>
        <author>
            <name>Zerorains</name>
            <uri>https://github.com/zeroRains</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLADNet - Low-Light Enhancement Network with Global Awareness]]></title>
        <id>GLADNet - Low-Light Enhancement Network with Global Awareness</id>
        <link href="https://ml.akasaki.space/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称: Low-Light Enhancement Network with Global Awareness]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称: <a href="https://ieeexplore.ieee.org/document/8373911">Low-Light Enhancement Network with Global Awareness</a></p><p>论文作者: Wenjing Wang, Chen Wei, Wenhan Yang, Jiaying Liu</p><p>Code: <a href="https://github.com/weichen582/GLADNet">https://github.com/weichen582/GLADNet</a></p></blockquote><p>这是一篇讲解使用神经网络进行低照度增强的论文。</p><ul><li>先对图像的光照进行估计，根据估计的结果来调整原图像</li><li>调整过程中会对图像中的细节重构，以便得到更加自然的结果。</li></ul><h2>Abstract (摘要)</h2><blockquote><p>In this paper, we address the problem of lowlight enhancement. Our key idea is to first calculate a global illumination estimation for the low-light input, then adjust the illumination under the guidance of the estimation and supplement the details using a concatenation with the original input. Considering that, we propose a GLobal illuminationAware and Detail-preserving Network (GLADNet). The input image is rescaled to a certain size and then put into an encoder-decoder network to generate global priori knowledge of the illumination. Based on the global prior and the original input image, a convolutional network is employed for detail reconstruction. For training GLADNet, we use a synthetic dataset generated from RAW images. Extensive experiments demonstrate the superiority of our method over other compared methods on the real low-light images captured in various conditions.</p></blockquote><p>本文主要解决了低照度增强的问题，<strong>关键的思想是输入一张低照度图像进行全局光照估计，然后在估计所得的指导下对亮度进行调整，并于原始图像连接来补充细节。</strong> 提出了GladNet，输入图像resize成一定的大小，放入Encoder-Decoder网络中，以生成的光照作为先验基础。<strong>将先验结果与原图输入卷积神经网络进行细节重构。</strong></p><h2>Introduction</h2><p>照明不足会严重影响图像质量，生活场景中会经常出现这样的问题。拍摄环境差，摄影设备性能有限，摄影师操作不当都有可能是图像质量差的原因。主要是因为图像的饱和度和对比度不足。</p><p>一般常用的低照度增强方式有直方图均衡化(HE)，基于视网膜模型的多尺度颜色恢复（视网膜模型：是图像被认为是反射和照明两个部分的组合），将人类视觉的机制引入到图像处理当中。</p><p>在低级的计算机视觉人物中，有人尝试使用叠加稀疏去噪自动编码器学习的变体，从合成黑暗和添加噪声的训练例子中同时进行微光增强和噪声降低。（LLNet）</p><p>本文提出了一种全局光照感知和细节保持网络，主要分为两个步骤。</p><ul><li>第一步是为了获得全局光照估计，将图片进行下采样到固定的大小，通过Encoder-Decoder网络。编码器-解码器的瓶颈层有一个覆盖整个图像的感受野。</li><li>第二步细节重构，主要是为了恢复和补充图像缩放过程种丢失的细节。</li><li>为了训练这样的网络，我们从在各种条件下捕获的原始图片合成训练数据集，并使用L1范数作为损失函数。GLADNet的效果是用其他最先进的方法在真实图像上评估的</li></ul><h2>Proposed method</h2><p>网络结构如下图所示。</p><p><img src="./src/GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness/image-20210630215301122.png" alt="image-20210630215301122"/></p><ul><li><p>输入被下采样到固定大小。然后，通过编码器-解码器网络传递特征图(产生一个固定大小的照度估计)。在瓶颈层，估计全局照度。缩回到原始尺寸之后，获得整个图像的照明预测。</p></li><li><p>全局照明估计步骤之后是细节重建步骤。三个卷积层参考全局级照度预测来调整输入图像的照度，并同时填充在下采样和上采样过程中丢失的细节。</p></li></ul><h3>全局光照估计</h3><p>这一步骤主要包含三个步骤</p><ul><li>将输入图像缩放到特定分辨率。</li><li>通过Encoder-Decoder网络进行全局光照预测</li><li>缩放到原来的分辨率</li></ul><p>使用最近邻插值法下采样到特定的大小$W_0\times H_0$.通过有ReLU模块的卷积分层，然后将这个特征图输入到一系列的级联下采样模块，下采样模块的数量根据设定的$W_0\times H_0$来决定，使得ED网络的Bottleneck的感受野能够覆盖整个图像。经过一系列对称的上采样块，得到用于光照预测的特征图。通过另一个上采样块，特征映射被重新缩放到原始输入的大小。</p><p>在图像经过ED网络的时候，对应上采样和下采样对应两层之间会做一次跳跃连接，这样能够将下采样的特征与上采样的特征进行映射相加，这个是为了让网络学习残差，而不是直接预测实际的像素值。</p><p>下采样模块是由一系列步长为2的卷积层和一个ReLU激活函数组成。在上采样块中与下采样模块类似，但上采样模块是用resize卷积层和来代替普通反卷积层，resize卷积层能够避免棋盘图案的伪影，且对输入的大小没有限制。Resize卷积层是由一个最近邻插值、步长为2的卷积层和一个ReLU激活函数组成。</p><h3>细节重构</h3><ul><li>从全局进行光照估计(但是，由于重新缩放过程，细节会丢失)</li></ul><p>原始输入被认为比编码器-解码器网络的输出包含更多的细节，因此可以为细节恢复提供信息。使用拼接代替跳跃连接来组合最后一个上采样块和输入图像的特征映射，使得原始信息和照度估计都可以被完全保留并传输到下一步。级联层后面是三个带有ReLU的卷积层。它将输入图像信息与估计的全局光照信息组合在一起，最终生成具有更好细节的增强结果。</p><h3>损失函数</h3><p>训练过程是通过最小化恢复图像和相应的地面真实图像之间的损失来实现的。损失函数公式如下：
$$
Loss(X,Y)=\frac{1}{N}\sum^N_{i=1}||F(X_i,\Theta)-Y_i||_1
$$
我们使用的是$L_1$范数作为损失函数，使用$L_2$番薯能够更好的去除噪声和抖动在增强结果上的影响。</p><p>此外，红色、绿色和蓝色通道在损失函数中有自己的权重:(0.29891、0.58661、0.11448)，这对于从RGB图像到灰度图像的转换是相同的权重。这有助于保持色彩平衡，提高网络的鲁棒性。</p><h2>Experiments</h2><p>本文训练的时候所有的卷积层都是采用$3\times3$，设定的$W_0\times H_0$大小为$96\times96$,ED网络中使用的卷积核大小也为$3\times3$,使用这个大小的目的是为了能够让感受野覆盖整个图像(因为需要对整个图片上的光照进行增强，如果感受野不能覆盖整个图片那么可能会导致只有局部进行了增强。)</p><p>使用GoogleCloud的视觉API来评估图像</p>]]></content>
        <author>
            <name>PommesPeter</name>
            <uri>https://blog.pommespeter.com/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Squeeze-and-Excitation Networks]]></title>
        <id>Squeeze-and-Excitation Networks</id>
        <link href="https://ml.akasaki.space/blog/[23]Squeeze-and-Excitation-Networks"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Squeeze-and-Excitation Networks（SENet）是由自动驾驶公司Momenta在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提升准确率。这个结构是2017 ILSVR竞赛的冠军，top5的错误率达到了2.251%，比2016年的第一名还要低25%。]]></summary>
        <content type="html"><![CDATA[<p>Squeeze-and-Excitation Networks（SENet）是由自动驾驶公司Momenta在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提升准确率。这个结构是2017 ILSVR竞赛的冠军，top5的错误率达到了2.251%，比2016年的第一名还要低25%。</p><blockquote><p>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the &quot;Squeeze-and-Excitation&quot; (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at <a href="https://github.com/hujie-frank/SENet">this https URL</a>.</p></blockquote><p><img src="./src/Squeeze-and-Excitation-Networks/image-20210703161305168.png" alt="image-20210703161305168"/></p><p>SENet的主要创新是一个模块。如上图，Ftr是传统卷积结构，其输入$X$($C&#x27;\times W&#x27; \times H&#x27;$)和输出$U$($C\times W \times H$)也都是传统结构中已经存在的。SeNet的模块是$U$之后的部分。SENet通过这种设在某种程度上引入了注意力。</p><hr/><h2>SE-Block（SE模块）</h2><p><img src="./src/Squeeze-and-Excitation-Networks/image-20210703163740490.png" alt="image-20210703163740490"/></p><p>SE模块是这篇论文的主要创新点。该模块的运作方法并不复杂。其步骤如下：</p><ol><li>接收$U$作为输入，对$U$做全局平均池化（Global Average  Pooling，原论文中称之为<strong>Squeeze过程</strong>，图中标记为$F_{sq}(・)$），得到一个$1\times 1\times C$的特征向量。</li><li>将得到的长条状特征向量输入一个两层的全连接层（原文中称为<strong>Excitation过程</strong>，图中标记为$F_{ex}(・,W)$），得到一个大小不变的特征向量，并使用$sigmoid$限制到$<!-- -->[0,1]<!-- -->$的范围内（原论文称之为self-gating mechanism）。</li><li>对于得到的特征向量，将其与最初输入$U$的各个$Channel$一一对应，并将其直接作为scale乘进$U$中（图中标记为$F_{scale}(・,・)$）。</li></ol><p>这样做的理论依据是为了对channel-wise进行重要性采样。或者换个说法，就是把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。</p><p><img src="./src/Squeeze-and-Excitation-Networks/image-20210703173424854.png" alt="image-20210703173424854"/></p><p>上图是对SEBlock处理过程的描述。</p><hr/><h2>Squeeze过程的设计细节</h2><p><img src="./src/Squeeze-and-Excitation-Networks/image-20210703161305168.png" alt="image-20210703161305168"/></p><p>图中的$F<em>{sq}(・)$称为<strong>Squeeze过程</strong>，全局平均池化(Global average pooling，GAP)有很多算法，作者用了最简单的求平均的方法：
$$
z_c = F</em>{sq}(u<em>c) = \frac{1}{W\times H}\sum</em>{i=1}^W\sum_{j=1}^H{u_c(i,j)}
$$
这使空间上所有点的信息都平均成了一个值。这么做是因为最终的scale是对整个通道作用的，这就得基于通道的整体信息来计算scale。另外作者要利用的是通道间的相关性，而不是空间分布中的相关性，用GAP屏蔽掉空间上的分布信息能让scale的计算更加准确。</p><h2>Excitation过程的设计细节</h2><p>Excitation部分的设计是为了利用<strong>通道相关性</strong>，详见下文中通道相关性部分。</p><p><img src="./src/Squeeze-and-Excitation-Networks/image-20210703161305168.png" alt="image-20210703161305168"/></p><p>图中的$F_{ex}(・,W)$称为<strong>Excitation过程</strong>，在SEBlock中这部分是用2个全连接来实现的。众所周知，全连接的计算量是非常恐怖的。层间连接数量较大时。所以，在SENet中第一个全连接把$C$个通道压缩成了$C/r$个通道来降低计算量（后面跟了$relu$），第二个全连接再恢复回$C$个通道（后面跟了$sigmoid$）。原论文中尝试了$r$在各种取值下的性能 ，最后得出结论$r=16$时整体性能和计算量较为平衡。</p><h2>通道相关性</h2><p>上述的Excitation过程是SENet能够利用通道相关性的关键设计。这种设计训练了一个全连接网络，该全连接网络在整个数据集上得出一个合适的$scale$，作用于前序网络的输出。这种设计是必要的：</p><p><img src="./src/Squeeze-and-Excitation-Networks/image-20210703225313459.png" alt="image-20210703225313459"/></p><p>例如，在上图存在过程1和过程2，过程1不经过全连接直接将$scale$作用于前序网络的输出。没有了全连接层，某个通道的调整值完全基于单个通道全局平均池化(Global average pooling，GAP)的结果，事实上只有GAP的分支是完全没有反向计算、没有训练的过程的，就无法基于全部数据集来训练得出通道增强、减弱的规律。</p><p>而过程2是SENet的做法。为什么要加全连接层呢？这里的全连接层旨在更好地捕获通道间的相关关系。为了实现这个目标，函数必须满足两个标准：</p><ol><li>它必须能够学习通道间的非线性关系。通道之间会存在关系。</li><li>它必须能够学习一种非互斥的关系。因为有用的也许是多个通道。</li></ol><p>为了满足这些标准，SENet选用带有$Sigmoid$激活的简单门控机制，利用通道间的相关性训练出真正的$scale$。一次mini-batch个样本的$squeeze$输出并不代表通道真实要调整的$scale$值，真实的scale要基于全部数据集来训练得出，而不是基于单个batch。</p><p>还有一个关键问题是：<strong>实际上，单纯通过卷积训练出的权值也有类似$scale$的成分，为什么要加入这个通道相关性的设计呢</strong>？SENet这样设计是为了<strong>排除空间上的干扰</strong>。在经过GAP后，某一个channel上具有的空间信息被压缩成一个点，从而失去了空间相关性，也就排除了空间上的干扰。</p><h2>Summary</h2><p>SENet的设计兼顾了空间相关性（通过1/8下采样的卷积）和通道相关性（通过Excitation的设计）。这种设计把重要通道的特征强化，非重要通道的特征弱化，得到了很好的效果，这是一种全新的思路，在这个方向上将来可能会有更多的成果。这种设计在当时产生了非常大的进步，证明了通道相关性的重要性。</p><p>这种设计也能很轻易地运用到其他网络中：</p><p><img src="./src/Squeeze-and-Excitation-Networks/image-20210703230727261.png" alt="image-20210703230727261"/></p><p>例如，在上图中，左侧是SE-Inception的结构，即Inception模块和SENet组和在一起；右侧是SE-ResNet，ResNet和SENet的组合，这种结构scale放到了残差的直连相加之前。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation]]></title>
        <id>BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[BiSeNet的目标是更快速的实时语义分割。在语义分割任务中，空间分辨率和感受野很难两全，尤其是在实时语义分割的情况下，现有方法通常是利用小的输入图像或者轻量主干模型实现加速。但是小图像相较于原图像缺失了很多空间信息，而轻量级模型则由于裁剪通道而损害了空间信息。BiSegNet整合了Spatial Path (SP) 和 Context Path (CP)分别用来解决空间信息缺失和感受野缩小的问题。]]></summary>
        <content type="html"><![CDATA[<p>BiSeNet的目标是更快速的实时语义分割。在语义分割任务中，空间分辨率和感受野很难两全，尤其是在实时语义分割的情况下，现有方法通常是利用小的输入图像或者轻量主干模型实现加速。但是小图像相较于原图像缺失了很多空间信息，而轻量级模型则由于裁剪通道而损害了空间信息。BiSegNet整合了Spatial Path (SP) 和 Context Path (CP)分别用来解决空间信息缺失和感受野缩小的问题。</p><blockquote><p>Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.</p></blockquote><p>论文原文：<a href="https://arxiv.org/abs/1808.00897">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>。阅读后你会发现，这篇论文有很多思路受到<a href="./%5B23%5DSqueeze-and-Excitation-Networks">SENet（Squeeze-and-Excitation Networks）</a>的启发。</p><hr/><h2>设计目的和思路</h2><p>在以往的工作中，为了对网络进行加速以达到实时的目的，研究者们往往会选择折中精度以求速度：</p><ol><li>通过剪裁或 resize 来限定输入大小，以降低计算复杂度。尽管这种方法简单而有效，空间细节的损失还是让预测打了折扣，尤其是边界部分，导致度量和可视化的精度下降；</li><li>通过减少网络通道数量加快处理速度，尤其是在骨干模型的早期阶段，但是这会弱化空间信息。</li><li>为追求极其紧凑的框架而丢弃模型的最后阶段（比如ENet）。该方法的缺点也很明显：由于 ENet 抛弃了最后阶段的下采样，模型的感受野不足以涵盖大物体，导致判别能力较差。</li></ol><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704101433666.png" alt="image-20210704101433666"/></p><p>上图中左侧是剪裁和resize方法的示意，右侧是跑去部分结构或减少通道的示意。为解决上述空间信息缺失问题，研究者普遍采用 U 形结构。通过融合 backbone 网络不同层级的特征，U 形结构逐渐增加了空间分辨率，并填补了一些遗失的细节。</p><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704102757859.png" alt="image-20210704102757859"/></p><p>上图是一种典型的U型结构。但是，这一技术有两个弱点：</p><ol><li>由于高分辨率特征图上额外计算量的引入，完整的 U 形结构拖慢了模型的速度。</li><li>绝大多数由于裁剪输入或者减少网络通道而丢失的空间信息无法通过引入浅层而轻易复原。换言之，U 形结构顶多是一个备选方法，而不是最终的解决方案。</li></ol><p>基于上述观察，本文提出了双向分割网络BiSeNet（Bilateral Segmentation Network），其主要的改进有：</p><ul><li>同时使用Spatial Path (SP) 和 Context Path (CP)，兼顾空间属性和感受野</li><li>提出特征融合模块（Feature Fusion Module/FFM）用于更好地融合SP和CP的特征</li><li>提出注意力优化模块（Attention Refinement Module/ARM）</li></ul><p>下图为BiSeNet的结构示意图：</p><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704102830425.png" alt="image-20210704102830425"/></p><p>它包含两个部分：Spatial Path (SP) 和 Context Path (CP)。顾名思义，这两个组件分别用来解决空间信息缺失和感受野缩小的问题。对于 Spatial Path，论文中只叠加三个卷积层以获得 1/8 特征图，其保留着丰富的空间细节。对于 Context Path，本文在<a href="//todo">Xception</a>尾部附加一个全局平均池化层，其中感受野是 backbone 网络的最大值。</p><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704103510162.png" alt="image-20210704103510162"/></p><p>上图是以上三种思路放在一起的对比图。在追求更快、更好模型的过程中，论文也研究了两个组件的融合，以及最后预测的优化，并分别提出特征融合模块FFM（Feature Fusion Module）和注意力优化模块ARM（Attention Refinement Module），这两个模块进一步从整体上提升了语义分割的精度。</p><hr/><h2>网络结构设计</h2><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704141854108.png" alt="image-20210704141854108"/></p><p>上图是BiSeNet的网络结构。可以看到其重要组成部分Spatial Path、Context Path以及两个优化模块Attention Refinement（原图中打错了单词）、Feature Fusion Module。</p><h3>Spatial Path</h3><p>在语义分割任务中，空间分辨率和感受野很难两全，尤其是在实时语义分割的情况下，现有方法通常是利用小的输入图像或者轻量主干模型实现加速。但是小图像相较于原图像缺失了很多空间信息，而轻量级模型则由于裁剪通道而损害了空间信息。</p><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704144527369.png" alt="image-20210704144527369"/></p><p>在原论文中，为了保持充足的空间信息，Spatial Path包含三个层，每个层由一个步长为2的卷积和一个BN层以及一个非线性的ReLU激活层构成。这样做使得Spatial Path仅对原图进行1/8下采样，保留了丰富的空间信息。</p><h3>Context Path</h3><p>在语义分割任务中，感受野对于性能表现至关重要。为增大感受野，一些方法利用金字塔池化模块，金字塔型空洞池化（ASPP）或使用&quot;large kernel&quot;，但是这些操作比较耗费计算和内存，导致速度慢，这些缺点在实时的任务上尤为突出。出于较大感受野和较高计算效率兼得的考量，本文提出 Context Path，它充分利用轻量级模型与全局平均池化以提供大感受野。</p><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704145307009.png" alt="image-20210704145307009"/></p><p>在本工作中，轻量级模型，比如 Xception，可以快速下采样特征图以获得大感受野，编码高层语义语境信息。接着，本文在轻量级模型末端添加一个全局平均池化，通过全局语境信息提供一个最大感受野。在轻量级模型中，本文借助 U 形结构融合最后两个阶段的特征，但这不是一个完整的 U 形结构。图 2(c) 全面展示了 Context Path。</p><h4>Attention Refinement Module (ARM)</h4><p>在 Context Path 中，本文提出一个独特的注意力优化模块，以优化每一阶段的特征：</p><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704145528506.png" alt="image-20210704145528506"/></p><p>如上图所示，ARM 借助全局平均池化捕获全局语境并计算注意力向量以指导特征学习。这一设计可以优化 Context Path 中每一阶段的输出特征，无需任何上采样操作即可轻易整合全局语境信息，因此，其计算成本几乎可忽略。</p><h3>Feature Fusion Module (FFM)</h3><p>在特征表示的层面上，两路网络的特征并不相同。因此不能简单地加权这些特征。由 Spatial Path捕获的空间信息编码了绝大多数的丰富细节信息。而 Context Path 的输出特征主要编码语境信息。换言之，Spatial Path 的输出特征是低层级的，Context Path 的输出特征是高层级的。因此，本文提出一个独特的特征融合模块以融合这些特征。</p><p><img src="./src/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/image-20210704150819590.png" alt="image-20210704150819590"/></p><p>在特征的不同层级给定的情况下，本文首先连接 Spatial Path 和 Context Path 的输出特征；接着，通过批归一化平衡特征的尺度。下一步，像<a href="./%5B23%5DSqueeze-and-Excitation-Networks">SENet</a>一样，把相连接的特征池化为一个特征向量，并计算一个权重向量。这一权重向量可以重新加权特征，起到特征选择和结合的作用。上图展示了这一设计的细节。</p><h2>实验</h2><p>实验部分请自行阅读原论文。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking BiSeNet For Real-time Semantic Segmentation]]></title>
        <id>Rethinking BiSeNet For Real-time Semantic Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, Xiaolin Wei]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan%2C+M">Mingyuan Fan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lai%2C+S">Shenqi Lai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+J">Junshi Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+X">Xiaoming Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chai%2C+Z">Zhenhua Chai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo%2C+J">Junfeng Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+X">Xiaolin Wei</a></p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210719132305088.png" alt="image-20210719132305088"/></p><blockquote><p>BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images.</p></blockquote><p>在阅读本文前，请先阅读<a href="./%5B24%5DBiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>。</p><p>该论文提出<a href="./%5B24%5DBiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet</a>被证明是不错的双路实时分割网络。不过，在BiSeNet中：</p><ul><li>单独为空间信息开辟一条网络路径在计算上非常的耗时</li><li>用于spatial path的预训练轻量级骨干网络从其他任务中（例如分类和目标检测）直接拿来，用在分割上效率不很高。</li></ul><p>因此，作者提出Short-Term Dense Concatenate network（<strong>STDC</strong> network）来代替BiSeNet中的context path。其核心内容是移除冗余的结构，进一步加速分割。具体来说，本文将特征图的维数逐渐降低，并将特征图聚合起来进行图像表征，形成了STDC网络的基本模块。同时，在decoder中提出Detail Aggregation module将空间信息的学习以single-stream方式集成到low-level layers中，用于代替BiSeNet中的spatial path。最后，将low-level features和deep features融合以预测最终的分割结果。</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210719100139212.png" alt="image-20210719100139212"/></p><p>注：上图中红色虚线框中的部分是新提出的STDC network；ARM表示注意力优化模块（Attention Refinement Module），FFM表示特征融合模块（Feature Fusion Module）。这两个模块是在<a href="./%5B24%5DBiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>就已经存在的设计。</p><p>有兴趣请阅读原论文<a href="https://arxiv.org/abs/2104.13188">Rethinking BiSeNet For Real-time Semantic Segmentation</a>。</p><hr/><h2>Related Works</h2><h3>Efficient Network Designs</h3><p>模型设计在计算机视觉任务中起着重要作用。 SqueezeNet使用Fire Module和某些策略来减少模型参数；MobileNet V1利用深度可分离卷积来减少推理阶段的 FLOP。 ResNet采用残差构建层来实现出色的性能； MobileNet V2 和 ShuffleNet 使用Group Convolution 来降低计算成本，同时保持可比的准确性。不过上述网络是针对图像分类任务设计的。对于实时分割任务，<strong>需要新的网络结构设计或优化</strong>。</p><h3>Generic Semantic Segmentation</h3><p>传统的分割算法，例如阈值选择、超分辨，利用手工制作的特征来分配图像中的像素级标签。 随着卷积神经网络的发展，基于全卷积网络的方法在各种基准测试中取得了令人印象深刻的性能。 Deeplabv3采用了一个多尺度空间的的空间金字塔池化模块来捕获多尺度上下文。 SegNet利用编码器-解码器结构来恢复高分辨率特征图。PSPNet设计了金字塔池化来捕获膨胀主干上的局部和全局上下文信息。 使用空洞卷积的主干网络和编码器-解码器结构都可以同时学习低级细节和高级语义。 然而，由于高分辨率特征和复杂的网络连接，大多数方法需要大量的计算成本。 在本文中，作者提出了一种高效且有效的架构，可以在速度和准确性之间取得良好的平衡。</p><h3>Real-time Semantic Segmentation</h3><p>最近，实时语义分割的实际应用正在快速增长。在这种情况下，有两种主流可以设计有效的分割方法：</p><ol><li>轻量级主干。 DFANet 采用轻量级主干来降低计算成本，并设计了一个跨级特征聚合模块来提高性能。 DFNet <!-- -->[21]<!-- --> 利用“偏序剪枝”算法获得轻量级主干和高效解码器。</li><li>多分支架构。 ICNet <!-- -->[31]<!-- --> 设计了多尺度图像级联以实现良好的速度-精度权衡。 BiSeNetV1 <!-- -->[28]<!-- --> 和 BiSeNetV2 <!-- -->[27]<!-- --> 分别为低级细节和高级上下文信息提出了双流路径。</li></ol><p>在本文中，我们提出了一种高效的轻量级主干来提供可扩展的感受野。 此外，我们设置了一个单路径解码器，它使用详细信息指导来学习低级细节。</p><h2>Short-Term Dense Concatenate network（STDC network）</h2><p>BiSeNet V1 利用轻量级主干，例如 ResNet18 和spatial path作为编码网络，形成双流分割架构。</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210726213138378.png" alt="image-20210726213138378"/></p><p>作者提到，额外使用一个网络分支对low level的feature进行编码在设计上有冗余，使整个编码网络效率低下。所以，作者取消了Spatial path，使用一个Detail Guidance指导主干网络进行学习。而这个被指导的主干网络就是STDC Network。</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210720115455260.png" alt="image-20210720115455260"/></p><p>上图中(a)为这篇论文中Encoder的设计，(b)是BiSeNet中context path的设计。(a)中红色标注部分是STDC  Network的设计，其功能是代替(b)称为主干网络。</p><p>下图是STDC Network的基本构成：</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210720120046524.png" alt="image-20210720120046524"/></p><p>上图中，(a)为STDC网络，可以看到它由很多个Stage组成，每个Stage又包含若干STDC Module；(b)为STDC Module；(c)为stride = 2的STDC Module。其中，$M$表示输入的维度，$N$​​​表示输出的维度。ConvX表示Conv-BN-ReLU的连续结构，每个网络的块是由不同kernal size的ConvX。</p><p>对于STDC网络，Stage1~Stage5进行下采样和编码；Stage6用于分类。</p><p>对于每个Stage，Stage1和Stage2使用单纯的卷积进行编码，而Stage3~Stage5使用STDC Module进行编码。</p><p>对于每个STDC模块（STDC Module），其输入通道数为$M$，其输出通道数为$N$，其由若干个Block组成，第$i$​个Block的输出可以表示成：
$$
x<em>i = ConvX_i(x</em>{i-1},k<em>i)
$$
其中$X</em>{i-1}$表示$x<em>i$表示第$i$个块的输出，$x</em>{i-1}$表示第$i$个块的输入（也就是第$i-1$个块的输出），$k_i$​表示卷积层的kernal size。</p><p>STDC模块中，第一个block的kernel size为1，其余简单设置为3。可以注意到，在网络的每个阶段，特征图的channel数量以$\frac{N}{2}$、$\frac{N}{4}$、$\frac{N}{8}$逐渐下降。作者认为在图像分类任务中，随着网络层数的增加逐渐增加特征图的通道数是一种常见做法。但是在语义分割任务中，在较浅的网络层，往往需要足够多的通道数保证感知的细粒度；而在感受野较大的深层网络中，应该更加注重高层级信息的归纳，一直增加通道的数量会导致信息的冗余。我们专注于可扩展的感受野和多尺度信息。</p><p>为了将低层级的细粒度特征转递至网络末端STDC中仅有Block2中进行了下采样（stride=2）；并且STDC中存在多条skip-path。在图中最后的Fusion中，各个阶段的特征会通过concatenation进行特征拼接：
$$
X_{output} = F(x_1,x_2,.....,x_n)
$$
其中$F$是任何可以进行特征融合的方法。在拼接之前，会通过$3\times 3$平均池化下采样到相同的空间大小。</p><table><thead><tr><th>STDC module</th><th>Block1</th><th>Block2</th><th>Block3</th><th>Block4</th><th>Fusion</th></tr></thead><tbody><tr><td>RF(S = 1)</td><td>$1\times 1$</td><td>$3\times 3$</td><td>$5\times 5$</td><td>$7\times 7$</td><td>$1\times 1$、$3\times 3$、$5\times 5$、$7\times 7$</td></tr><tr><td>RF(S = 2)</td><td>$1\times 1$</td><td>$3\times 3$</td><td>$7\times 7$</td><td>$11\times 11$</td><td>$3\times 3$、$7\times 7$、$11\times 11$</td></tr></tbody></table><p>上表为STDC中各个阶段的感受野大小。$X_{output}$是多尺度融合的特征。</p><p>这种STDC的设计有如下的优点：</p><ol><li>随着网络层数加深，卷积核的shape呈几何级数下降，计算复杂度显著性降低。</li><li>最终的输出中融合了来自不同阶段Block的特征，集成了来自可变大小的特征图。</li></ol><p>如果给定了输入大小$M$和输出大小$N$，可以算出STDC的参数量是：
$$
S<em>{param} = M\times 1\times 1 \times \frac{N}{2^1} + \sum</em>{i=2}^{n-1}\frac{N}{2^{i-1}}\times 3 \times 3 \times \frac{N}{2^i}+\frac{N}{2^{n-1}}<!-- -->\<!-- -->
=\frac{NM}{2}+\frac{9N^2}{2^3}\times \sum_{i=0}^{n-3}\frac{1}{2^{2i}}+\frac{9N^2}{2^{2n-2}}<!-- -->\<!-- -->
=\frac{NM}{2}+\frac{3N^2}{2}\times (1+\frac{1}{2^{2n-3}})
$$
对于给定的$M$和$N$​，STDC的参数量几乎不受其他因素的影响。</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210726214508297.png" alt="image-20210726214508297"/></p><p>上表是STDC每个Stage设计的细节。</p><h2>Detail Ground-truth Generation</h2><p>在这篇论文中，作者认为BiSeNet中的Spatial path产生了大量的冗余计算，因此作者设计Detail Ground-truth Generation，将其作为网络学习的指导（Detail Guidance），替代Spatial path的作用。这样以来，Spatial path被移除，节约了大量的计算量。</p><p>作者将参与训练的整个网络结构分为三部分：</p><ul><li>分割网络（Network Architecture，图中蓝色背景的(a)部分）</li><li>Loss计算部分（Train Loss，图中绿色背景(b)的部分）</li><li>细节特征拾取部分（Detail Ground-truth Generation，图中绿色背景(c)部分）</li></ul><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210719100139212.png" alt="image-20210719100139212"/></p><p>在整个网络结构图中，可以看到部分网络结构（绿色背景）是Train-only的。也就是这些结构只在训练中出现，而在预测过程中不会出现。其中Detail Ground-truth Generation就是Train-only的。</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210720172550752.png" alt="image-20210720172550752"/></p><p>上图是对网络传播过程的直观理解，可以看出，Detail Ground-truth Generation是为encoder（STDC特征提取网络）服务的。其中，输入1表示原图，输入2表示分割的Ground-truth。</p><p>Detail Ground-truth Generation的设计目的是为了使用已知的图像细节（ground truth）来引导STDC网络学习过程，以使其具有与添加spatial path一样的良好效果。其具体步骤是：</p><ol><li>读取数据集的Ground-truth，在Ground-truth上使用拉普拉斯算子进行卷积，产生Detail Map。</li><li>在STDC网络的Stage3插入一个称为Detail Head的模块，Detail Map通过该模块汇入STDC网络。</li></ol><p>其中在二步的Detail Head模块设计如下图所示：</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210720194310086.png" alt="image-20210720194310086"/></p><p>Detail Head是一个Conv-BN-ReLU形的结构，后面接续一个$1\times 1$卷积。在实验中，作者证明了Detail Head的设计能够有效增强特征。</p><p>第一步用于产生Detail Map的算子是一个称为<strong>拉普拉斯（Laplacian）算子</strong>的特征提取器：
$$
\begin{matrix}
-1 &amp; -1 &amp; -1 <!-- -->\<!-- -->
-1 &amp; 8 &amp; -1 <!-- -->\<!-- -->
-1 &amp; -1 &amp; -1
\end{matrix} \tag{1}
$$
拉普拉斯（Laplace）算子作为边缘检测之一，和Sobel算子一样也是工程数学中常用的一种积分变换。拉普拉斯算子用在图像上就属于空间锐化滤波操作。</p><p>在这篇论文中，作者通过三个不同步幅的拉普拉斯算子卷积得到多尺度的detail，在再将它们上采样到原来的大小，再通过$1\times 1$卷积将它们融合（这里原文是dynamic re-wegihting）形成最终的Detail GT：</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210720172757986.png" alt="image-20210720172757986"/></p><p>拉普拉斯算子和其他的一些边缘检测算子具有区别。Prewitt和Sobel两个出名的算子都是通过求一阶导数来计算梯度的，通常用于边缘线的检测。对于细节特征，除了检测线，有时候也需要检测特殊点，著名的就是拉普拉斯（Laplacian）算子。拉普拉斯算子在边缘检测的应用中并不局限于水平方向或垂直方向，这是Laplacian与soble的区别。</p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/v2-dbe4e9f6c8049721f462c70af98fcc63_b.jpg" alt="img"/></p><p>上图为图像边缘的信号经过一阶和二阶求导后信号变化的直观表示。对图像求两次导数，公式如下：
$$
\begin{aligned}
&amp;\nabla^2f(x,y) = \frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}<!-- -->\<!-- -->
&amp;=<!-- -->{<!-- -->f(x+1,y)+f(x-1,y)-2f(x,y)<!-- -->}<!-- --> + <!-- -->{<!-- -->f(x,y+1)+f(x,y-1)-2f(x,y)<!-- -->}<!-- -->\<!-- -->
&amp;=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)
\end{aligned}
$$
下图是作者对网络各个阶段的特征的可视化。可以看出，通过这种设计，被移除Spatial path后，前置特征提取网络依然能较好地关注空间细节。 </p><p><img src="./src/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/image-20210720173553120.png" alt="image-20210720173553120"/></p><h2>Detail Loss</h2><p>由于在一幅图片中，代表细节的像素数量比较少。具体来说，对于画面中的实体，边缘的像素总比内部的像素少。所以，detail-prediction是一个类别不均衡的问题，具有权重的交叉熵（cross-entropy）总是会导致粗略的学习结果。因此，参考Learning to predict crisp boundaries（ECCV的一篇文章），这里作者使用了二元交叉熵（binary cross-entropy）+ 骰子损失（dice loss）来对学习进行优化。</p><p>骰子损失能够衡量prediction和ground-truth之间的交叠水平，并且它对前景/背景的像素不敏感——这意味着它可以缓解类不平衡问题。 因此对于高度为 $H$ 宽度为 $W$ 的预测细节图（predicted detail map），细节损失$L<em>{detail}$ 的公式如下：
$$
L</em>{detail}(p<em>d,g_d) = L</em>{dice}(p<em>d,g_d)+L</em>{bce}(p<em>d,g_d)
$$
上式中$p_d\in\R^{H\times W}$代表预测的细节图（predicted detail map），$g_d\in\R^{H\times W}$表示相应的Ground-truth。$L</em>{bce}$表示二元交叉熵损失公式，$L<em>{dice}$表示骰子损失的公式。其中骰子损失的公式如下：
$$
L</em>{dice}(p_d,g_d) = 1-\frac{2\sum_i^{H\times W}p_d^ig_d^i + \epsilon}{\sum_i^{H\times W}(p_d^i)^2+\sum_i^{H\times W}(g_d^i)^2 + \epsilon}
$$
上式中 $i$ 表示第$i$个像素，$\epsilon$表示拉普拉斯平滑项（为了避免除数是0）.在这篇论文中$\epsilon = 1$。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CBAM - Convolutional Block Attention Module]]></title>
        <id>CBAM - Convolutional Block Attention Module</id>
        <link href="https://ml.akasaki.space/blog/[26]CBAM-Convolutional-Block-Attention-Module"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[image-20210723203210974]]></summary>
        <content type="html"><![CDATA[<p><img src="./src/CBAM-Convolutional-Block-Attention-Module/image-20210723203210974.png" alt="image-20210723203210974"/></p><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Woo%2C+S">Sanghyun Woo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Park%2C+J">Jongchan Park</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+J">Joon-Young Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kweon%2C+I+S">In So Kweon</a></p><blockquote><p>We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.</p></blockquote><p>CBAM是一篇结合了通道注意力和空间注意力的论文。它通过在同个模块中叠加通道注意力和空间注意力达到了良好的效果。为了提升 CNN 模型的表现，除了对网络的深度、宽度下手，还有一个方向是注意力。注意力不仅要告诉我们重点关注哪里，提高关注点的表达。 我们的目标是通过使用注意机制来增加表现力，关注重要特征并抑制不必要的特征。</p><p>为了强调空间和通道这两个维度上的有意义特征，作者依次应用通道和空间注意模块，来分别优化卷积神经网络在通道和空间维度上学习能力。作者将注意力过程分为通道和空间两个独立的部分，这样做不仅可以节约参数和计算力，而且保证了其可以作为即插即用的模块集成到现有的网络架构中去。</p><hr/><h2>Channel Attention Module（通道注意力）</h2><p>关于通道注意力的相关设计细节请参考<a href="./%5B23%5DSqueeze-and-Excitation-Networks">Squeeze-and-Excitation Networks (SENet)</a>。在这里不进行非常详细的设计。特征的每一个通道都代表着一个专门的检测器，因此，通道注意力是关注什么样的特征是有意义的。为了汇总空间特征，作者采用了全局平均池化和最大池化两种方式来分别利用不同的信息。</p><p><img src="./src/CBAM-Convolutional-Block-Attention-Module/image-20210723194442312.png" alt="image-20210723194442312"/></p><p>在这篇论文中，作者使用Global Average Pool和Max Pool共用构造Channel wise的注意力。如上图所示，输入是一个 $H\times W\times C$的特征 $F$，我们先分别进行一个空间的全局平均池化和最大池化得到两个$1\times 1\times C$的通道描述。接着，再将它们分别送入一个两层的神经网络，第一层神经元个数为$C/r$，激活函数为 $ReLU$，第二层神经元个数为。注意，它们使用共享的参数建立通道相关性。将得到的通道描述乘以输入特征$F$即可得到加入了通道相关性的特征。</p><p>相较于原本的<a href="./%5B23%5DSqueeze-and-Excitation-Networks">SENet</a>，本文同时使用Global Average Pooling和Max Pooling。在后续的实验结果中可以看到，在通道注意力模块中，同时引入最大池化和平均池化可以得到最好的效果。</p><h2>Spatial Attention Module（空间注意力）</h2><p><img src="./src/CBAM-Convolutional-Block-Attention-Module/image-20210723194510586.png" alt="image-20210723194510586"/></p><p>与通道注意力相似，给定一个 $H\times W\times C$ 的特征 $F$，先分别进行一个通道维度的平均池化和最大池化得到两个 $H\times W\times 1$ 的通道描述，并将这两个描述按照通道拼接在一起。经过一个 $7\times 7$ 的卷积层，激活函数为 $Sigmoid$，得到权重系数 $M_s$。将权重系数和特征 $F$ 相乘即可得到加入了空间注意力的特征图。</p><p>在空间注意力模块中，同时引入最大池化和平均池化比利用一个 $1\times 1$ 的卷积要好，同时，卷积层采用 $7\times 7$ 的卷积核要优于 $3\times 3$ 的卷积核。</p><h2>Convolutional Block Attention Module</h2><p>通道注意力和空间注意力这两个模块可以以并行或者顺序的方式组合在一起，但是作者发现顺序组合并且将通道注意力放在前面可以取得更好的效果。</p><p><img src="./src/CBAM-Convolutional-Block-Attention-Module/image-20210723194528528.png" alt="image-20210723194528528"/></p><p>一个完整的 CBAM 模块如上图所示，其 TensorFlow 的一个实现如下所示：</p><pre><code class="language-python">def CBAM(input, reduction):
    &quot;&quot;&quot;
    @Convolutional Block Attention Module
    &quot;&quot;&quot;

    _, width, height, channel = input.get_shape()  # (B, W, H, C)

    # channel attention
    x_mean = tf.reduce_mean(input, axis=(1, 2), keepdims=True)   # (B, 1, 1, C)
    x_mean = tf.layers.conv2d(x_mean, channel // reduction, 1, activation=tf.nn.relu, name=&#x27;CA1&#x27;)  # (B, 1, 1, C // r)
    x_mean = tf.layers.conv2d(x_mean, channel, 1, name=&#x27;CA2&#x27;)   # (B, 1, 1, C)

    x_max = tf.reduce_max(input, axis=(1, 2), keepdims=True)  # (B, 1, 1, C)
    x_max = tf.layers.conv2d(x_max, channel // reduction, 1, activation=tf.nn.relu, name=&#x27;CA1&#x27;, reuse=True)
    # (B, 1, 1, C // r)
    x_max = tf.layers.conv2d(x_max, channel, 1, name=&#x27;CA2&#x27;, reuse=True)  # (B, 1, 1, C)

    x = tf.add(x_mean, x_max)   # (B, 1, 1, C)
    x = tf.nn.sigmoid(x)        # (B, 1, 1, C)
    x = tf.multiply(input, x)   # (B, W, H, C)

    # spatial attention
    y_mean = tf.reduce_mean(x, axis=3, keepdims=True)  # (B, W, H, 1)
    y_max = tf.reduce_max(x, axis=3, keepdims=True)  # (B, W, H, 1)
    y = tf.concat([y_mean, y_max], axis=-1)     # (B, W, H, 2)
    y = tf.layers.conv2d(y, 1, 7, padding=&#x27;same&#x27;, activation=tf.nn.sigmoid)    # (B, W, H, 1)
    y = tf.multiply(x, y)  # (B, W, H, C)

    return y
</code></pre><h2>Example</h2><p>使用该模块的例子，如将其集成于ResNet中：</p><p><img src="./src/CBAM-Convolutional-Block-Attention-Module/image-20210723203033736.png" alt="image-20210723203033736"/></p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-local Neural Networks]]></title>
        <id>Non-local Neural Networks</id>
        <link href="https://ml.akasaki.space/blog/[27]Non-local-Neural-Networks"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network.]]></summary>
        <content type="html"><![CDATA[<blockquote><p>The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network.</p></blockquote><p>Non-local旨在使用单个Layer实现长距离的像素关系构建，属于自注意力（self-attention）的一种。常见的CNN或是RNN结构基于局部区域进行操作。例如，卷积神经网络中，每次卷积试图建立一定区域内像素的关系。但这种关系的范围往往较小（由于卷积核不大）。</p><p>为了建立像素之间的长距离依赖关系，也就是图像中非相邻像素点之间的关系，本文另辟蹊径，提出利用non-local operations构建non-local神经网络。这篇论文通过非局部操作解决深度神经网络核心问题：捕捉长距离依赖关系。</p><blockquote><p>Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code is available at <a href="https://github.com/facebookresearch/video-nonlocal-net">this https URL</a> .</p></blockquote><p>本文另辟蹊径，提出利用non-local operations构建non-local神经网络，解决了长距离像素依赖关系的问题。很值得阅读<a href="https://arxiv.org/abs/1711.07971">论文原文</a>。受计算机视觉中经典的非局部均值方法启发，作者的非局部操作是将所有位置对一个位置的特征加权和作为该位置的响应值。这种非局部操作可以应用于多种计算机视觉框架中，在视频分类、目标分类、识别、分割等等任务上，都有很好的表现。</p><hr/><h2>Non-local means（非局部均值）</h2><p>非局部均值（Non-local means）起初是一种是一种<a href="https://zh.wikipedia.org/wiki/%E5%BD%B1%E5%83%8F%E9%99%8D%E5%99%AA">影像降噪</a>的算法，<strong>基本思想是：当前像素的估计值由图像中与它具有相似邻域结构的像素加权平均得到</strong>。</p><p><img src="./src/Non-local-Neural-Networks/sphx_glr_plot_nonlocal_means_001.png" alt="noisy, non-local means"/></p><p>由于<strong>一张图片中最相似的点不一定是距离近的点</strong>，反之亦然，故搜寻整张图片上相似的点，利用周期性重复出现的部分如材质纹理或是伸长的边缘等进行降噪可以得到更好的结果。相较于局部（local）的算法（如<a href="https://zh.wikipedia.org/wiki/%E9%AB%98%E6%96%AF%E6%A8%A1%E7%B3%8A">高斯模糊</a>、<a href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%AD%89%E5%90%91%E6%80%A7%E6%93%B4%E6%95%A3">非等向性扩散</a>）只考虑了每个点附近的点，非局部平均考虑了图上所有的点，故称为非局部的算法。非局部平均算法则对各个目标像素周围定义一个区块，并且<strong>对整个影像的所有像素依照该像素周围区块的区块与目标像素区块的相似度赋予权重、进行平均</strong>。</p><p>理论上，该算法需要在整个图像范围内判断像素间的相似度，也就是说，每处理一个像素点时，都要计算它与图像中所有像素点间的相似度。</p><p><img src="./src/Non-local-Neural-Networks/20150905213634707.png" alt="img"/></p><h3>连续的非局部均值</h3><p>非局部平均的定义为：
$$
NL<em>{u}(x) = \frac{1}{C(x)}\int</em>{\Omega}f<!-- -->[d(B_x,B_y)]<!-- -->u(y)dy
$$
其中$u$为要处理的图片，$\Omega$为整张图片的区域，$B<em>x$为以$x$为中心的一块区域，$d(x,y)$为$x$与$y$的欧几里得距离（$d(x,y)= {|x-y|}_2^2$），$f$是一个递减函数，$C(x)$为标准化因子（$C(x) = \int</em>{\Omega}{f<!-- -->[d(B_x,B_y)]<!-- -->}dy$）</p><p>此式可以解释为图上一点$x$经过降噪后的值为整张图片上所有点的加权平均，其中每个点的权重为该点的附近区块与$x$附近区块的相似度（将两个区块各自的点以相同的排列视为一向量，计算欧几里得距离），再经过一指数衰减的的函数（权重将落在(0,1]区间）。</p><h3>离散的非局部均值</h3><p>上述的式子所定义的算法为连续的，无法在实际的数位影像中使用，在实际应用中离散化版本的non-localoperations通用表示为：
$$
NL<em>u<!-- -->[i]<!-- --> = \frac{1}{C(i)}\sum</em>{j\in\Omega}{w(i,j)g(j)}
$$
其中$u$代表要处理的图片，$g(x)$表示$x$的某种线性变换，$w(i,j)$用于衡量为以$i,j$为中心的点的区块相似度（或者衡量i和j之间的关系），作为计算点$i$将早厚的值时点$j$对应的权重（常见的方法为计算欧几里得距离），$C(i)$为标准化因子（$C(i) = \sum_{i\in\Omega}{w(i,j)}$）。</p><p>在这里为了方便理解我写下一个字面意思的公式：s
$$
NonLocal<em>{u}(i,j) = 标准化(\sum</em>{所有j\in原图} 点j\times 点i和点j的相似度)
$$
但由于对每个点都要搜寻整张图片上其他的点来比较相似度，故运算复杂度往往会比局部的算法高，很明显这会导致很大的计算开销，所以实现的时候，会设定两个固定大小的窗口：搜索窗口（$D\times D, D=2<em>Ds+1$）以及邻域窗口（$d\times d, d=2</em>ds+1$），邻域窗口在搜索窗口中滑动，根据邻域间的相似性确定像素的权值。</p><p>如此可以使经过处理的影像更为清晰，并且损失较少的细节。也就是说，相较于local的算法，non-local的方法注重个体和全局的关系。</p><h2>为什么要使用Non-Local</h2><p>Non-Local operations、卷积以及全连接的目的都是建立某种关系（学术地讲可以叫做“学习一种分布”）。接下来我们说明它们的相似性和区别，以此说明为什么使用Non-Local。</p><h3>卷积和Non-Local</h3><p>以图像数据为例，要想捕捉长距离依赖，通常的做法是堆积卷积层，随着层数的加深，感受野越来越大，就能把原先非相邻的像素点纳入到一个整体考虑，获取的信息分布广度也越来越高。这种靠堆叠卷积层得到的感受野提升，需要不断重复卷积过程，而这种重复会带来几个弊端：</p><ol><li>计算效率很低，层的加深意味着更多的参数，更复杂的关系。</li><li>优化困难，需要谨慎设计优化过程。</li><li>建模困难，尤其是对于那些多级依赖项，需要在不同距离位置传递信息。</li><li>不够通用，卷积操作只能捕获单张图像上的关系，而对视频等序列上的关系无计可施。</li></ol><p>有一句话非常好地总结了卷积层如何建立像素间关系：</p><blockquote><p>convolution layer builds pixel relationship in a local neighborhood</p></blockquote><p>所以建立长距离依赖关系需要更好的、只需要一层的方法，也就是Non-Local：
$$
NL<em>u<!-- -->[i]<!-- --> = \frac{1}{C(i)}\sum</em>{j\in\Omega}{w(i,j)g(j)}
$$
在上式中，图像中的每一个位置 j 都被考虑到。与之相对应的，我们可以考虑一下卷积的过程，一个3x3的卷积核，能覆盖到的位置只是位置 j 的相邻点，只能在相邻局部进行操作。所以使用Non-Local更合适。</p><h3>全连接和Non-Local</h3><p>对比Non-Local和全连接，在non-local operation的公式中：
$$
NL<em>u<!-- -->[i]<!-- --> = \frac{1}{C(i)}\sum</em>{j\in\Omega}{w(i,j)g(j)}
$$
$NL_u$中的值是通过计算不同区域之间的关系得到的，而在全连接层中，是通过赋给每个神经元一个学到的权重。换而言之，在全连接层中，两个点或两个区域之间的关系被一个线性的常数确定而在non-local中，这种关系被一个可以自定义行为的函数$w$确定。</p><p>再者，non-local公式支持可变大小的输入，并在输出中保持相应的大小，在全连接层中，要求固定大小的输入和输出，并且由于被拉伸成一列，丢失了原有的位置信息。这也是为什么在传统的CNN结构中，卷积处于网络的前半部分用于提取特征，而全连接层通常被用在最后用于分类。当然，non-local operation可以被灵活地添加到深度神经网络中卷积层中的位置，这给了我们一个启发：能够构建一个更丰富的层次结构，将非本地信息和本地信息结合起来。</p><h2>Non-Local operations（非局部操作）</h2><p>在这篇论文中，作者将<strong>非局部操作</strong>（non-local operations）作为一种简洁高效且通用的组件，用于捕获深度神经网络的中的长距离依赖关系。</p><blockquote><p> For each query position, the non-local network first computes the pairwise relations between the query position and all positions to form an attention map, and then aggregates the features of all positions by weighted sum with the weights defined by the attention map.</p></blockquote><p>上文直接引用自GCNet的论文。这段话很好地总结了Non-Local Neural Network进行“非局部操作”的流程：对于每个位置，（在一张注意力图中）计算该位置和其他任何位置的相关性，形成一个权值，并用加权的方法计算当前区域的预测值。也就是，non-local operations在计算某个位置$i$处的相关性时，有：
$$
NL<em>u<!-- -->[i]<!-- --> = \frac{1}{C(i)}\sum</em>{j\in\Omega}{w(i,j)g(j)}
$$
<img src="./src/Non-local-Neural-Networks/image-20210711155849037.png" alt="image-20210711155849037"/></p><p>考虑所有位置features的加权——所有位置可以是空间的，时间的，时空的，这意味着non-localoperations适用于图像、序列和视频问题。</p><p>Non-Local operations的优点是：</p><ol><li>与递归操作和卷积操作的渐进行为不同，non-localoperations通过计算任意两个位置之间的交互直接捕捉远程依赖，而不用局限于相邻点，摒弃了距离的概念。</li><li>作者通过实验表明，在层数很少的情况下，non-localoperations都能取得非常好的性能表现。</li><li>non-local可以作为一个组件，和其它网络结构结合。</li></ol><p>我们在上面“为什么要使用Non-Local”一节中说明了卷积在处理长距离关系上的弊端，并说明了Non-Local为何有效。接下来给出该论文中Non-Local operation的具体实现形式。</p><h3>衡量关系的方法</h3><p>接下来对论文中的$w(i,j)$和$g(j)$的具体实现方法进行描述（原论文中的实验证明了选择何种$w(i,j)$和$g(j)$对于网络本身的表现来说不很敏感）。
$$
NL<em>u<!-- -->[i]<!-- --> = \frac{1}{C(i)}\sum</em>{j\in\Omega}{w(i,j)g(j)}
$$</p><p>为了简洁描述，可以将$g(j)$视为一个线性转化$g(j) = W_g \cdot u(j)$，其中$W_g$是要学习的权重矩阵（例如，在图像空间可以采用$1\times 1$卷积实现，在视频空间可以采用$1\times 1\times 1$卷积实现等等）。</p><h4>使用Gaussian衡量相似度</h4><p>对于$w(i,j)$，其主要功能是计算衡量两个图像区域的相似度。人们很自然会想到使用高斯函数。在这篇论文中有：
$$
w(x_i,x_j) = e^{x_i^Tx_j}
$$</p><h4>使用Embedded Gaussian衡量相似度</h4><p>为了获得一种更加普适的衡量方法，对上述高斯函数进行修改：
$$
w(x<em>i,x_j) = e^{\theta(x_i)^T\phi(x_j)}
$$
其中$\theta(x_i) = W</em>{\theta}x<em>i$和$\phi(x_i) = W</em>{\phi}x_i$是线性变换，可以将$x_i$映射到其他计算空间，从而更加具有普适性。</p><h4>使用点积（dot product）衡量相似度</h4><p>将$w(x_i,x_j)$简单地表示为点积：
$$
w(x_i,x_j) = {\theta(x_i)^T\phi(x_j)}
$$
在这里还是使用了&quot;Embedded sversion&quot;，也就是在$x$的外面套了一个线性变换。在这种情况下归一化因子$C(i)$被简单地描述为$N$（$N$是参与计算的点的个数），这样有助于简化梯度的计算。</p><p><img src="./src/Non-local-Neural-Networks/image-20210712203856857.png" alt="image-20210712203856857"/></p><p>如上图，在这篇论文的实验中，点积在top-1准确率中取得了相当不错的成绩。</p><h4>归一化</h4><p>观察原公式：
$$
NL<em>u<!-- -->[i]<!-- --> = \frac{1}{C(i)}\sum</em>{j\in\Omega}{w(i,j)g(j)}
$$
其中的$C(x)=\sum<em>{\forall{j}}{w(x_i,x_j)}$是归一化因子。将其带入后，会发现：$\frac{1}{C(i)}w(x_i,x_j)$实际上就是$softmax$的基本形式。所以上述公式可以写作：
$$
NL_u<!-- -->[i]<!-- --> = softmax(\sum</em>{j\in\Omega}w(i,j))g(j)
$$
如果我们选用点积作为$w(i,j)$进行相似度的衡量，再令$Y = NL<em>u<!-- -->[i]<!-- -->$，带入后会得到：
$$
Y = softmax(x^TW</em>{\theta}^TW_{\phi}x)g(x)
$$
<strong>这个就是目前常用的位置注意力机制的表达式。以上关于Non-Local的描述，在深度学习技术中可以归为自注意力机制自注意力机制（self-attention）</strong>，即通过关注特征图中所有位置并在嵌入空间中取其加权平均值来表示图片中某位置处的响应。嵌入空间可以认为是一个更抽象的图片空间表达，目的是汇聚更多的信息，提高计算效率。</p><p><code>使用点积方法和使用Embedded Gaussian方法区别在于是否使用softmax函数进行激活。</code></p><h3>Non-Local Block</h3><p>论文中实现Non-Local的方式是通过一个称为Non-Local block的结构实现的。</p><p><img src="./src/Non-local-Neural-Networks/image-20210712150626557.png" alt="image-20210712150626557"/></p><p>上图中$\theta(x<em>i) = W</em>{\theta}x<em>i$和$\phi(x_i) = W</em>{\phi}x<em>i$，$w(x_i,x_j) = {\theta(x_i)^T\phi(x_j)}$。将上方提到的归一化处理之后的公式抄下来就有：
$$
Z = softmax(x^TW</em>{\theta}^TW_{\phi}x)g(x)
$$
和上方计算图中表示的过程完全一样。</p><p><img src="./src/Non-local-Neural-Networks/image-20210712175504851.png" alt="image-20210712175504851"/></p><p>上图是一种区域相关性$w(i,j)$的直观表示。可以看出，待计算的像素位置是$p$，故先构造一个以$p$为中心的block，然后计算其他位置block和当前block的相关性，可以看出$q1$和$q2$区域和$q$非常相似，故计算时候给予一个大权重，而$q3$给予一个小的权重。这样的做法可以突出共性（关心的区域），消除差异（通常是噪声）。</p><p>当然，你也可以用欧几里得距离进行衡量（小声BB）。</p><h2>Summary</h2><p>这篇论文的主要思想其实是（空间位置）自注意力机制的泛化表达。不过这篇论文只强调了空间注意力，并没有明显使用像<a href="./%5B23%5DSqueeze-and-Excitation-Networks">SENet</a>那样的通道注意力。</p><p>原论文中也没有进行对Non-Local生成的attention map进行可视化的实验。不过另一篇论文（GCNet）中进行了这样的实验：s</p><p><img src="./src/Non-local-Neural-Networks/image-20210712213203846.png"/></p><p>GCNet的作者从COCO数据集中随机选择6幅图，分别可视化3个不同位置和它们的attention maps。作者发现对于不同位置来说，它们的attention maps几乎是相同的。这说明，<strong>虽然non-local block想要计算出每一个位置特定的全局上下文，但是经过训练之后，实际上形成的attention map受位置的影响非常低。</strong>这也是这篇论文的一个缺陷。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-local Networks Meet Squeeze-Excitation Networks and Beyond]]></title>
        <id>Non-local Networks Meet Squeeze-Excitation Networks and Beyond</id>
        <link href="https://ml.akasaki.space/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao%2C+Y">Yue Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+J">Jiarui Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin%2C+S">Stephen Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+F">Fangyun Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu%2C+H">Han Hu</a></p><p>GCNet（原论文：<a href="https://arxiv.org/abs/1904.11492">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a>或<a href="https://arxiv.org/abs/2012.13375">Global Context Networks</a>）这篇论文的研究思路类似于DPN，深入探讨了Non-local和SENet的优缺点，然后结合Non-local和SENet的优点提出了GCNet。</p><blockquote><p>The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at <a href="https://github.com/xvjiarui/GCNet">this https URL</a>.</p></blockquote><p><img src="./src/GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/image-20210713150550619.png" alt="image-20210713150550619"/></p><p>GCNet提出一种模块框架称为Global context modeling framework（上图中(a)），并将其分为三步：Context modeling、Transform、Fusion。</p><p>这篇论文选用<a href="./%5B27%5DNon-local-Neural-Networks">Non-Local Neural Networks</a>（上图中(b)是其简化版）的Context modeling 和 <a href="./%5B23%5DSqueeze-and-Excitation-Networks">Squeeze and Excitation Networks </a>（上图中(c)是其一种形式）的 Transform过程组成新的模块Global Context (GC) block，同时训练spacial和channel-wise上的注意力。这是一篇很好的论文，有兴趣请阅读<a href="https://arxiv.org/abs/1904.11492">原文</a>。</p><h2>Non-Local在实践中的缺陷</h2><p>这篇论文提出，NLNet（<a href="./%5B27%5DNon-local-Neural-Networks">Non-Local Neural Networks</a>）在长距离注意力方面作出了开创性的工作。在这里作者提出，为了建立长距离的注意力，有两种办法：</p><ol><li>采用<strong>自注意力机制</strong>来建模query对的关系（也就是Non-Local的做法）。</li><li>第二种是对query-independent（可以理解为无query依赖）的<strong>全局上下文</strong>建模。</li></ol><p>NLNet就是典型的采用自注意力机制来建模像素对关系。但是，作者从COCO数据集中随机选择6幅图，分别可视化3个不同位置和它们的attention maps进行实验：</p><p><img src="./src/GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/image-20210712213203846.png"/></p><p>结果发现对于不同位置来说，它们的attention maps几乎是相同的。这说明，<strong>虽然non-local block想要计算出每一个位置特定的全局上下文，但是经过训练之后，实际上形成的attention map受位置的影响非常低。</strong></p><p>也就是说，NLNet对于每一个位置学习不受位置依赖的attention map，造成了大量的计算浪费。通过严格的实验分析，作者发现non-local network的全局上下文在不同位置几乎是相同的，这表明NLNet经过训练之后实际上学习到的是弱位置依赖的全局上下文。</p><h2>简化Non-Local block</h2><p>如果你不知道什么是Non-Local block，请先阅读<a href="./%5B27%5DNon-local-Neural-Networks">Non-Local Neural Networks</a>。</p><p>刚才说到通过严格的实验分析，作者发现non-local network的全局上下文上造成了大量的计算浪费。作者取其精华去其糟粕设计了简化的Non-Local block：</p><p><img src="./src/GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/image-20210712230924295.png" alt="image-20210712230924295"/></p><p>上图中（a）是Non-Local neural Networks的原论文中提出的Non-Local block结构；（b）是这篇论文的作者重新设计的简化版本。</p><h3>原版Non-Local block</h3><p><strong>原版Non-Local block</strong>（上图中（a））（如果不知道这是什么，请先阅读请先阅读<a href="./%5B27%5DNon-local-Neural-Networks">Non-Local Neural Networks</a>）可以描述为：
$$
z<em>i = x_i + \sum</em>{j=1}^{N_p}\frac{f(x_i,x_j)}{C(x)}(W_v\cdot x_i)
$$
其中$C(x)$为归一化因子；$f(x_i,x_j)$为衡量相关性的函数，为了方便，定义$w(x_i,x_j)=\frac{f(x_i,x_j)}{C(x)}$为归一化的关系衡量函数；$W_z$和$W_v$是线性变换矩阵。</p><p>在本文中，作者选择将$f(x_i,x_j)$表示为Embedded Gaussian的形式：
$$
w(x_i,x_j) = \frac{\exp(W_q x_i,W_k x_j)}{\sum_m\exp(W_q x_i,W_k x_m)}
$$</p><h3>简化的Non-Local block</h3><p><strong>简化后的Non-Local block</strong>（上图中（b））可以描述为：
$$
z<em>i = x_i + \sum</em>{j=1}^{N<em>p}[\frac{\exp(W_k x_j)}{\sum</em>{m=1}^{N<em>p}\exp(W_k x_m)}(W_v\cdot x_j)]
$$
其中$W_k$和$W_v$是线性变换矩阵。为了进一步降低计算量作者将$W_v$移动到attention pooling的外面，表示为：
$$
z_i = x_i + W_v\sum</em>{j=1}^{N<em>p}\frac{\exp(W_k x_j)}{\sum</em>{m=1}^{N_p}\exp(W_k x_m)}x_j
$$
这样修改之后，$1\times 1$卷积的FLOPs从$O(HWC^2)$降低到了$O(C^2)$。</p><p>不同于原始的non-local block，简化版non-local block的第二项是不受位置依赖的，所有位置共享这一项。因此，作者直接将全局上下文建模为所有位置特征的加权平均值，然后聚集全局上下文特征到每个位置的特征上。</p><h2>Global Context Modeling Framework</h2><p>如同上文的叙述，简化版的non-local block可以抽象为3个步骤：</p><ol><li>全局attention pooling：采用$1\times1$卷积$W_k$和softmax函数来获取attention map，然后执行attention pooling获得全局上下文特征。</li><li>使用$1\times 1$卷积的线性变换$W_v$进行特征转换。</li><li>将得到的全局上下文特征通过加和的方式整合到每个位置上。</li></ol><p>整体过程的模块设计如图所示：</p><p><img src="./src/GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/image-20210713155953783.png" alt="image-20210713155953783"/></p><p>作者将该抽象过程描述为下面的这个公式：
$$
z<em>i = F(x_i, \delta(\sum</em>{j=1}^{N<em>p}\alpha_jx_j))
$$
其中$\sum</em>{j=1}^{N_p}\alpha_jx_j$被称为<strong>context modeling</strong>模块，用于通过权值$\alpha$聚合来自全局各个位置的特征；$\delta(\cdot)$表示一种用于捕获通道见相关性的transform；$F(\cdot,\cdot)$表示一个特征fusion的过程，在这里特指使用fusion函数将获得的全局上下文特征整合到每个位置上。</p><p>接下来，论文提出将SEBlock的操作分为三步：</p><ol><li>global average pooling用于上下文建模(即squeeze操作)。</li><li>bottleneck transform用于计算每个通道的重要程度(即excitation operation)。</li><li>rescaling function用于通道特征重标定(即element-wise multiplication)。</li></ol><p>并提出，其中的第一步（squeeze操作）可以替换为简化版的Non-Local Block。</p><h2>Global Context Block (GCBlock)</h2><p>作者提出了一种新的全局上下文建模框架，global context block(简写GCNet)，即能够像SNL block一样建立有效的长距离依赖，又能够像SEblock一样建立通道间的关系：</p><p><img src="./src/GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/image-20210713161721108.png" alt="image-20210713161721108"/></p><p>在简化版的non-local block中，transform模块有大量的参数。为了获得SE block轻量的优点，1x1卷积用bottleneck transform模块来取代，能够显著的降低参数量(其中r是降低率)。因为两层bottleneck transform增加了优化难度，所以在ReLU前面增加一个layer normalization层(降低优化难度且作为正则提高了泛化性)。</p><p>GCBlock可以被表示为下面的公式：
$$
z<em>i = x_i + W</em>{v2}Relu(LN(W<em>{v1}\sum</em>{j=1}^{N<em>p}\alpha_j x_j))
$$
上式中：
$$
\alpha_j = \frac{e^{W_k x_j}}{\sum</em>{m=1}^{N<em>p}e^{W_k x_m}}
$$
是全局attention pooling的权值，$\delta(\cdot)=W</em>{v2}ReLU(LN(W_{v1}(\cdot)))$表示bottleneck transform的过程。</p><p>也就是说，GCblock的操作可以分为以下三个步骤：</p><ol><li>global attention pooling用于上下文建模。</li><li>bottleneck transform来捕获通道间依赖。</li><li>broadcast element-wise addition用于特征融合。</li></ol><h2>有效性的讨论</h2><p>这篇论文很精彩，但是有以下几个问题让人想不通：</p><ol><li><p>为什么Non-Local在不同的query位置生成的attention map是几乎相同的？</p><p>已经有两篇论文（OCNet、DANet）证实了在分割模型里面加入non-local算子可以让学习得到的attention map集中在和query pixel同类别的区域，而不是和query无关。可能这个问题是和具体任务相关的。</p></li><li><p>文中的Global Context Modeling为何要这样设计也是一个问题。</p><p>根据论文中给出的公式，似乎使用全连接和一个残差连接能达到类似的功能。</p></li></ol>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Non-Local Neural Networks]]></title>
        <id>Disentangled Non-Local Neural Networks</id>
        <link href="https://ml.akasaki.space/blog/[29]Disentangled-Non-Local-Neural-Networks"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, Han Hu]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin%2C+M">Minghao Yin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao%2C+Z">Zhuliang Yao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao%2C+Y">Yue Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+X">Xiu Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zheng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin%2C+S">Stephen Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu%2C+H">Han Hu</a></p><blockquote><p>The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics.</p></blockquote><p>从论文名称上来看，这篇论文分析了<a href="./%5B27%5DNon-local-Neural-Networks">Non-Local Neural Networks</a>中的Non-Local模块中所存在的注意力机制，并对其设计进行了解耦。解耦后该注意力分为两部分：成对项（pairwise term）用于表示像素之间的关系，一元项（unary term）用于表示像素自身的某种显著性。这两项在Non-Local块中是紧密耦合的。这篇论文发现当着两部分被分开训练后，会分别对不同的视觉线索进行建模，并达到不错的效果。</p><p>整篇论文从对Non-Local分析到新的方法提出都非常地有调理。有时间请阅读原论文<a href="https://arxiv.org/abs/2006.06668">Disentangled Non-Local Neural Networks</a>。</p><blockquote><p>在阅读本文之前请先阅读<a href="./%5B27%5DNon-local-Neural-Networks">Non-Local Neural Networks</a>。</p></blockquote><h2>Non-Local的深入分析</h2><p>Non-Local块用于在单层内建立像素之间的长距离依赖关系，是一种像素级的自注意力机制。这种像素级的自注意力是通过像素的特征在嵌套（Embedding）空间进行点积运算获得的。初见Non-Local，会不自然地将其当作一种成对（Pairwise）的关系，即每个像素都会和其他像素建立一对一的关系。在更加深刻的思考后，本文的作者认为除了Pairwise的关系，这样的注意力还带有一个一元（Unary）的显著性信息，即每个像素本身也通过这种运算对其他像素产生影响。</p><p>回顾Non-Local是如何捕获长距离特征的：
$$
y<em>i = \sum</em>{j\in\Omega}w(x<em>i,x_j)g(x_j)
$$
其中$y_i$表示在位置$i$上的输出，$\Omega$表示所有像素对应的特征的集合，$g(\cdot)$是一个emedding函数，$w(x_i,x_j)$是在嵌套（Embedded）空间内计算$x_i,x_j$相关性的函数（忘记了这个公式的话可以复习一下<a href="./%5B27%5DNon-local-Neural-Networks">Non-Local Neural Networks</a>）。当$w(x_i,x_j)$是Embedded Gaussian时，$w(x_i,x_j)$可以展开为：
$$
w(x_i,x_j) = \sigma(q_i^Tk_j) = \frac{exp(q_i^Tk_j)}{\sum</em>{t\in\Omega}exp(q_i^Tk_t)}
$$
其中$q_i=W_qx_i$，$k_i=W_kx_i$，分别表示$x_i$的查询值（query）和键（key）（如有疑惑请查阅<a href="../ch3p2/%5B4%5Dattention">注意力机制</a>中有关键值注意力的部分），$\sigma(\cdot)$表示$softmax$函数。</p><p>第一眼看上去，该公式建立了一种成对（pairwise）的关系，即每个像素都会和其他像素建立一对一的关系。但是，作者同是发现该公式会同时编码像素的显著性信息。考虑如下特殊情况：当整张图片上的所有像素使用同个常量作为其query值时，每个像素的key就会对其他所有像素的位置上的输出产生影响（原文为<code>Considering a special case where the query vector is a constant over all image pixels, a key pixel will have global impact on all query pixels.</code>）。在<a href="./%5B28%5DGCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">GCNet</a>中，作者通过一系列实验展现了在一些视觉任务中，Non-Local块常常退化为一种仅包含一元项（unary term）的注意力：</p><p><img src="./src/Disentangled-Non-Local-Neural-Networks/image-20210712213203846.png"/></p><p>上图表示Non-Local块在不同的query位置（图片中的红点位置）产生的attention map可视化（详见<a href="./%5B28%5DGCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a>）。可以看出，当每个像素的key和所有的query具有类似的相似度时，Non-Local块就会退化成一个仅包含一元项（unary term）的注意力，表达像素之间特殊关系的能力降低了。这些现象都说明了，除了第一眼看上去的成对（pairwise）关系，一元（unary）的关系（也就是像素的显著性关系）也被Non-Local块编码。</p><p>所以，接下来作者将Non-Local块中所产生的注意力<strong>拆分为两个项（分量）：</strong></p><ul><li><strong>成对项（pairwise term）用于表示像素对另一个像素的单独影响</strong></li><li><strong>一元项（unary term）用于表示一个像素对所有像素的公共影响</strong>。</li></ul><p>在这篇论文中，作者单独训练了两个网络，分别用于对一元的和二元的两种注意力进行实验，保证它们互相不受对方干扰，并发现两种注意力分别产生了不同的作用：</p><p><img src="./src/Disentangled-Non-Local-Neural-Networks/image-20210716214158300.png" alt="image-20210716214158300"/></p><p>上图是Non-Local块和解耦的Non-Local块各部分产生注意力的可视化。可以看出，成对项（pairwise term）产生的注意力很好地建模了区域内的线索，而一元项（unary term）的注意力很好地建模了显著边界。对比Non-Local块在Non-Local Neural Networks中的表现，当这两种注意力没有被分开时，并没有学习到如此清晰的视觉线索。在后续的测试中，作者发现结合了这两种注意力的Non-Local本身的性能甚至比仅包含一元项（unary term）的变体还差。这表明将这两个项（term）耦合在一起可能不利于这些视觉线索的学习。</p><p>为了解决这个问题，作者就将这两项分开，提出了解耦（Disentangled）的Non-Local。在这个设计中，上文提到的一元项（unary term）经过单独的嵌入（embadding）和单独的$softmax$从原本的分布中脱离独立出来，从而实现了两项的解耦。</p><h2>Dividing Non-local Block into Pairwise and Unary Terms</h2><p>于是产生了一个问题，如何将上述公式分解为pairwise和unary两项，并使它们相互不产生影响呢？</p><p>为了解决这个问题，作者提出了一种能单纯表示key和query之间关系的点积，称为白化（whitened）点积用于表示pairwise项：
$$
(q<em>i-\mu_q)^T(k_j-\mu_k)
$$
其中$\mu_q=\frac{1}{|\Omega|}\sum</em>{i\in\Omega}q<em>i$和$\mu_k = \frac{1}{|\Omega|}\sum</em>{j\in\Omega}k<em>j$分别表示所有像素的query和key（在嵌入空间）的均值，这种白化（whitened）的设计使用query和key之间的归一化差异来衡量其相似性。上述表达式和Non-Local中的原始设计关系如下：
$$
q_i^Tk_j =(q_i-\mu_q)^T(k_j-\mu_k) + \mu_q^Tk_j+q_i^T\mu_k+\mu_q^T\mu_k
$$
其中最后两项是同时出现在相关性函数：
$$
w(x_i,x_j) = \sigma(q_i^Tk_j) = \frac{exp(q_i^Tk_j)}{\sum</em>{t\in\Omega}exp(q<em>i^Tk_t)}
$$
的分子和分母中。在原论文的附录中，作者证明了这使得最后两项可以被约掉。所以，相关性函数可以被写作：
$$
w(x_i,x_j) = \sigma(q_i^Tk_j) = \sigma(\underbrace{(q_i-\mu_q)^T(k_j-\mu_k)}</em>{pairwise}+\underbrace{\mu<em>q^Tk_j}</em>{unary})
$$
通过这种设计，深度学习技术在此处就学习了两种分布，从而在分布上pairwise和unary的关系就被分离了。</p><p><img src="./src/Disentangled-Non-Local-Neural-Networks/image-20210716214959782.png" alt="image-20210716214959782"/></p><p>上图中（b）、（c）分别表示Pairwise Non-Local和Unary Non-Local。（d）是上述公式的网络结构表示。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Retinex Decomposition for Low-Light Enhancement]]></title>
        <id>Deep Retinex Decomposition for Low-Light Enhancement</id>
        <link href="https://ml.akasaki.space/blog/[30]RetinexNet-for-Low-Light-Enhancement"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称: Deep Retinex Decomposition for Low-Light Enhancement]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称: <a href="https://arxiv.org/abs/1808.04560">Deep Retinex Decomposition for Low-Light Enhancement</a></p><p>论文作者: Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu</p><p>Code: <a href="https://github.com/weichen582/RetinexNet">https://github.com/weichen582/RetinexNet</a></p></blockquote><p>这是一篇讲解使用卷积神经网络进行低照度增强的论文。</p><ul><li>采用了分解网络和增强网络，使用Retinex理论构建分解网络，分解后再进行增强。</li></ul><h2>Abstract (摘要)</h2><blockquote><p>Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deepRetinex-Netlearned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjusment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.</p></blockquote><p>Retinex模型是弱光图像增强的有效工具。它假设观察到的图像<strong>可以分解为反射率和照度</strong>。大多数现有的基于视网膜的方法都为这种高度不适定的分解精心设计了手工制作的约束和参数，当应用于各种场景时，这可能会受到模型容量的限制。在本文中，我们收集了一个包含弱光/正常光图像对的弱光数据集，并在此数据集上提出了一个Deeprinex Netlearn，包括一个用于分解的Decom-Net和一个用于光照调整的增强-Net。在Decom-Net的训练过程中，不存在分解反射率和光照的基本事实。该网络仅在关键约束条件下学习，包括成对的弱光/正常光图像共享的一致反射率以及照明的平滑度。在分解的基础上，通过增强网络对光照进行后续的亮度增强，对于联合去噪，对反射率进行去噪操作。RetinexNet是端到端训练的，因此学习的分解本质上有利于亮度调整。大量实验表明，我们的方法不仅在弱光增强方面获得了视觉上令人满意的质量，而且提供了图像分解的良好表示。</p><h2>Introduction</h2><p>图像捕捉中的照明不足会显著降低图像的可见性。丢失的细节和低对比度不仅会造成不愉快的主观感受，还会损害许多为正常光线图像设计的计算机视觉系统的性能。照明不足的原因很多，例如光线不足的环境、摄影设备性能有限以及设备配置不当。</p><p>直方图均衡化(HE)  <!-- -->[20]<!-- -->及其变体抑制输出图像的直方图以满足某些约束。基于去雾的方法<!-- -->[5]<!-- -->利用了光照不足的图像和朦胧环境中的图像之间的反向联系。另一类弱光增强方法建立在Retinex理论<!-- -->[12]<!-- -->的基础上，该理论假设观察到的彩色图像可以<strong>分解为反射率和照度</strong>。作为早期的尝试，单尺度视网膜(SSR)  <!-- -->[11]<strong>通过高斯滤波器将光照图限制为平滑</strong>的。<strong>多尺度视网膜(MSRCR)  <!-- -->[10]<!-- -->通过多尺度高斯滤波器和颜色恢复扩展了SSR。</strong>[23]<!-- -->提出了一种利用亮度级误差测量来保持照明自然性的方法。Fu等人<!-- -->[7]<!-- -->提出融合初始光照图的多个导数。SRIE  <!-- -->[7]<!-- -->使用加权变分模型同时估计反射率和照度。操纵照明后，可以恢复目标结果。另一方面，LIME <!-- -->[9]<!-- -->仅利用结构先验估计光照，并使用反射作为最终的增强结果。还有基于Retinex的联合弱光增强和噪声去除方法<!-- -->[14，15]<!-- -->。对于CNN方法，CNN在低级别图像处理中得到了广泛应用，包括<strong>超分辨率<!-- -->[6，24，26，27]<!-- -->，去雨<!-- -->[16，21，25]<!-- -->等.  Lore等<!-- -->[17]<!-- -->使用堆叠稀疏去噪自动编码器同时进行弱光增强和降噪(LLNet)</strong>，但没有考虑弱光图片的性质。(前人常用的方法)</p><p>为了克服这些困难，我们提出了一种数据驱动的Retinex分解方法。建立了一个深度网络，称为RetinexNet，它集成了图像分解和连续增强操作。首先，DecompositionNet被用来将观察到的图像分割成独立于光照的反射和结构感知的平滑光照。Decom-Net是在两个约束条件下学习的。</p><p>首先，低/正常光图像具有相同的反射率。其次，光照图应该是平滑的，但保留主要结构，这是<strong>通过结构感知的总变化损失</strong>获得的。然后，另一个<strong>增强网络调整光照图</strong>，以保持大区域的一致性，同时通过多尺度拼接调整局部分布。由于噪声在黑暗区域通常更大，甚至被增强过程放大，因此引入了对反射率的去噪。</p><p>为了训练这样一个网络，我们从真实摄影和原始数据集中的合成图像构建了一个低/正常光图像对的数据集。大量实验表明，我们的方法不仅在弱光增强中获得了令人满意的视觉质量，而且提供了图像分解的良好表示。我们工作的贡献总结如下:</p><ul><li>构建了一个大规模数据集，其中包含在真实场景中捕获的成对弱光/正常光图像。据我们所知，它是微光增强领域的首次尝试；</li><li>构建了一个基于Retinex模型的深度学习图像分解模型；</li><li>分解网络是用连续的弱光增强网络进行端到端训练的，因此能够自然地调节光照；</li><li>针对深度图像分解，<strong>提出了一种结构感知的全变差约束。通过减轻梯度强的地方的总变化的影响，约束成功地平滑了光照图并保留了主要结构</strong>。</li></ul><p><img src="./src/RetinexNet-for-Low-Light-Enhancement/image-20210712162050981.png" alt="image-20210712162050981"/></p><h2>RetinexNet低照度增强</h2><p>经典的Retienx理论模拟了人类的颜色感知。它假设观察到的图像可以分解成两个分量，反射率和照度。让我们表示源图像，那么它可以表示为
$$
S=R\circ I
$$
其中$R$表示反射率，$I$表示亮度，$\circ$表示点乘，图像中的各个元素逐元素进行相乘。反射率描述了被捕获物体的内在属性，它被认为在任何亮度条件下都是一致的。照明表示物体上的各种亮度。在弱光图像上，它通常受到黑暗和不平衡照明分布的影响。</p><p>在视网膜理论的启发下，我们设计了一个深度视网膜网络来联合执行反射/光照分解和弱光增强。网络由三个步骤组成:<strong>分解、调整和重建</strong>。</p><p>在分解步骤中，视网膜神经网络通过分解神经网络将输入图像分解成图像。它在训练阶段接收成对的弱光/正常光图像，而在测试阶段只接收弱光图像作为输入。</p><p>在弱光/正常光图像具有相同的反射率和光照平滑度的约束下，Decom-Net学习以数据驱动的方式提取不同光照图像之间的一致R。</p><p>在调整步骤中，使用增强网来照亮光照图。<strong>增强网络采用一个整体的编码器-解码器框架</strong>。<strong>使用多尺度拼接来保持大区域中照明与上下文信息的全局一致性，同时集中注意力调整局部分布。</strong>此外，如果需要，通常在弱光条件下出现的放大噪声将从反射率中去除。然后，我们在重建阶段通过逐元素乘法来组合调整后的照明和反射。</p><h3>数据驱动型图像分解</h3><p>分解观察到的图像的一种方法是利用精心手工制作的约束直接在弱光输入图像上估计反射率和亮度。从(1)式中可以看出高度不适定的，不容易设计适合各种场景的约束函数。因此，我们试图以数据驱动的方式解决这个问题，<strong>通过引入数据来实现估计图像的反射率和亮度。</strong></p><p>在训练阶段，Decom-Net每次接收成对的弱光/正常光图像，并在弱光图像和正常光图像具有相同反射率的指导下，学习弱光及其对应的正常光图像的分解。请注意，虽然分解是用成对数据训练的，但它可以在测试阶段单独分解低照度输入。在训练过程中，不需要提供反射率和照度的基本事实。只有必要的知识，<strong>包括反射率的一致性和光照图的平滑度作为损失函数嵌入到网络中。</strong>因此，我们的网络的分解是从成对的低/正常光图像中自动学习的，并且本质上适合于描述不同光条件下图像之间的光变化。</p><p>需要注意的一点是，虽然这个问题在形式上可能与原有图像分解相似，但本质上是不同的。在我们的任务中，<strong>我们不需要精确地获得实际的原本的图像，而是一个很好的光线调节表示</strong>。因此，我们让网络学习<strong>寻找弱光图像与其对应的增强结果之间的一致性成分</strong>。</p><p>DecomNet将低照度图像$S<em>{low}$和正常图像$S</em>{normal}$作为输入，估计$S<em>{low}$的反射率$R</em>{low}$和亮度$I<em>{low}$，对于$S</em>{normal}$也是估计反射率$R<em>{normal}$和亮度$I</em>{normal}$。首先使用$3\times3$的卷积层从输入图像中提取特征，通过几个带有ReLU激活函数的$3\times3$的卷积层将RGB图像映射为反射率和照度。3×3卷积层从特征空间投射R和I，并使用sigmoid函数将R和I都约束在<!-- -->[0，1]<!-- -->范围内。</p><p>Loss的组成主要有三个部分：重构损失$L<em>{recon}$，不变反射率损失$L</em>{ir}$和光照平滑损失$L<em>{is}$，总的Loss的损失为:
$$
L=L</em>{recon}+\lambda<em>{ir}L</em>{ir}+\lambda<em>{is}L</em>{is}
$$</p><p>其中，$\lambda<em>{ir}$和$\lambda</em>{is}$表示反射率和平滑光照损失的系数。基于我们预想的，$R<em>{low}$和$R</em>{high}$可以用对应的光照图重构图像，重构损失表示为</p><p>$$
L<em>{recon}=\sum</em>{i=low,normal}\sum<em>{j=low,normal}\lambda</em>{ij}||R\circ I_j-S_j||_1
$$</p><p>不变损失用来约束反射率的一致性，损失函数表示为</p><p>$$
L<em>{ir}=||R</em>{low}-R_{normal}||_1
$$</p><p>光照损失当中，光照图的一个基本假设是局部一致性和结构感知，换句话说，光照贴图的一个好的解决方案应该是在纹理细节平滑的同时仍然可以保留整体结构边界。</p><ul><li><p>总可变最小化损失Total variation minimization (TV)，也就是使得整个图片的梯度最小，这是比较常用来做平滑先验在多种图像的恢复任务中他直接使用TV作为损失在局部区域中会失效，例如图像中有很强的结构或者亮度变化很大的地方。这是由于光照图的梯度的均匀减少，无论该区域是文本细节还是强边界。换句话说，TV损失函数是一种没有考虑到结构的损失函数，如图 2 所示，照明是模糊的，并且在反射上留下了强烈的黑色边缘。</p><p>  <img src="./src/RetinexNet-for-Low-Light-Enhancement/image-20210715091914773.png" alt="image-20210715091914773"/></p><p>  为了让损失函数意识意识到图像的结构，对原本的TV函数可以给反射图的梯度进行加权最终$L<em>{is}$表示为:
$$
L</em>{is}=\sum<em>{i=low,normal}||\nabla I_i\circ \exp(-\lambda\nabla R_i)||
$$
其中$\nabla$表示梯度，包含水平方向$\nabla_h$和垂直方向$\nabla_v$，$\lambda_g$表示平衡结构意识强度的系数。对于$\exp(-\lambda_g\nabla R_i)$，$L</em>{is}$减轻了反射率的梯度比较陡峭的平滑约束，也就是说，图像结构的定位和光照都是不连续的。</p></li></ul><h3>多尺度光照调整</h3><p>增强网络采用编码器-解码器的结构作为主体，为了从分层的角度调整光照，在不同尺度上引入了多尺度的concat。编码器解码器结构在大区域中能够获得图像的上下文信息，<strong>在输入图像被连续下采样到小尺度，在这个尺度下网络可以有比较大尺度光照分布的视野。这样可以让我们具有自适应调整的能力。</strong>利用大尺度光照信息，上采样块重建局部光照分布。通过元素求和，从下采样块到其对应的镜像上采样块引入跳跃连接，这强制网络学习残差。</p><p>为了分层调整光照，即保持全局光照的一致性，同时适应不同的局部光照分布，引入了<strong>多尺度拼接</strong>。如果有$M$个递增的上采样块，每个上采样块提取一个$C$通道特征图，我们通过最近邻插值调整这些不同比例的特征到最终比例，并将它们连接到一个$C\times M$通道特征图。然后，通过$1×1$卷积层，级联特征被简化为$C$通道。遵循$3×3$卷积层来重建照明图$\hat I$.</p><p>下采样块由步长为2的卷积层和ReLU组成。在上采样块中，使用了大小调整卷积层。调整大小卷积层由最近邻插值操作、步长为1的卷积层和ReLU组成。</p><p>增强网络的损失函数就由重构损失和光照平滑损失组成，重构损失意味着可以产生一个正常光照图$\hat S$
$$
L<em>{recon}=||R</em>{low}\circ\hat I-S<em>{normal}||_1
$$
光照平滑损失与(5)式相同，除了$\hat I$是被加权之后的$R</em>{low}$的梯度图。</p><h3>基于反射率的去噪方法</h3><p>在分解步骤中，对网络施加若干约束，其中之一是光照图的结构感知平滑性。当估计的光照图平滑时，细节都保留在反射率上，包括增强的噪声。因此，在用光照图重建输出图像之前，可以对反射率进行去噪处理。鉴于暗区域的噪声在分解过程中根据亮度强度被放大，我们应该使用与光照相关的去噪方法。</p><h2>Dataset</h2><p>我们的数据集命名为低光配对数据集(LOL)，包含500个低/正常光图像对。据我们所知，LOL是第一个包含从真实场景中获取的用于弱光增强的图像对的数据集。大多数弱光图像是通过改变曝光时间和ISO来收集的，而相机的其他配置是固定的。我们从各种场景中捕捉图像，例如房屋、校园、俱乐部、街道。图3示出了场景的子集。由于相机抖动、物体移动和亮度变化可能导致图像对之间的未对准，受<!-- -->[1]<!-- -->的启发，我们使用三步方法来消除数据集中图像对之间的这种未对准。实现细节可以在补充文件中找到。这些原始图像会调整到400×600，并转换为便携式网络图形格式。数据集将公开提供。(LOL数据集)</p><p>...剩余内容查看原文</p><h2>Experiments</h2><p>见原文...</p><h2>Conclusion</h2><p>本文提出了一种深度视网膜分解方法，该方法可以学习以数据驱动的方式将观测图像分解为反射和光照，而不需要分解反射和光照的基本事实。随后的光照增强和反射率去噪操作被引入。分解网络和弱光增强网络是端到端训练的。实验结果表明，我们的方法产生了视觉上令人满意的增强结果，以及图像分解的良好表现。</p>]]></content>
        <author>
            <name>PommesPeter</name>
            <uri>https://blog.pommespeter.com/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSR-net - Low-light Image Enhancement Using Deep Convolutional Network]]></title>
        <id>MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</id>
        <link href="https://ml.akasaki.space/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称Low-light Image Enhancement Using Deep Convolutional Network]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称: <a href="https://arxiv.org/abs/1711.02488">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></p><p>论文作者: Liang Shen, Zihan Y ue, Fan Feng, Quan Chen, Shihao Liu, Jie Ma</p><p>Code: None</p></blockquote><p>这是一篇讲解基于Retinex理论使用卷积神经网络进行低照度增强的论文。</p><ul><li>基于MSR传统理论构造卷积神经网络模型</li><li>直接学习暗图像和亮图像之间的端到端映射</li></ul><h2>Abstract (摘要)</h2><blockquote><p>Images captured in low-light conditions usually suffer from very low contrast, which increases the difficulty of sub-sequent computer vision tasks in a great extent. In this paper, a low-light image enhancement model based on convolutional neural network and Retinex theory is proposed. Firstly, we show that multi-scale Retinex is equivalent to a feedforward convolutional neural network with different Gaussian convolution kernels. Motivated by this fact, we consider a Convolutional Neural Network(MSR-net) that directly learns an end-to-end mapping between dark and bright images. Different fundamentally from existing approaches, low-light image enhancement in this paper is regarded as a machine learning problem. In this model, most of the parameters are optimized by back-propagation, while the parameters of traditional models depend on the artificial setting. Experiments on a number of challenging images reveal the advantages of our method in comparison with other state-of-the-art methods from the qualitative and quantitative perspective.</p></blockquote><p>本文提出了一种基于卷积神经网络和视网膜理论(Retinex Theory)的低照度图像增强模型。证明了多尺度视网膜等价于一个具有不同高斯卷积核的前馈卷积神经网络。考虑一种卷积神经网络(MSR网络)，它<strong>直接学习暗图像和亮图像之间的端到端映射</strong>。</p><h2>Introduction</h2><p>在弱光条件下拍摄时，图像有时候会表现出非常低的对比度和亮度，这在很大程度上增加了后续高级任务的难度。一般很多细节都被隐藏在黑暗的背景中。现在有很多种方法解决，主要是分为基于直方图的方法和基于Retinex理论的方法。</p><p>本文提出的方法是基于卷积神经网络和视网膜理论(Retinex Theory)的低照度图像增强模型。我们解释了多尺度视网膜等价于一个具有不同高斯卷积核的前馈卷积神经网络。我们提出了一种卷积神经网络(MSR网络)，它可以直接学习暗图像和亮图像之间的端到端映射。</p><p>我们主要做三个方面的工作:</p><ul><li>我们建立了多尺度视网膜神经网络和前馈卷积神经网络之间的关系。</li><li>我们将弱光图像增强视为一个监督学习问题，暗图像和亮图像分别作为输入和输出。</li><li>在一些具有挑战性的图像上的实验揭示了我们的方法与其他最先进的方法相比的优势。(实验对比)</li></ul><h2>Related Work</h2><h3>低照度增强</h3><p>一般来说，微光图像增强可以分为两类:基于直方图的方法和基于视网膜的方法(histogram-based methods and
Retinex-based methods)。通过直方图变换直接放大弱光图像可能是最直观的亮暗图像的方法。最简单也是应用最广泛的技术之一是直方图均衡化(he)，它使整个图像的直方图尽可能平衡。伽马校正也是一个很好的方法，通过同时扩大暗区和压缩亮区来增强对比度和亮度。然而，这些方法的主要缺点是图像中的每个像素都是单独处理的，没有其邻域的依赖性，这使得结果看起来与真实场景不一致。为了解决上述问题，提出了在直方图上使用不同正则项的变分方法。相对而言，更多地会使用Retinex的方法。</p><p>Retinex理论的主要内容是：图像可以被分解成为反射和亮度(公示表达为:$I(x,y)=r(x,y)\cdot S(x,y)$)。此外，修正的MSR <!-- -->[16]<!-- -->在色度空间中应用颜色恢复函数(CRF)来消除MSR输出中明显的颜色失真和灰色区域。</p><h3>使用CNN做低级视觉任务</h3><p>近期越来越多的人使用卷积神经网络实现低级的视觉任务。这方面做了很多工作，如去除雨水，去除大雾等。</p><h2>CNN for Image Enhancement</h2><p>本文主要提出的是一个直接学习低照度图像和亮图像之间端到端的映射。</p><h3>多尺度视网膜</h3><p>首先介绍SSR模型，也就是单尺度Retinex是基于Retinex理论，SSR的核心公式如下
$$
I(x,y)=r(x,y)\cdot S(x,y)
$$
其中$I$和$r$分别表示捕获的图像和反射。基于中心/周围视网膜的单尺度视网膜(SSR)类似于自然视觉科学中广泛使用的高斯差分(DOG)函数。
$$
R<em>i(x,y)=\log I_i(x,y)-\log<!-- -->[F(x,y)*I_i(x,y)]<!-- -->
$$
其中$R_i(x，y)$是相关的Retinex输出，$I_i(x，y)$是第$i$个颜色光谱带中的图像分布，$<em>$表示卷积运算，而$F(x，y)$是高斯环绕函数。
$$
F(x,y)=Ke^{-\frac{x^2+y^2}{2c^2}}
$$
其中$c$是标准差，$K$是一个系数，使得(3)式满足:
$$
\iint F(x,y)dxdy=1
$$
通过改变对数在上述公式中的位置得
$$
R_i(x,y)=\log I_i(x,y)-<!-- -->[\log I_i(x,y)]<!-- --> </em> F(x,y)
$$
通过上述操作就得到了一个经典的高通线性滤波器。(2)式和(5)式在数学上表达式是不等价的，<strong>前者是求图像与其加权平均值之比的对数，后者是图像与夹全乘积之比的对数。</strong>本质上相当于是算术平均值和几何平均值之间进行选择。那么求解多尺度的Retinex增强可以通过对不同尺度大小的分别进行增强，然后求其加权平均值。
$$
R</em>{MSR<em>i}=\sum^N</em>{n=1}w<em>nR</em>{n<em>i}
$$
其中$N$表示不同尺度的数量，$R</em>{n<em>i}$表示第$n$个尺度的第$i$个分量，$R</em>{MSR<em>i}$表示MSR输出的第$i$个分量。$w_n$表示与第$n$个尺度相关的权重。为了消除强边缘附近可见的“光晕”伪影，对第三个中间范围的需求立即变得明显<!-- -->[16]<!-- -->。因此，公式如下:
$$
R</em>{MSR<em>i}(x, y) =\frac{1}{3}\sum^3</em>{n=1}<!-- -->{<!-- -->\log I<em>i(x, y) − <!-- -->[\log I_i(x, y)]<!-- --> ∗ F_n(x, y)<!-- -->}<!-- -->
$$
具体而言就得到
$$
R</em>{MSR<em>i}(x,y)=\log I_i(x,y)-\frac{1}{3}\log I_i(x,y)*[\sum^3</em>{n=1}K_ne^{-\frac{x^2+y^2}{2c^2_n}}]
$$
注意到两个高斯函数的卷积仍然是高斯函数，其方差等于两个原始方差之和。因此，我们可以使用级联结构来表示上面的等式(8)，如图2(a)所示<strong>(该图不是神经网络结构图)</strong>。</p><p><img src="./src/MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network/image-20210708085043777.png" alt="image-20210708085043777"/></p><p>三个级联卷积层被视为三个不同的高斯核。更具体地，第一卷积层的参数基于高斯分布，其方差为$c^2_1$。类似地，第二和第三卷积层的方差分别为$c^2_2-c^2_1$，$c_3^2-c_2^2$，级联层和$1 × 1$卷积层代表加权平均值。总之，多尺度视网膜神经网络实际上相当于一个具有残差结构的前馈卷积神经网络。</p><h3>卷积神经网络方法</h3><p>在前一节中，我们提出了多尺度视网膜等效于前馈卷积神经网络的事实。在这一部分中，受这个新事实的启发，我们考虑用卷积神经网络来解决弱光图像增强问题。我们在图2(b)中概述的方法与现有方法有根本的不同，现有方法将弱光图像增强作为一个有监督的学习问题。<strong>输入和输出数据分别对应于弱光和亮图像</strong>。</p><p><img src="./src/MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network/image-20210708085645880.png" alt="image-20210708085645880"/></p><p>我们的模型由三部分组成:多尺度对数变换、卷积差和颜色恢复函数。与MSR的单尺度对数变换相比，我们的模型尝试使用多尺度对数变换，在实践中得到验证，取得了更好的性能。图7给出了一个例子。卷积差在MSR起到与高斯差类似的作用，色彩恢复功能也是如此。我们的模型与原始MSR模型的主要区别在于，我们的模型中的大多数参数是从训练数据中学习的，而MSR模型中的参数，如方差和其他常数，则取决于人工设置。</p><p>形式上，我们将弱光图像表示为输入$X$，相应的亮图像表示为Y，假设$f_1、f_2、f_3$表示三个子函数:多尺度对数变换、卷积差和颜色恢复函数。我们的模型可以写成三个函数的组合:
$$
f(X)=f_3(f_2(f_1(X)))
$$</p><ul><li><p>多尺度对数变换</p><p>多尺度对数变换$f<em>1(X)$以原始低照度图像$X$为输入，计算相同大小的输出$X_1$。首先，对低照度图像进行多次差分对数变换增强。公式如下:
$$
M_j=\log</em>{v<em>j+1}(1+v_j\cdot X),j=1,2,...,n
$$
其中$M_j$表示在经过底数为$v_j+1$的对数运算之后的第$j$个尺度输出。n表示对数变换函数的数量。接下来，我们将这些3D张量$Mj(3个通道×宽度×高度)$连接成更大的3D张量$M(3n个通道×宽度×高度)$，然后使其通过卷积层和ReLU层。
$$
M=<a href="%E5%81%9Aconcat">M_1,M_2,...,M_n</a> <!-- -->\<!-- -->
X_1=\max(0, M*W</em>{-1}+b_{-1})<em>W_0+b_0
$$
其中$</em>$表示卷积算子，$w_1$是将$3n$个通道收缩为3个通道的卷积核，$max(0,\cdot)$对应于ReLU，$w_0$是具有三个输出通道的卷积核，以获得更好的非线性表示。从上面的操作可以看出，这部分主要是为了<strong>通过多次对数变换的加权和得到更好的图像，加快了网络的收敛速度。</strong></p></li><li><p>卷积差</p><p>卷积差函数$f<em>2$取输入$X_1$，并计算相同大小的输出$X_2$。首先，输入$X_1$穿过多层卷积层和一层ReLU。
$$
H_0=X_1<!-- -->\<!-- -->
H_m=\max(0, H</em>{m-1}*W<em>m+b_m),m=1,2,...,K
$$
其中$m$表示第$m$个卷积层，$K$等于卷积层数。而$W_m$代表第m个核的权重。$H_1,H_2,...,H_K$被认为是不同尺度下的平滑图像，然后我们将这些3D张量$H_m$连接到更大的3D张量H，并使其通过卷积层:
$$
H = <!-- -->[H_1, H_2, ..., H_K]<!-- --> <!-- -->\<!-- -->
H</em>{K+1}=H∗W<em>{K+1}+ b</em>{K+1}
$$
其中$W<em>{K+1}$是具有三个输出通道和$1  × 1$感受野的卷积层，相当于对这些$K$个图像求平均。类似于MSR，$f_2$的输出$X_2$是$X_1$和$H</em>{K+1}$之间的减法:
$$
X<em>2=f_2(X)=X_1-H</em>{K+1}
$$</p></li><li><p>颜色恢复函数</p><p><strong>考虑到MSR结果通常看起来不自然</strong>，修改后的MSR在色度空间中应用颜色恢复函数(CRF)来消除MSR输出中<strong>明显的颜色失真和灰色区域</strong>。在我们的模型中，颜色恢复函数是用一个具有三个输出通道的$1 ×  1$卷积层来模拟:
$$
\hat Y=f<em>3(X_2)=X_2*W</em>{K+2}+b_{K+2}
$$
其中$\hat Y$是最终的增强图像。为了更加直观，图3中分别显示了弱光图像和$f_1、f_2、f_3$的结果。</p><h3>目标函数</h3><p>我们模型的目标是训练一个深度卷积神经网络，使输出$f(X)$和标签$Y$<strong>在Frobenius范数</strong>的准则下尽可能接近。
$$
L =\frac{1}{N}\sum^N<em>{i=1}||f(X_i) − Y_i||^2_F+λ\sum^{K+2}</em>{i=-1}||Wi||^2_F
$$
其中$N$为训练样本数，$λ$为正则化参数。权重$W$和偏差$b$是我们模型中的全部参数。此外，模型中考虑了正则化参数$λ$、对数变换函数个数$n$、对数变换尺度$v$和卷积层数$K$作为超参数。模型中的参数通过反向传播进行优化，超参数通过网格搜索进行选择。</p></li></ul><h2>问题</h2><p>  本文由于我们模型中的感受野有限，像晴空这样非常平滑的区域有时会受到光晕效应的攻击。扩大感受野或增加隐藏层可以解决这个问题。</p><p>  <img src="./src/MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network/image-20210708095003523.png" alt="image-20210708095003523"/></p><h2>实验</h2><p>实验部分自行阅读原文</p>]]></content>
        <author>
            <name>PommesPeter</name>
            <uri>https://blog.pommespeter.com/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLCNN - A convolutional neural network for low-light image enhancement]]></title>
        <id>LLCNN - A convolutional neural network for low-light image enhancement</id>
        <link href="https://ml.akasaki.space/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称 A convolutional neural network for low-light image enhancement]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称: <a href="https://ieeexplore.ieee.org/abstract/document/8305143/">LLCNN: A convolutional neural network for low-light image enhancement</a></p><p>论文作者: Li Tao, Chuang Zhu, Guoqing Xiang, Yuan Li, Huizhu Jia, Xiaodong Xie</p><p>Code: <a href="https://github.com/BestJuly/LLCNN">https://github.com/BestJuly/LLCNN</a></p></blockquote><h3>这篇笔记的写作者是<a href="https://github.com/PommesPeter">PommesPeter</a>。</h3><p>这是一篇讲解使用卷积神经网络进行低照度增强的论文。</p><ul><li>本文使用卷积神经网络进行低照度增强</li><li>使用SSIM损失更好地评价图像好坏和梯度收敛</li></ul><h2>Abstract (摘要)</h2><blockquote><p>In this paper, we propose a CNN based method to perform low-light image enhancement. We design a special  module to utilize multiscale feature maps, which can avoid  gradient vanishing problem as well. In order to preserve image textures as much as possible, we use SSIM loss to train our model. The contrast of low-light images can be adaptively enhanced using our method. Results demonstrate that our CNN based method  outperforms other contrast enhancement methods. </p></blockquote><p>本文提出了一种基于CNN的低照度图像增强方法。我们设计了一个特殊的模块来<strong>利用多尺度特征映射</strong>，这样可以避免梯度消失的问题。<strong>为了尽可能地保留图像纹理，我们使用SSIM损失来训练我们的模型</strong>。使用我们的方法可以<strong>自适应地增强弱光图像的对比度</strong>。</p><h2>Introduction</h2><p>为了增强图像对比度和提高图像亮度，提出了几种算法。<strong>直方图均衡(HE)方法</strong>[1]<!-- -->，<!-- -->[2]<!-- -->重新排列像素值，使其服从均匀分布。<strong>基于视网膜理论的方法</strong>[3]<!-- -->–<!-- -->[7]<!-- -->利用了一个模型，该模型假设图像是照明和反射的相互作用。<strong>基于dehaze模型的方法</strong>[8]<!-- -->固定像素值，使其服从自然分布。我们称所有这些方法为传统方法。</p><p>本文应用CNN对低照度图像进行增强。我们称所提出的网络为低照度卷积神经网络。它学习用不同的核过滤弱光图像，然后将多尺度特征图组合在一起生成增强图像，这些图像看起来是在正常光照条件下捕获的，并且保留了原始特征和纹理。此外，<strong>SSIM损失被整合到我们的LLCNN，以重建更准确的图像纹理。</strong>我们将我们的结果与其他方法进行了比较，结果表明我们的方法在常见的弱光图像增强方法中取得了最好的性能。</p><h2>Related Work</h2><h3>低照度增强方法</h3><p>一般来说，低照度图像增强方法可以分为<strong>三类</strong>。</p><ul><li><p>HE方法保持像素值之间的相对关系，尽量使其服从均匀分布。</p></li><li><p>动态直方图均衡化(DHE)  <!-- -->[1]<!-- -->将直方图分成几个部分，并在每个子直方图中执行HE处理。</p></li><li><p>对比度受限的自适应直方图均衡化(CLAHE)  <!-- -->[2]<!-- -->自适应地限制了HE的对比度增强结果的程度。</p></li></ul><p>对于这些HE方法，在许多情况下会出现严重的偏色问题，而且暗黑暗区域的细节没有得到适当的增强。</p><p>基于Retinex理论的方法计算图像的照度，并通过去除它们来实现图像增强。单尺度视网膜(SSR)  <!-- -->[3]<!-- -->、多尺度视网膜(MSR) <!-- -->[4]<!-- -->和带颜色恢复的多尺度视网膜(MSRCR) <!-- -->[5]<!-- -->是这类方法的典型作品。最近，一些新的方法(SRIE  <!-- -->[6]<!-- -->，LIME<!-- -->[7]<!-- -->)被提出来估计光照图和反射率以增强低照度图像。在许多情况下，这些方法可能会出现严重的<strong>颜色失真</strong>。</p><p>基于Dehaze模型的方法反转低照度图像，并在其上应用dehaze方法。在Dehaze模型中，这种方法用于增强低照度图像。然而，图像通常被过度增强，并且增强图像的饱和度通常被夸大。</p><h3>使用深度学习进行图像处理的方法</h3><p>*<!-- -->LLNet是唯一使用深度神经网络增强低照度图像的方法。该网络是<strong>堆叠稀疏去噪自动编码器的变体</strong>，并且它不使用卷积层。使用非线性方法模拟弱光条件，使自然图像变暗。这些图像被设置为训练数据。经过训练，网络可以增强弱光图像。</p><h3>Inception模块和残差学习</h3><p>在许多计算机视觉任务中，深度网络比非深度网络具有更好的性能。然而，当堆叠的卷积层越深，网络就会遇到梯度消失的严重问题，这可能会导致训练难以收敛。使用inception模块和残差学习能够解决这些问题。(因为GoogleNet and ResNet取得了比较好的效果)</p><h2>Proposed Method</h2><h3>网络结构</h3><p>虽然低照度图像增强属于低层次(low-level)的图像处理任务，但它与超分辨率和图像去噪有很大不同。对于这两个任务，劣质图像中的像素值都在真值附近，平均像素值几乎不变，这在我们的任务中是不同的。因此，我们设计了一个不同但有效的CNN网络来增强弱光图像。网络结构如下</p><p><img src="./src/LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement/image-20210709090203976.png" alt="image-20210709090203976"/></p><p>具体流程如Fig2，可分为两个阶段。</p><p>在第一阶段，数据以两种不同的方式处理。一种方式是1×1卷积层，另一种方式是两个3×3卷积层。我们将它们组合在一起，形成第二阶段的输入。第一阶段类似于初始模块。我们不对结果做concat，而是直接相加它们。</p><p>第二阶段，也有两种方式。第一种方式是使用两个3×3卷积层处理数据，而第二种方式是直接绕过输入数据，这是残差学习中使用残差连接。</p><p><img src="./src/LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement/image-20210709090435335.png" alt="image-20210709090435335"/></p><p>对于VDSR和DnCNN，网络会生成一个残差图像，通过将残差图像与原始图像相加来计算最终图像。这是因为对于超分辨率和图像去噪，地面真实和劣化图像之间的差异不是很大。对于弱光图像增强，网络学习残留图像似乎比一点一点地增强图像更困难。因此，我们不使用这种在VDSR或DnCNN的架构。另一个原因是我们已经在模块中利用了残差学习。</p><p>LLCNN的结构是这样描述的:</p><p>一个卷积层做预处理产生归一化的输入，另一个卷积层产生增强图像，在这两层之间放置几个特殊设计的卷积模块。对于每个过滤器，我们使用64个过滤器，除了最后一个。最后一层中使用的滤镜数量取决于颜色通道的数量。</p><p>该网络以弱光图像作为输入，并进行处理，以使输出图像看起来像是在正常光线条件下捕获的。类似于VDSR和DnCNN，我们将原始图像切割成面片。补丁大小设置为41×41。所有输入图像均采用非线性方法生成，以模拟弱光条件。</p><h3>SSIM损失函数</h3><p>在低照度增强中，我们不需要增强在特定光照环境下的图像。例如，给定在白天拍摄的自然图像，通过添加或减去一个小数字来改变所有像素值是影响很小，在这种情况下，结构几乎不会改变，但使用PSNR函数会有很大的差异。SSIM公式如下
$$
SSIM(p)=\frac{2\mu<em>x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}\cdot\frac{2\sigma</em>{xy}+C<em>2}{\sigma_x^2+\sigma_y^2+C_2}
$$
SSIM的值域范围为$(0,1]$，值为1时表示两张图像完全一样，因此我们用$1-SSIM(p)$来计算像素的损失。损失函数定义如下
$$
l</em>{ssim}\frac{1}{N}\sum_{p\in P}1-SSIM(p)
$$
我们遵循<!-- -->[11]<!-- -->并将自然图像设置为地面真实图像，并且通过退化方法产生弱光图像。随机伽玛调整用于模拟弱光图像。参数γ在范围(2，5)内随机设置，这将使网络能够自适应地增强图像。我们测试了两种不同深度的网络。一个叫做LLCNN，它有三个模块，所以它有17个卷积层。另一个LLCNN-s使用两个模块，卷积层数为12层。除了最后一层，我们在每层设置了64个过滤器。训练时动量设置为0.9，重量衰减为0.0001。基础学习率为0.01。学习速度会在训练过程中发生变化。这里还提供了SSIM损耗层的参数。卷积核大小和$C_1$,$C_2$分别设置为8、0.0001和0.001。在我们的训练过程中，每迭代4万次表示1个epoch。</p><h2>Experiments</h2><p>详见原文</p><p><img src="./src/LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement/image-20210709091917127.png" alt="image-20210709091917127"/></p><p><img src="./src/LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement/image-20210709091927771.png" alt="image-20210709091927771"/></p>]]></content>
        <author>
            <name>PommesPeter</name>
            <uri>https://blog.pommespeter.com/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOLO - Vision Outlooker for Visual Recognition]]></title>
        <id>VOLO - Vision Outlooker for Visual Recognition</id>
        <link href="https://ml.akasaki.space/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：VOLO: Vision Outlooker for Visual Recognition]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：VOLO: Vision Outlooker for Visual Recognition</p><p>作者：Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, Shuicheng Yan</p><p>Code： <a href="https://github.com/sail-sg/volo">https://github.com/sail-sg/volo</a></p></blockquote><h2>摘要</h2><ul><li>视觉识别任务已被$CNN$主宰多年。基于自注意力的$ViT$在$ImageNet$分类方面表现出了极大的潜力，在没有额外数据前提下，$Transformer$的性能与最先进的$CNN$模型仍具有差距。在这项工作中，我们的目标是缩小这两者之间的性能差距，并且证明了基于注意力的模型确实能够比$CNN$表现更好。</li><li>与此同时，我们发现限制$ViTs$在$ImageNet$分类中的性能的主要因素是其在将细粒度级别的特征编码乘$Token$表示过程中比较低效，为了解决这个问题，我们引入了一种新的$outlook$注意力，并提出了一个简单而通用的架构，称为$Vision$ $outlooker$ ($VOLO$)。$outlook$注意力主要将$fine$​-$level$级别的特征和上下文信息更高效地编码到$token$表示中，这些$token$对识别性能至关重要，但往往被自注意力所忽视。</li><li>实验表明，在不使用任何额外训练数据的情况下，$VOLO$在$ImageNet$-$1K$分类任务上达到了87.1%的$top$-$1$准确率，这是第一个超过87%的模型。此外，预训练好的VOLO模型还可以很好地迁移到下游任务，如语义分割。我们在$Cityscapes$验证集上获得了84.3% $mIoU$，在$ADE20K$验证集上获得了54.3%的$mIoU$，均创下了最新记录。</li></ul><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210730004322.png"/><p><strong>总结</strong>：本文提出了一种新型的注意力机制——$Outlook\ Attention$，与粗略建模全局长距离关系的$Self\ Attention$不同，$Outlook$能在邻域上更精细地编码领域特征，弥补了$Self\ Attention$对更精细特征编码的不足。</p><h2>OutLooker Attention</h2><p><strong>OutLooker</strong>模块可视作拥有两个独立阶段的结构，第一个部分包含一堆$OutLooker$用于生成精细化的表示（$Token$ $representations$），第二个部分部署一系列的转换器来聚合全局信息。在每个部分之前，都有块嵌入模块（$patch$ $embedding$ $module$）将输入映射到指定形状。</p><h3>理论和形式</h3><p>OutLooker的提出主要基于以下两点：</p><ol><li>每个空间位置的特征都具有足够的代表性，可以生成聚集了局部邻近信息的注意力权重</li><li>密集的局部空间聚合信息可以有效地编码精细层次的信息</li></ol><p>OutLooker由用于空间信息编码的outlook注意层和用于通道信息交互的MLP组成，给定$X\in \mathbb{R}^{H\times W\times C}$，有一下形式：
$$
\tilde{\mathbf{X}}=OutlookAtt(LN(\mathbf{X}))+\mathbf{X},\qquad(1)<!-- -->\<!-- -->
\mathbf{Z}=MLP(LN(\tilde{\mathbf{X}}))+\tilde{\mathbf{X}}.\qquad\qquad(2)
$$
其中$LN$表示$LayerNorm$d</p><h3>方法</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210723151620803.png" alt="image-20210723151620803"/></p><p>从上图我们可以很容易得到，整个过程分为两条路线，接下来先介绍第一条</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210730004422.gif"/></p><p><strong>Outlook Attention Generation</strong>：</p><ol><li>通过全连接层将$Input$​的通道由$<!-- -->[H,W,C]<!-- -->$​变为$<!-- -->[H,W,K^4]<!-- -->$​，得到$Q$</li><li>通过$reshape$​将注意力权重变为$<!-- -->[H<em>W,K</em>K,K*K]<!-- -->$​，表示每个像素生成的权重</li><li>在最后一维使用$Softmax$​​，这里可以看出为什么通道数变为$K^4$​​​​，因为其需要为$K\times K$大小的窗口里所有的像素建立相互关系，也就是说，这可以看作是一种局部的$self-attention$，这也是与$Involution$​等类似工作的一个巨大不同之处；这里的$Softmax$就是对每一个像素计算与其他所有像素（包括自己）的一个相似度</li></ol><p><strong>Dense aggregation(Value Generation)</strong>：</p><ol><li>首先使用全连接层进行一次线性变换，通道数不改变</li><li>使用<code>nn.Unfold()</code>进行展开，维度为$<!-- -->[H<em>W,K</em>K,C]<!-- -->$​，得到$V$</li></ol><p><strong>Calculate The Attention</strong>：</p><ol><li>两条路线得到的矩阵进行矩阵乘积，相当进行了一次卷积操作，卷积核为$Outlook\ Attention\ Weight$​</li><li>使用<code>nn.Fold</code>折叠回原尺寸</li></ol><p>整个过程的代码如下所示：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210723162203748.png" alt="image-20210723162203748"/></p><pre><code class="language-python">class OutlookAttention(nn.Module):
    &quot;&quot;&quot;
    Implementation of outlook attention
    --dim: hidden dim
    --num_heads: number of heads
    --kernel_size: kernel size in each window for outlook attention
    return: token features after outlook attention
    &quot;&quot;&quot;

    def __init__(self, dim, num_heads, kernel_size=3, padding=1, stride=1,
                 qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        head_dim = dim // num_heads
        self.num_heads = num_heads
        self.kernel_size = kernel_size
        self.padding = padding
        self.stride = stride
        self.scale = qk_scale or head_dim**-0.5

        self.v = nn.Linear(dim, dim, bias=qkv_bias)
        self.attn = nn.Linear(dim, kernel_size**4 * num_heads)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding, stride=stride)
        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True) # stride的实现可能靠破

    def forward(self, x):
        B, H, W, C = x.shape

        v = self.v(x).permute(0, 3, 1, 2)  # B, H, W, C -&gt;B, C, H, w

        h, w = math.ceil(H / self.stride), math.ceil(W / self.stride)
        v = self.unfold(v).reshape(B, self.num_heads, C // self.num_heads,
                                   self.kernel_size * self.kernel_size,
                                   h * w).permute(0, 1, 4, 3, 2)  # B, N, C//N, K*K, H*W-&gt;B, N, H*W, K*K, C//N 

        attn = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)
        attn = self.attn(attn).reshape(
            B, h * w, self.num_heads, self.kernel_size * self.kernel_size,
            self.kernel_size * self.kernel_size).permute(0, 2, 1, 3, 4)  # B,H,N,kxk,kxk
        attn = attn * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).permute(0, 1, 4, 3, 2).reshape(
            B, C * self.kernel_size * self.kernel_size, h * w)
        x = F.fold(x, output_size=(H, W), kernel_size=self.kernel_size,
                   padding=self.padding, stride=self.stride)

        x = self.proj(x.permute(0, 2, 3, 1))
        x = self.proj_drop(x)

        return x
</code></pre><h3>多头机制的实现</h3><p>多头机制的实现十分简单，假设设置头数为$N$，只需要调整$W_A\in \mathbb{R}^{C\times K^4}\rightarrow W_A\in \mathbb{R}^{C\times N\cdot K^4}$，最后生成$N$个$A_n\in\mathbb{R}^{H\times W\times K^4},V_n\in\mathbb{R}^{H\times W\times C_N}$，其中$C_N\times N=C$，分别计算最后Concat起来</p><h3>Patch Embedding</h3><p>Patch Embedding最初应该是源于ViT，类似于池化，通过卷积的线性变换将特征图中的一小块（patch）进行映射，最终实现一种下采样的效果</p><p>不同与池化的粗暴，Patch Embedding能一定程度上保留信息，扩大感受野</p><p>同时，可以减少后续模块的计算量</p><p>其实现方法通过控制卷积核大小和步长来实现，VOLO实现的代码如下</p><pre><code class="language-python">class PatchEmbed(nn.Module):
    &quot;&quot;&quot;
    Image to Patch Embedding.
    Different with ViT use 1 conv layer, we use 4 conv layers to do patch embedding
    &quot;&quot;&quot;

    def __init__(self, img_size=224, stem_conv=False, stem_stride=1,
                 patch_size=8, in_chans=3, hidden_dim=64, embed_dim=384):
        super().__init__()
        assert patch_size in [4, 8, 16]

        self.stem_conv = stem_conv
        if stem_conv:
            self.conv = nn.Sequential(
                nn.Conv2d(in_chans, hidden_dim, kernel_size=7, stride=stem_stride,
                          padding=3, bias=False),  # 112x112
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 112x112
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 112x112
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU(inplace=True),
            )

        self.proj = nn.Conv2d(hidden_dim,
                              embed_dim,
                              kernel_size=patch_size // stem_stride,
                              stride=patch_size // stem_stride)
        self.num_patches = (img_size // patch_size) * (img_size // patch_size)

    def forward(self, x):
        if self.stem_conv:
            x = self.conv(x)
        x = self.proj(x)  # B, C, H, W
        return x

</code></pre><p>不同与ViT使用一层卷积进行嵌入，本文使用四层，前三层提取一定的特征，最后一层将整张feature map分为patch_size个部分，形状变为$patch<!-- -->_<!-- -->size\times patch<!-- -->_<!-- -->size$</p><p>$Patch\ Embedding$在此起着关键的作用，其不但降低了注意力模块所需的计算量，其次，能够在一定程度上起到聚合临近信息的作用，因为$Outlook\ Attention$​的生成是仅仅在通道上使用线性转换，其感受野实际上为$1$，$Patch\ Embedding$使得感受野大大增加，虽然$Outlook\ Attention$只是注意中心像素几个邻近像素，但是其在原图上的感受野十分大。</p><p>当然，在$Value\quad Generation$中也得到了局部邻近信息的聚合。</p><h3>网络结构</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210723195055970.png" alt="image-20210723195055970"/></p><h3>Attention</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210730004036.png" alt="image-20210730004022377"/></p><p>在$Self\ Attention$​中，Q，K，V都是输入本身的线性变换</p><p>在$EXternal\ Attention$中，Q为输入本身的线性变换，而K和V是引入的参数</p><p>在$Outlook\ Attention$​中，Q为输入本身，V为输入本身的线性变换，而K是引入的参数</p><h3>其他</h3><p>实际上这篇论文与<a href="https://arxiv.org/abs/2103.06255"><em>Involution: Inverting the Inherence of Convolution for Visual Recognition</em></a>十分相似，在<strong>Involution</strong>一文中，提出了更为广泛的方法，本文可以看作是Involution的一种实例</p><p>对于这个情况，作者是如此回应的：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210723210045503.png" alt="image-20210723210045503"/></p><p>也就是上文所说的$K^4$的原因</p><p>两篇文章的不同之处在于$Involution$将这种方法视为一种新型的卷积，而本文则是视其为一种注意力模块，实际上二者存在着某些异曲同工之妙，譬如本文中的多头机制则对应着$Involution$中的分组(并不完全相同)</p><p>不同之处又在于，本文学习了$ViT$​的$patch\ embedding$方法，减少了注意力模块中的计算量，并且改善了“卷积核”的生成仅与中心像素有关的情况（或许通过不断学习，卷积核能够建模中心像素与临近像素的关系）。</p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polarized Self-Attention - Towards High-quality Pixel-wise Regression]]></title>
        <id>Polarized Self-Attention - Towards High-quality Pixel-wise Regression</id>
        <link href="https://ml.akasaki.space/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：Polarized Self-Attention: Towards High-quality Pixel-wise Regression]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="https://arxiv.org/pdf/2107.00782.pdf">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</a></p><p>作者：Huajun Liu,  Fuqiang Liu, Xinyi Fan</p><p>Code：<a href="https://github.com/DeLightCMU/PSA">https://github.com/DeLightCMU/PSA</a></p></blockquote><p>这篇笔记的写作者是<a href="https://github.com/asthestarsfalll">AsTheStarsFall</a>。</p><h2>摘要</h2><p>细粒度的像素级任务（比如语义分割）一直都是计算机视觉中非常重要的任务。不同于分类或者检测，细粒度的像素级任务要求模型在低计算开销下，能够建模高分辨率输入/输出特征的长距离依赖关系，进而来估计高度非线性的像素语义。$CNN$​​​中的注意力机制能够捕获长距离的依赖关系，但是这种方式十分复杂且<strong>对噪声敏感</strong>。</p><p>本文提出了即插即用的极化自注意力模块，该模块包含两个关键设计，以保证高质量的像素回归：</p><ol><li>极化滤波（$Polarized\ filtering$​）：在通道和空间维度保持比较高的分辨率（在通道上保持$C/2$​的维度，在空间上保持$<!-- -->[H,W]<!-- -->$​的维度 ），进一步减少低分辨率、低通道数和上采样造成的信息损失。</li><li>增强（$Enhancement$​）：采用细粒度回归输出分布的非线性函数。</li></ol><h2>相关工作</h2><h3>逐像素回归任务</h3><p>用于像素回归的$DCNN$​​​的进展基本上都是追求更高的分辨率，目前有相当多的网络挑战如何保持图像的高分辨率，研究人员也越来越认识到高分辨率信息的重要性，$PSA$​​​将从注意力的角度进一步追求上述努力的高分辨率目标，并进一步推动$DCNN$​​​的发展。基于双重注意力机制,本文针对$Pixel-wise\ regression$​​的任务,提出了一种更加精细的双重注意力机制——极化自注意力</p><h3>自注意力机制</h3><p>注意力机制已经被引入到许多视觉任务中，以弥补标准卷积的缺点。自注意力在序列建模和生成性建模任务中获得成功后，已经成为捕获长距离关系的标准模块。已有工作证明，拥有足够数量头的多头自注意力层可以至少与任何卷积层具有相同的表现力。$PSA$促进了逐像素回归的自注意力。</p><h2>方法</h2><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210805155514030.png" alt="image-20210805155514030"/></p><p>上图显示了一些注意力模块在通道和空间维度上的分辨率和时间复杂度，可以看到，$NL$​在通道和空间维度的分辨率很高，但是其时间复杂度也很高；剩下的模块虽然计算量较小，但是很难做到保持高分辨率。</p><p>而本文提出的$PSA$​​可以在保持高分辨率的情况下，实现较低的时间复杂度</p><h3>Polarized Self-Attention (PSA) Block</h3><p>作者在$PSA$中采用了一种极化滤波（$polarized filtering$）的机制。类似于光学透镜过滤光一样，每个$SA$​的作用都是用于增强或者削弱特征。（在摄影时，所有横向的光都会进行反射和折射。极化滤波的作用就是只允许正交于横向方向的光通过，以此来提高照片的对比度。 由于在滤波过程中，总强度会损失，所以滤波后的光通常动态范围较小，因此需要额外的提升，用来恢复原始场景的详细信息。）</p><p>其实就是编故事（X），简而言之，$PSA$​​模块分别在空间和通道维度上进行注意力的计算，其结构如下所示，有两种排列方式：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210805165646225.png" alt="image-20210805165646225"/></p><p>在进一步介绍模块结构前，先了解一下各个符号的含义：</p><p>$X\in\mathbb{R}^{C\times H\times W}$​​​​​​表示输入，$W$​​​​​​表示$1×1$​​​​​​卷积，$θ<em>i$​​​​​​表示$reshape$​​​​​​操作，$\sigma$​​​​​​表示$Sigmoid$​​​​​​，$LN$​​​​​​表示层归一化，$F</em>{SM}=\sum<em>{j=1}^{N_p}\frac{e^{x_j}}{\sum</em>{m=1}^{N<em>p}e^{x_m}}x_j$​​​​​​​，这其中$x_j$​​​​表示$X$​​​的第$i$​​​个通道上的特征图，也就是在通道维度上进行$Softmax$​,$F</em>{GP}$​​表示全局平均池化。</p><h3>Channel-only branch</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210805194024766.png" alt="image-20210805194024766"/></p><p>通道注意力可以表示为：
$$
A^{ch}(X)=\sigma\bigg[LN\bigg[W<em>z\bigg[\bigg(\theta_1(W_v(X)\bigg)\times F</em>{SM}\bigg(\theta_2\big(W_q(X)\big)\bigg)\bigg]\bigg]\bigg]
$$
其主要流程为：</p><ul><li><p>生成$Q$​​​并计算相似度​：通过$1\times 1$​​​卷积降低通道数至$1$​​​​，使用$Reshape$​​​操作代替$K^TQ$​​​​，与通道注意力的全局平均池化降低了大量分辨率不同，这里的$Softmax$​​​​给出了空间维度上各个像素点的相对重要性（并不能称之为相似度吧）并且起到一种归一化的作用。</p></li><li><p>生成$V$​​​​​：通过$1\times 1$​​​​​卷积降低通道数至$C/2$​​​​​，使用$Reshape$​​​​​​​​​​操作方便与相似度进行矩阵乘法，同时聚集了各个通道上的信息，也就是说这里在每个通道上保持高分辨率的同时完成了注意力的计算，具体演示如下：
$$
VQ=
\left<!-- -->[\begin{array}
{c}
x<em>{1,1}&amp;x</em>{1,2}&amp;\cdots&amp;x<em>{1,hw}<!-- -->\<!-- -->
x</em>{2,1}&amp;x<em>{2,2}&amp;\cdots&amp;x</em>{2,hw}<!-- -->\<!-- -->
\vdots&amp;\vdots&amp;\ddots&amp;\vdots<!-- -->\<!-- -->
x<em>{\frac{c}{2},1}&amp;x</em>{\frac{c}{2},2}&amp;\cdots&amp;x_{\frac{c}{2},hw}<!-- -->\<!-- -->
\end{array}\right]<!-- -->
\left<!-- -->[\begin{array}
{c}
y<em>{1,1}<!-- -->\<!-- -->
y</em>{1,2}<!-- -->\<!-- -->
\vdots<!-- -->\<!-- -->
y_{1,hw}
\end{array}\right]<!-- -->
$$
$V$​​的每一行都代表着每一个通道上的空间信息，与$Q$进行加权。</p></li><li><p>得到通道注意力：使用$1\times 1$​​卷积降维，$LN$​进行归一化，$Simoid$​​负责最后激活，最终得到一个$C\times 1\times 1$​​​的通道注意力权重。</p></li></ul><p>可能的代码：</p><pre><code class="language-python">class ChannelAttention(nn.Module):
    def __init__(self, in_ch):
        super(ChannelAttention).__init__()
        self.wq = nn.Conv2d(in_ch, 1, 1)
        self.wv = nn.Conv2d(in_ch, in_ch//2, 1)
        self.softmax = nn.Softmax(dim=1)
        self.wz = nn.Conv2d(in_ch//2, in_ch, 1)
        self.ln = nn.LayerNrom(in_ch)

    def forward(self, x):
        b, c, h, w = x.size()
        q = self.wq(x).reshape(b, h*w, 1, 1)
        v = self.wv(x).reshape(b, c//2, -1)
        z = self.wz(torch.matmul(v, self.softmax(q))
        out=torch.sigmoid(nn.ln(z))
        return out
</code></pre><p>其实这和空间注意力的计算有某些相似之处：</p><ol><li>空间注意力一般在通道维度上进行最大或者平均池化，在这里使用了$1\times 1$​的卷积来聚集信息；</li><li>这里的第二步相当于空间注意力的加权，一般来说，空间注意力在此就已经完成了；</li><li>在加权之后，通过一个空间维度上的求和，来获取每一个通道的权重。</li></ol><p>因此，我们可以写一个类似的代码，更充分的利用空间上的信息来获得通道注意力：</p><pre><code class="language-python">class Channel_Attention_With_Spatial(nn.Module):
    def __init__(self):
        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=false)
        super(Channel_Attention_With_Spatial, self).__init__()
    def forward(self, x):
        b = x.shape[0]
        ave = torch.mean(x, dim=1, keepdim=True)
        m, _ = torch.max(x, dim=1, keepdim=True)
        weight = torch.sigmoid(self.conv(torch.cat([ave, m], dim=1)))
        atten = (x*weight).sum(axis=[2, 3], keepdim=True)
        return atten*x

</code></pre><h3>Spatial-only branch</h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210805194037181.png" alt="image-20210805194037181"/></p><p>空间注意力可以表示为：
$$
A^{sp}(X)=\sigma\bigg<!-- -->[\theta<em>3\bigg(F</em>{SM}\big(\theta<em>1(\sigma(F</em>{GP}(W_q(X))))\times\theta_2(W_v(X))\big)\bigg)\bigg]<!-- -->
$$
其主要流程为：</p><ul><li><p>生成$Q$​​并计算相似度：通过$1\times 1$​​的卷积降低通道数至$C/2$​​，使用全局平均池化聚集信息，使用$Reshape$​​​代替$K^TQ$​​，在最后一维使用$Softmax$​​​​​求解相似度；</p></li><li><p>生成$V$：通过$1\times 1$卷积降低通道数至$C/2$，使用$Reshape$​​​​操作方便与相似度进行矩阵乘法；其计算了每个像素点通道上的注意力，如下图：
$$
QV=
\left<!-- -->[\begin{array}
{c}
x<em>{1,1}&amp;x</em>{1,2}&amp;\cdots&amp;x_{1,\frac{c}{2}}<!-- -->\<!-- -->
\end{array}\right]<!-- -->
\left<!-- -->[\begin{array}
{c}
y<em>{1,1}&amp;y</em>{1,2}&amp;\cdots&amp;y<em>{1,hw}<!-- -->\<!-- -->
y</em>{2,1}&amp;y<em>{2,2}&amp;\cdots&amp;y</em>{2,hw}<!-- -->\<!-- -->
\vdots&amp;\vdots&amp;\ddots&amp;\vdots<!-- -->\<!-- -->
y<em>{\frac{c}{2},1}&amp;y</em>{\frac{c}{2},2}&amp;\cdots&amp;y_{\frac{c}{2},hw}
\end{array}\right]<!-- -->
$$
$Q$​​的每一列都代表着某个像素点上所有通道的信息，与$V$进行加权。</p></li><li><p>得到空间注意力：$Reshape$并使用$Sigmoid$​进行最后的激活，最终得到一个$1\times H\times W$​的空间注意力权重。</p></li></ul><p>可能的代码：</p><pre><code class="language-python">class SpatialAttention(nn.Module):
    def __init__(self, in_ch):
        super(SpatialAttention).__init__()
        self.wq=nn.Conv2d(in_ch, in_ch//2, 1)
        self.wv=nn.Conv2d(in_ch, in_ch//2, 1)
        self.gp=nn.AdaptiveAvgPool2d((1, 1))
        self.softmax=nn.Softmax(dim=-1)

    def forward(self, x):
        b, c, h, w=x.size()
        q=self.gp(self.wq(x)).reshape(b, 1, c//2)
        v=self.wv(x).reshape(b, c//2, -1)
        z=torch .matmul(nn.softmax(q), v).reshape(b, 1, h, w)
        return torch.sigmoid(z)
</code></pre><p>同理，这里与上文类似，也是类似于通道注意力：</p><ol><li>使用全局平均池化聚集信息</li><li>不过这里没有使用$MLP$​来建模通道间关系，而是直接进行加权，并且在通道维度上进行了求和</li><li>将上述结果作为注意力</li></ol><p>同样地，我们也可以使用经典的SEnet来实现这样的操作：</p><pre><code class="language-python">class Spatial_Attention_With_Channel(nn.Module):
    def __init__(self, in_ch, ratio=16):
        super(Spatial_Attention_With_Channel, self).__init__()
        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))
        self.fc1 = nn.Conv2d(in_ch, in_ch//ratio, 1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(in_ch//ratio, in_ch, 1, bias=False)

    def forward(self, x):
        ave = self.fc2(self.relu(self.fc1(self.global_avg(x))))
        atten = (x*ave).sum(axis=1, keepdim=True)
        return x*atten
</code></pre><h3>Composition</h3><p>并列：
$$
\begin{align}
PSA_P(X)&amp;=Z^{ch}+Z^{sp}<!-- -->\<!-- --> \tag1
&amp;=A^{ch}(X)\cdot X+A^{sp}(X)\cdot X
\end{align}
$$
顺序：
$$
\begin{align}
PSA_s(X)&amp;=Z^{sp}(Z^{ch})<!-- -->\<!-- --> \tag2
&amp;=A^{sp}(A^{ch}(X)\cdot X)\cdot A^{ch}(X)\cdot X
\end{align}
$$</p><h2>总结</h2><p>$Polarized$​翻译作<code>极化</code>或许不是那么贴切，可能翻译作<code>偏振</code>更符合本文的思想</p><p>本文的主要贡献是计算注意力的同时，在通道和空间上保持着高分辨率</p><p>但是我觉得实际上就是把空间注意力或者通道注意力向后多推了一步，即在计算空间注意力时，使用的是通道注意力的方法，在以往通道注意力得到的$Refined\ Feature$​的基础上，进行通道上的求和，将最后结果作为$Spatial\ Attention$；本文的通道注意力也是如此，这样或许能够在计算这二者时，更好地利用其他维度的信息吧。</p>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks]]></title>
        <id>SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</id>
        <link href="https://ml.akasaki.space/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[论文名称：SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks]]></summary>
        <content type="html"><![CDATA[<blockquote><p>论文名称：<a href="http://proceedings.mlr.press/v139/yang21o/yang21o.pdf">SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></p><p>作者：<a href="https://zjjconan.github.io/"><strong>Lingxiao Yang</strong></a>, <a href="https://ruyuanzhang.github.io/">Ru-Yuan Zhang</a>, <a href="https://github.com/lld533">Lida Li</a>, <a href="http://sdcs.sysu.edu.cn/content/2478">Xiaohua Xie</a></p><p>Code：<a href="https://github.com/ZjjConan/SimAM">https://github.com/ZjjConan/SimAM</a></p></blockquote><h2>介绍</h2><p>本文提出了一种简单有效的3D注意力模块，基于著名的神经科学理论，提出了一种能量函数，并且推导出其快速解析解，能够为每一个神经元分配权重。主要贡献如下：</p><ul><li>受人脑注意机制的启发，我们提出了一个具有3D权重的注意模块，并设计了一个能量函数来计算权重；</li><li>推导了能量函数的封闭形式的解，加速了权重计算，并保持整个模块的轻量；</li><li>将该模块嵌入到现有ConvNet中在不同任务上进行了灵活性与有效性的验证。</li></ul><h2>相关工作</h2><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210722184726269.png" alt="image-20210722184726269"/></p><p>就目前而言，网络表达能力的提升主要体现在两个方面：<strong>网络结构</strong>和<strong>注意力模块</strong></p><p><strong>网络结构</strong>：从AlexNet到VGG再到ResNet越来越深的网络堆叠结构，或是GoogLeNet等更宽的结构赋予了网络更强的表达能力，也有相当多的工作使用AutoML来避免人工设计。</p><p>然而，作者的目标是设计一个轻量级的即插即用的模块以应用于各种任务，而无需对网络结构做出较大的改变</p><p><strong>注意力模块</strong>：</p><p>以SENet为代表的通道注意力模块或是一些空间注意力模块，虽然取得了不错的效果，但是其计算权重的算法多是人工设计，需要大量的计算量，当然也有一些模块受到神经科学的启发。</p><p><strong>局限性</strong>：对于通道注意力/空间注意力来说，他们往往只是对不同通道/位置区别对待，而对所有的位置/通道同等对待；并且其往往需要额外的子网络来生成权重。</p><h2>方法</h2><p>作者认为注意机制的实现应该遵循神经计算中的一些统一原则。因此，基于一些成熟的神经科学理论提出了一种新的方法。</p><p>在视觉神经学中，那些信息量（most informative）最大的神经元通常与周围神经元拥有不同的放电模式。</p><p>同时，一个活跃的神经元也可能一直周围的神经元活动，这种现象被称为”空间抑制“。</p><p>换言之，在视觉中，表现出明显空间一直效应的神经元应该被赋予更高的重要性，而找到这些神经元的最简单方式就是测量一个目标神经元与其他神经元之间的线性可分性。</p><h3>能量函数</h3><p>基于以上科学发现，提出了以下的能量函数（公式来源<a href="https://blog.csdn.net/liangdas/article/details/44251469">参考</a>）：
$$
e<em>t(w_t,b_t,\mathbf{y},x_i) = (y_t-\hat{t})^2+\frac{1}{M-1}\sum</em>{i=1}^{M-1}(y_0-\hat{x}_i)^2.\tag{1}
$$
$t$和$x_i$是输入$X\in \mathbb{R}^{C\times H\times W}$中单通道上的目标神经元和其他神经元</p><p>$\hat{t}=w_tt+b_t$和$\hat{x}_i=w_tx_i+b_t$是$t$和$x_i$的线性变换，$w_t$和$b_t$分别代表线性变换的权重和偏置</p><p>$i$是空间维度上的索引，$M=H\times W$代表该个通道上神经元的个数</p><p>$(1)$式中的所有量都是标量，当$y_t=\hat{t}$和所有$x_i=y_o$时取得最小值，其中，$y_t$和$y_o$是两个不同的值</p><p>求解$(1)$式的最小值等价于求解目标神经元和其他所有神经元之间的线性可分性</p><p>简便起见，使用二值标签，即$y<em>t=1\quad y_o=-1$，并且添加了正则项，则最终的能量函数如下：
$$
e_t(w_t,b_t,\mathbf{y},x_i) = \frac{1}{M-1}\sum</em>{i=1}^{M-1}(-1-(w_tx_i+b_t))^2+(1-(w_tt+b_t))^2+\lambda w_t^2.\tag2
$$</p><p>公式的来源应该是SVM，将当前神经元设置为正类，其余神经元设置为负类，来衡量他们之间的差异性。</p><h3>解析解</h3><p>理论上， 每个通道拥有$M$个能量函数，逐一求解是很大的计算负担</p><p>幸运的是，可以获得$(2)$的闭式解（即解析解），如下：
$$
w_t=-\frac{2(t-\mu_t)}{(t-\mu_t)^2+2\sigma_t^2+2\lambda},\tag3
$$
$$
b_t=-\frac{1}{2}(t-\mu_t)w_t.\tag4
$$</p><p>其中$\mu<em>t=\frac{1}{M-1}\sum</em>{i=1}^{M-1}x<em>i$，$\sigma_t^2=\frac{1}{M-1}\sum</em>{i=1}^{M-1}(x_i-\mu_t)^2$，实际上就是该通道中除去目标神经元的均值和方差</p><p>由于解析解是在单个通道上获得的，因此可以合理假设每个通道中所有像素遵循相同的分布，最小能量即为：
$$
e_t^<em>=\frac{4(\mu^2+\lambda)}{(t-\mu)^2+2\sigma^2+2\lambda}.\tag5
$$
<strong>能量越低，神经元t与周围神经元的区别越大，重要性越高</strong>。因此，神经元的重要性可以通过$1/e_t^</em>$得到。</p><p>根据以往的神经学研究，哺乳动物大脑中的注意力调节通常表现为神经元反应的增益效应，因此使用放缩运算而非加法来实现加权：
$$
\widetilde{X}=sigmoid(\frac{1}{E})\otimes X,\tag6
$$
同时$sigmoid$函数还可以限制$E$中的过大值，并且不会影响每个神经元的相对重要性</p><p>Pytorch代码为：</p><pre><code class="language-python">def forward(X,lambda):
    n = X.shape[2] * X.shape[3] - 1
    d = (X - X.mean(dim=[2,3])).pow(2)
    v = d.sum(dim=[2,3])/n
    E_inv = d / (4 * (v + lambda)) +0.5
    return X * torch.sigmoid(E_inv)
</code></pre><h2>实验</h2><h1>在各类任务上都取得了相当好的效果。</h1>]]></content>
        <author>
            <name>AsTheStarsFall</name>
            <uri>https://github.com/AsTheStarsFalll</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOLO - Segmenting Objects by Locations]]></title>
        <id>SOLO - Segmenting Objects by Locations</id>
        <link href="https://ml.akasaki.space/blog/[36]SOLO-Segmenting-Objects-by-Locations"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, Lei Li]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+X">Xinlong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kong%2C+T">Tao Kong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen%2C+C">Chunhua Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang%2C+Y">Yuning Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+L">Lei Li</a></p><blockquote><p>We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the &#x27;detect-thensegment&#x27; strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of &quot;instance categories&quot;, which assigns categories to each pixel within an instance according to the instance&#x27;s location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.</p></blockquote><p>实例分割相比于语义分割，不仅需要预测出每一个像素点的语义类别，还要判断出该像素点属于哪一个实例。以往<strong>二阶段</strong>的方法主要是：</p><ol><li>先检测后分割：例如Mask R-CNN ，先用检测的方法到得每一个实例，然后对该实例进行语义分割，分割得到的像素都属于此实例。</li><li>先分割后分类：先采用语义分割的方法对整个图的所有像素点做语义类别的预测，然后学习一个嵌入向量，使用聚类方法拉近属于同一实例的像素点，使它们属于同一类（同个实体）。</li></ol><p>单阶段方法（Single Stage Instance Segmentation）方面的工作受到单阶段目标检测的影响大体上也分为两类：一种是受one-stage, anchot-based检测模型如YOLO，RetinaNet启发，代表作有YOLACT和SOLO；一种是受anchor-free检测模型如 FCOS 启发，代表作有PolarMask和AdaptIS。上述这些实例分割的方法都不那么直接，也不那么简单。SOLO的出发点就是做更简单、更直接的实例分割。</p><p>基于对MSCOCO数据集的统计，作者提出，验证子集中总共有36780个对象，其中98.3％的对象对的中心距离大于30个像素。至于其余的1.7％的对象对，其中40.5％的大小比率大于1.5倍。在这里，我们不考虑像X形两个物体这样的少数情况。总之，<strong>在大多数情况下，图像中的两个实例要么具有不同的中心位置，要么具有不同的对象大小</strong>。</p><p>于是作者提出通过物体在图片中的<strong>位置</strong>和<strong>形状</strong>来进行实例的区分。同一张图片中，位置和形状完全相同，就是同一个实例，由于形状有很多方面，文章中朴素地使用尺寸描述形状。</p><p>该方法与 Mask R-CNN 实现了同等准确度，并且在准确度上优于最近的单次实例分割器。</p><hr/><h2>YOLO如何描述目标</h2><p>YOLO的设计思路是的CNN网络将输入的图片分割成$S\times S$​个网格，然后每个单元格负责去检测那些中心点落在该格子内的目标：</p><p><img src="./src/SOLO-Segmenting-Objects-by-Locations/image-20210812171831245.png" alt="image-20210812171831245"/></p><p>可以看到上图中目标“狗”的中心落在左下角一个单元格内，那么该单元格负责预测该目标。每个单元格具有一定信息量（这些信息量堆叠在channel上)：</p><ul><li><p>每个单元格会预测$B$​​（bounding box）个边界框，每个边界框包含位置$x,y$​和大小$w,h$​，以及这个bounding box的置信度$c$​（confidence）；</p></li><li><p>每个单元格还要给出对$C$​（num of classes）个类别的概率值。其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率（实际上是在各个边界框置信度下的条件概率）。值得注意的是，不管一个单元格预测多少个边界框，其只预测一组类别概率值，这是Yolo算法的一个缺点。所以从YOLO9000开始，类别概率和置信度一样绑定在每个框上。</p></li></ul><p>在YOLO V3中，每个单元格后面的channel堆叠的共有$<!-- -->[B\times (4+1+C)]<!-- -->$个值。对于$N\times N$个单元格，输出的信息总量为：
$$
N\times N\times <!-- -->[B\times(4+1+C)]<!-- -->
$$
其中$+4$​表示框的位置和大小，$+1$​表示框的置信度，$+C$​​​​表示类别的置信度。如果认为每个单元格仅出现一个物体，则一个单元格中出现多个物体时，势必会造成忽略。调节$B$的值可以解决这个问题。但是过大的$B$会导致运算量的增大。</p><h2>SOLO如何描述实例</h2><p>本文提出一种新的、非常简单的并且思路像极了YOLO的单阶段实例分割方法，首先对一幅图进行$S\times S$​​​​​个网格的划分，如果某个实例的中心点落入这个网格中，那么这个网格就需要负责这个实例的形状和实例类别。其主体思想在于利用图像中不同实例的位置和尺度信息的不同，来区分出不同的实例。</p><p>简而言之，SOLO将输入分为$S\times S$​​个网格，也就是输出张量的大小为$S\times S\times (S^2 +C)$​​​，其中$S\times S$​​对应每个网格，$S^2+1$​​是channel维度堆叠的信息，$S^2$​​表示这个网格负责的实例的mask​，$C$​​​​表示类别概率。channel维度上的$S^2$和$C$分别由两条网络分支生成。</p><p><strong>SOLO框架的中心思想是将实例分割重新表述为两个同时的、类别感知预测和实例感知掩码生成问题</strong>：</p><p><img src="./src/SOLO-Segmenting-Objects-by-Locations/image-20210812175633495.png" alt="image-20210812175633495"/></p><p>作者将实例分割任务分为两个子任务：类别预测和掩码生成。如上图所示，网络分为Category Branch和Mask Branch作为一个SOLO Head，分别完成类别预测和掩码生成。</p><p>要理解本文的思想，重点就是要理解SOLO提出的<strong>实例类别（Instance Category）</strong>的概念。作者指出，实例类别就是量化后的<strong>物体中心位置（location）</strong>和物体的<strong>尺寸（size)</strong>。</p><p>SOLO的这两个分支前面连接的是FPN骨干网络，它生成不同大小的金字塔特征图，每个级别具有固定数量的通道（通常为 256）。这些feature map用作每个预测头的输入：语义类别和实例掩码。每个Head的权重在不同级别之间共享。</p><h3>Semantic Category: Category Branch</h3><p>SOLO在预测的输出中用C维表示语义类别概率，其中C（num of classes）是类别的数量。我们将输入图像划分为$S\times S$​​​​个网格，故输出空间的维度为$S\times S\times C$​​​。 这种设计基于这样的假设：$S\times S$​​网格的每个单元格必属于一个单独的实例，即只属于一个语义类别。 在推理过程中，C个通道分别表示每个对象实例的类概率。</p><h3>Instance Mask: Mask Branch</h3><p>Mask Branch和Category Branch并行，每个网格单元除了对应一组类别概率也会生成相应的实例掩码。对于输入图像$I$，如果将其划分为$S\times S$个网格，则总共最多有$S^2$个预测掩码。SOLO在输出张量的通道维度上显式地编码掩码信息。</p><p>具体来说，对于输入$I\in R^{H,W}$​​实例掩码输出的维度为$H\times W\times S^2$​​。第$k$​​个通道将负责在网格$(i,j)$​​处分割实例，其中$k = i·S + j$​​（$i$和 $j$​从零开始）。为此，在语义类别和类别不可知掩码之间建立了一对一的对应关系：</p><p><img src="./src/SOLO-Segmenting-Objects-by-Locations/image-20210812235500474.png" alt="image-20210812235500474"/></p><p>SOLO中获得实例掩码的直接方法是采用全卷积网络。然而，传统的卷积操作在某种程度上是空间不变的。空间不变性对于某些任务（例如图像分类）是可取的，因为它引入了足够的鲁棒性。然而，对于实例分割任务，我们需要一个空间变化的模型，或者更准确地说，位置敏感的模型。</p><p>解决方案非常简单：在网络开始时，受Coord-Conv算子的启发，直接将归一化像素坐标提供给网络。具体来说，SOLO使用一个与输入相同大小的张量，其每一位对应一个所在位置像素的座标，这些坐标被归一化为$<!-- -->[−1, 1]<!-- -->$​​。然后将该张量连接到输入特征并传递到以下层。通过简单地让卷积访问它自己的输入坐标，我们将空间功能添加到传统的 FCN 模型中。<code>请注意，为了实现上述目标Coord-Conv 并不是唯一的选择。例如，semi-convolutional也是可以的。不过论文中由于是因为简单且易于实现而使用了Coord-Conv。</code></p><p>如果原始特征张量的大小为$H\times W\times D$，则新张量的大小变为$H\times W\times (D +2)$，其中最后两个通道用于表示$x,y$像素坐标。</p><h3>关于CoordConv</h3><p>在这里对刚才说道的CoordConv作出补充说明。CoordConv是Uber AI Lab：</p><p><img src="./src/SOLO-Segmenting-Objects-by-Locations/image-20210813090143777.png" alt="image-20210813090143777"/></p><p>这份工作的提出原因是卷积神经网络 (CNN) 在一些很简单很直接的任务里面会失守。例如上图中，监督渲染、监督坐标分类，以及监督回归等任务中，卷积导致座标信息丢失对CNN来说都是大难题。</p><p><img src="./src/SOLO-Segmenting-Objects-by-Locations/image-20210813090428559.png" alt="image-20210813090428559"/></p><p>而CoordConv要做的，就在给神经网络的输入里，加入<strong>两个坐标通道</strong>，每一位分别表示对应像素的$x,y$​座标。</p><h3>合并</h3><p>在SOLO中，类别预测和相应的mask自然由它们的参考网格单元关联，即$k = i\cdot S + j$​​​​。 基于此，SOLO直接为每个网格形成最终实例分割结果。原始实例分割结果是通过收集所有网格结果生成的。最后，使用非极大抑制（NMS）获得最终实例分割结果，不需要其他后处理操作。</p><h2>SOLO的网络结构</h2><p>SOLO的整个网络由三个部分组成：backbone、neck、head。例如，使用ResNet50作为backbone、FPN作为neck、solo_head作为head，那么网络结构可以表示为：</p><p><img src="./src/SOLO-Segmenting-Objects-by-Locations/image-20210814213819473.png" alt="image-20210814213819473"/></p><p>SOLO head的前向传播代码中，对每个划分出来的单元格单独进行前向传播。如下是每个单元格进行前向传播的代码：</p><pre><code class="language-python">def forward_single(self, x, idx, eval=False, upsampled_size=None):
        &#x27;&#x27;&#x27;
        :param x: fpn每个level的feature map [N,C,H,W]
        :param idx:  [0,1,2,3,4]中的一个,用来指示当前的level级别
        :param upsampled_size: 最大feature map/C1 的h,w
        &#x27;&#x27;&#x27;
        # 因为这里有两个branch，要把输入复制为两份
        ins_feat = x
        cate_feat = x

        # 这里先处理mask分支
        x_range = torch.linspace(-1, 1, ins_feat.shape[-1], device=ins_feat.device) # w --&gt; x
        y_range = torch.linspace(-1, 1, ins_feat.shape[-2], device=ins_feat.device) # h --&gt; y
        # 对x_range, y_range 进行扩充
        # torch.meshgrid（）的功能是生成网格，可以用于生成坐标。函数输入两个数据类型相同的一维张量，两个输出张量的行数为第一个输入张量的元素个数，列数为第二个输入张量的元素个数
        # 其中第一个输出张量填充第一个输入张量中的元素，各行元素相同；第二个输出张量填充第二个输入张量中的元素各列元素相同。
        y, x = torch.meshgrid(y_range, x_range)
        # 将两个坐标扩成4维
        y = y.expand([ins_feat.shape[0], 1, -1, -1])
        x = x.expand([ins_feat.shape[0], 1, -1, -1])
        # 将坐标cancat到feature map的通道上
        coord_feat = torch.cat([x, y], 1)
        ins_feat = torch.cat([ins_feat, coord_feat], 1)
        # 将处理好的新的fearure map送进ins_convs
        for i, ins_layer in enumerate(self.ins_convs):
            ins_feat = ins_layer(ins_feat)
        # 这里将feature map上采样到2H*2W,应该是为了提高mask分割的精度
        ins_feat = F.interpolate(ins_feat, scale_factor=2, mode=&#x27;bilinear&#x27;)
        # 这里获得了mask分支的结果
        ins_pred = self.solo_ins_list[idx](ins_feat)

        # 这里开始处理category分支
        for i, cate_layer in enumerate(self.cate_convs):
            # 如果是第一个conv，则需要进行采样，因为category分支的尺寸是h=w=S
            if i == self.cate_down_pos:
                seg_num_grid = self.seg_num_grids[idx]
                cate_feat = F.interpolate(cate_feat, size=seg_num_grid, mode=&#x27;bilinear&#x27;)
            cate_feat = cate_layer(cate_feat)
        # 这里获得了category分支的结果
        cate_pred = self.solo_cate(cate_feat)

        # 如果使测试模式,
        # 将mask分支的结果取sigmoid,并且上采样到原始C1的尺寸
        # category分支的结果进行points_nms,这个待会再看
        if eval:
            ins_pred = F.interpolate(ins_pred.sigmoid(), size=upsampled_size, mode=&#x27;bilinear&#x27;)
            cate_pred = points_nms(cate_pred.sigmoid(), kernel=2).permute(0, 2, 3, 1)
        return ins_pred, cate_pred
</code></pre><h3>输出是如何产生的</h3><p>SOLO光看论文可能稍微有一点迷糊。还好代码是开源的。通过代码可以更加清晰地了解head部分的操作流程。总结如下：</p><ol><li><p>模型输入backbone中（例如ResNet50），产生特征图（feature map）。</p></li><li><p>特征图输入FPN neck，产生不同level的特征图。实验证明，不同level对SOLO分割不同大小的物体起到了非常大的帮助作用。</p></li><li><p>上述处理的特征图输入SOLO head：</p><ol><li>特征图进入Category branch，输出一维的类别置信度信息。之后会用于极大值抑制。</li><li>特征图输入Mask Branch。对于每个单元格，都要对整个feature map进行处理。因为物体的边界可能会超出单元格的范围。只要物体的中心还在这个单元格里，就还算属于这个单元格。</li></ol><p>上述两个分支处理完成后的结果合并。</p></li><li><p>结果输入进入非极大值抑制，进行筛选</p></li><li><p>输出</p></li></ol><h2>Loss设计</h2><h3>Label Assignment</h3><p>对于类别预测分支，网络需要给出每个$S\times S$​网格的对象类别概率。具体来说，如果网格$(i，j)$​落入任何真实注释蒙版的中心区域，则将其视为正样本；否则，它为负样本。在最近的FCOS等目标检测相关的工作中已经证明了中心采样是有效的，这里SOLO也使用类似的技术进行掩码类别分类。给定ground truth 掩码的质心$(c_x, c_y )$​、宽度$w$​和高度$h$​，中心区域由一个常量$\epsilon$​控制：$(c_x , c_y , \epsilon w,\epsilon h)$​。SOLO的代码中设置$\epsilon = 0.2$​，平均每个ground truth掩码有 3 个正样本。
除了实例类别的标签，我们还为每个正样本提供了一个二进制分割mask。由于存在$S^2$​个网格，因此每个图像都有$S^2$​个输出mask。对于每个正样本，对应的目标二值mask将被注释。可能有人担心mask的顺序会影响mask预测分支，论文中证明了最简单的按行列顺序的排序方法是有效的。</p><h3>Loss Functions（损失函数设计）</h3><p>训练损失函数设计为：
$$
L = L<em>{cate} + λL</em>{mask}
$$
其中$L<em>{cate}$为对语义分割分类的Focal loss，$L</em>{mask}$是对mask预测的loss：
$$
L<em>{mask} = \frac{1}{N</em>{pos}}\sum<em>{k}1</em>{P^<em><em>{i,j&gt;0}d</em>{mask}(m_k,m_k^</em>)}
$$
其中，$i = int(k/s)$ ，$j = k \mod s$，位置索引是从上到下，从左往右产生的。$N_{pos}$是正样本的数量。$p<em>$、$m</em>$分别代表了分类和mask。</p><p>$d<em>{maxk}(\cdot)$​可以采用不同的实现，例如Binary Cross Entropy (BCE)、Focal Loss和Dice Loss。最终出于效率和稳定性，选用了Dice Loss：
$$
L</em>{Dice} = 1-D(p,q)<!-- -->\<!-- -->
D(p,q) = \frac{2\sum<em>{x,y}(p</em>{x,y}\cdot q<em>{x,y})}{\sum</em>{x,y}p^2<em>{x,y} + \sum</em>{x,y}q^2<em>{x,y}}
$$
其中$p,q$​​分别为预测的mask和ground truth mask，$p</em>{x,y},q_{x,y}$​为对应$x,y$​​位置的值。</p><h2>SOLO的有效性</h2><p><img src="./src/SOLO-Segmenting-Objects-by-Locations/image-20210813100527967.png" alt="image-20210813100527967"/></p><p>论文中可视化了$S=12$时网格生成的网络输出。子图$i,j$表示由相应的mask通道（在Sigmoid之后）生成的soft mask预测结果。在这里我们可以看到不同的实例在不同的模板预测通道上激活。通过在不同位置显式地分割实例，SOLO将实例分割问题转换为位置感知的分类任务。</p><p>在每个网格处将仅代表一个实例，并且可以由多个相邻的mask通道来预测一个实例。在推断过程中，使用NMS抑制冗余掩码。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLACT - Real-time Instance Segmentation]]></title>
        <id>YOLACT - Real-time Instance Segmentation</id>
        <link href="https://ml.akasaki.space/blog/[37]YOLACT-Real-time-Instance-Segmentation"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bolya%2C+D">Daniel Bolya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+C">Chong Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao%2C+F">Fanyi Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+Y+J">Yong Jae Lee</a></p><blockquote><p>We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn&#x27;t depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.</p></blockquote><p>YOLACT是You Only Look At CoefficienTs 的简写，其中 coefficients 是这个模型的输出之一，这个命名风格应是致敬了另一目标检测模型 YOLO。</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210818180207356.png" alt="image-20210818180207356"/></p><p>上图：YOLACT的网络结构图。<strong>YOLACT的目标是将掩模分支添加到现有的一阶段（one-stage）目标检测模型</strong>。我个人觉得这是夹在一阶段和二阶段中间的产物。将其分为一阶段的依据是其实现“将掩模分支添加到现有的一阶段目标检测模型”的方式与Mask R-CNN对 Faster-CNN 操作相同，但没有诸如feature repooling和ROI align等明确的目标定位步骤。也就是，<code>定位-分类-分割</code>的操作被变成了<code>分割-剪裁</code>。</p><p>根据评估，当YOLACT 处理$550\times 550$​​​大小的图片时，其速度达到了 33FPS，而互联网上多数视频一般是 30FPS 的，这也就是实时的含义了。这是单阶段比较早的一份工作，虽然这个速度不快但也还行了。</p><hr/><h2>YOLACT网络结构</h2><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210818180207356.png" alt="image-20210818180207356"/></p><p>本文作者将实例分割的复杂任务分解为两个更简单的并行任务，分别是使用FCN生成一组对象无关的分割掩码（称为prototype masks，“原型掩码”）以及在目标检测分支添加额外的head生成某种相关系数（称为mask coefficients，“掩模系数”）。</p><p>可以看出，整个网络共享了一个backbone，使用FPN作为neck，在后面才分为了两个分支。其中上方的分支是加入了一个head用于预测掩模系数的，下放的分支是使用protonet生成原型掩模的。</p><h2>老朋友：backbone+FPN</h2><p>在YOLACT中，输入经过backbone+FPN结构处理后才进入两个分支。或者换句话说，两个分支共享这个backbone。</p><p>Backbone+FPN（neck）是需要获得不同level特征图时常用的提取结构，在很多其他网络中也见得着。例如，在<a href="./%5B36%5DSOLO-Segmenting-Objects-by-Locations">SOLO: Segmenting Objects by Locations</a>中，前序网络也是类似的结构。以resnet101为例：</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210818200548063.png" alt="image-20210818200548063"/></p><p>上图中分割线上方是ResNet的backbone，图中_make_layer是YOLACT代码中的helper函数。在后续结构中会用到backbone在多个level上的特征，也就是左侧标号0、1、2、3的蓝色块。这些不同level的输出进入了FPN。分割线下放是FPN结构，其使用了来自backbone的多级输出，并产生新的多级输出，即图中右下侧黄色三角0、1、2、3、4。</p><p>这种Backbone+FPN的结构能够融合多感受野，浅层低层级特征得到了保留，深层网络的高层级特征也融入了进来。所以这种结构在分割网络中经常使用。</p><h2>prototype mask分支</h2><p>网络结构图中下侧的分支是用于生成图像大小的原型掩码（prototype masks）的网络分支。</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210818205234469.png" alt="image-20210818205234469"/></p><p>上图：YOLACT中两个分支的结构。其中黄色三角0、1、2、3、4分别表示前序的backbone+FPN输出的不同level的特征图。上图中灰色分割线左侧是prototype mask分支的结构。可以看出用于预测原型mask分支的分支使用了了上采样到最大分辨率并再融合的最大特征图（标号0）作为输入，目的是为了获得精细的分割mask。</p><p>最终这个分支的输出有两个：</p><ul><li><code>proto</code>：与mask系数配合使用的分割特征图，形状为（1，138，138，32）</li><li><code>segm</code>：得到一个类似分割的热度图，这里的形状为（1，num_class-1，69，69），估计segm是为了使得网络快速收敛用的。</li></ul><p>在原论文中，作者用超参数$k$描述protonet部分输出的通道数：</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210819165708915.png" alt="image-20210819165708915"/></p><p>上图：protonet的结构。上面的话换个理解，就是对一组输出，prototype mask分支会生成一组共$k$​个mask。这些mask和后面的掩模系数线性组合会产生实例级的分割结果。</p><h2>mask coefficients分支</h2><p>这个分支用于获得检测框和掩模相关系数，对一组输出进行处理的是一个head。上方的机构图中灰色分割线右侧是该分支结构图。</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210818205345171.png" alt="image-20210818205345171"/></p><p>上图：对于不同level的特征图输入，分别对应多个head，即对黄色三角代表的0、1、2、3、4输入，分别对应一个这样的结构。多个level上的head处理完后会concat在一起，最终在该分支上产生五种输出：</p><ul><li><code>loc</code>：每个anchorbox的预测偏移量，形状为（1，19248，4）</li><li><code>conf</code>：每个anchorbox的类别预测，形状为（1，19248，num_class）</li><li><code>mask</code>：就是论文中指出的mask系数，形状为（1，19248，32）</li><li><code>priors</code>：预设的anchorbox的坐标，形状为（19248，4）</li></ul><h2>YOLACT如何学习</h2><h3>实例级掩模合成（Mask Assembly）</h3><p>前面我们聊了两个分支。将整个网络结构拼在一起就是：</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/flkjdsfhdsjfkdshgkfjdsglkfdgfdgdfgfd.png" alt="在这里插入图片描述"/></p><p>上图：整个网络前向传播流程。所以我们就有一种简单的理解：YOLACT是进行了一次语义特征图的生成，然后用相关系数和语义特征图产生最终的实例分割结果。</p><p>在原论文的代码中，为了生成实例掩模，通过基本的矩阵乘法配合sigmoid函数来处理两分支的输出，从而合成mask：
$$
Mask = \sigma(PC^T)
$$
其中$P\in h\times w\times k$是原型mask集合（也就是Protonet生成的一组特征图），$C\in n\times k$​是掩模系数（Coefficient）集合，代表有n个通过NMS和阈值过滤的实例，每个实例对应有k个mask系数。</p><h3>Loss设计</h3><p>YOLACT在模型训练时，采用三个Loss对模型进行优化：</p><ol><li>分类Loss，即$L_{cls}$​。可能的选择是类别上做交叉熵。</li><li>边界框回归Loss，即$L_{box}$。可能的选择是IOU系列Loss。</li><li>语义分割的掩模loss，即$L<em>{maxk}$​。计算方式是二值交叉熵$L</em>{mask} = BCE(M,M_{gt})$。</li></ol><p>上述loss中前两个作用于掩模相关系数分支，第三个作用于原型掩模产生的分支。</p><h3>Mask剪裁的细节</h3><p>在前向传播时，YOLACT会使用bounding box裁剪掩模形成实例分割结果。值得注意的细节是，裁剪使用的bounding box在预测和训练时是不同的。为了使两个分支的训练不过多地相互干涉，在训练时，裁剪mask使用的是来自ground truth的bounding box，而在预测时才会使用另一个分支产生的bounding box。这样，训练时两个分支的Loss就不会相关。</p><h2>Emergent Behavior</h2><p>阅读过Fully Convolutional Instance-aware Semantic Segmentation的朋友都知道，实例分割任务有一个共识，FCN是平移等变的。这导致在feature localizing能力的缺失。因此，在Mask R-CNN和FCIS等二阶段实例分割方法中，通过将mask预测分支放在第二个stage来解决平移等变带来的问题：都使得它们不需要再处理localize的问题。</p><p>和上述方法不同，YOLACT通过学习在原型掩模生成时会生成一组不同的激活：</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210818224206183.png" alt="image-20210818224206183"/></p><p>例如上图(a)中的纯色红色方块，如果想在FCN中产生(a)下方的激活图，这是不可能的。原因非常简单：</p><blockquote><p>if its input everywhere in the image is the same, the result everywhere in the conv output will be the same.</p></blockquote><p>由于卷积在整个输入上都是权值共享的，卷积核在每个位置的输出为单个值，如果输入一样，输出也会一样。所以对(a)中的纯红色方块，全卷积应该不能产生其下方的这种有变化的激活图。</p><p>在原论文代码中，FCN使用的是ResNet作为Backbone进行特征提取。如果有多个Layer堆叠在一起，并且它们都使用了padding，就会产生一个有趣的现象：</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210818231430047.png" alt="image-20210818231430047"/></p><p>上图：padding对下一层的影响。例如，我们取卷积核大小为$3\times 3$​，$padding = 1$​​，图中蓝色表示依然平移不变的输入，橘黄色表示当前层的padding像素，黄色表示受之前层中padding像素影响的像素。不难想象，随着层数的加深，这种padding带来的影响可以描述“某个像素与边界的距离”。</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210819000815145.png" alt="image-20210819000815145"/></p><p>上图：自己做的一个实验。在经历了五层卷积后，一个纯色的方块输入在无padding和使用0进行padding的情况下展现出差异。</p><p>在原论文中，作者描述了YOLACT中原型掩模的有效性：</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210819164117962.png" alt="image-20210819164117962"/></p><p>在上图中，每一列mask就是一组原型掩模。可以看出，在经过学习后，该分支的输出对实体的类别和实体本身都展现出了特异性。例如，在mask编号1-3中，掩模表现出方向相关性，即mask只有沿一条曲线的一侧被激活；而再mask编号4-6中，掩模表现出类别敏感，例如在输入f对应的掩模mask编号5和6分别激活了非前景的部分和地面的部分。</p><p><img src="./src/YOLACT-Real-time-Instance-Segmentation/image-20210819165708915.png" alt="image-20210819165708915"/></p><p>上图：protonet的结构。在上面的实验中使用的编号1-6的mask，可以理解为在protonet中将$k$​设置为6，从而产生了一组6个原型掩模。作者实验说明了即使设置$k=32$​模型的性能也不会降低。或者说，增加$k$很可能是无效的，因为预测掩模系数很困难。如果网络即使在一个系数中出现大错误，由于线性组合的性质，生成的掩码可能会消失或包括来自其他对象的泄漏。因此，网络必须发挥平衡作用以产生正确的系数，而添加更多原型会使这变得更加困难。</p><p>在实验中作者发现对于更高的$k$​值，网络只是简单产生了边缘级别变化，这使得AP95略微上涨，但除此之外并没有太多其他用途。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Look One-level Feature]]></title>
        <id>You Only Look One-level Feature</id>
        <link href="https://ml.akasaki.space/blog/[38]You-Only-Look-One-level-Feature"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, Jian Sun]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Q">Qiang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yingming Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+T">Tong Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+J">Jian Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a></p><blockquote><p>This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - {\em utilizing only one-level feature for detection}. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5× faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7× less training epochs. With an image size of 608×608, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4. Code is available at <a href="https://github.com/megvii-model/YOLOF">this https URL</a>.</p></blockquote><p>本文简称YOLOF。截至到本文写作时，二阶段和单阶段目标检测的SOTA方法中广泛使用了多尺度特征融合的方法。FPN方法几乎已经称为了网络中理所应当的一个组件。</p><p>本文中作者重新回顾了FPN模块，并指出FPN的两个优势分别是其分治（divide-and-conquer）的解决方案、以及多尺度特征融合。本文在单阶段目标检测器上研究了FPN的这两个优势，并在RetinaNet上进行了实验，将上述两个优势解耦，分别研究其发挥的作用，并指出，FPN在多尺度特征融合上发挥的作用可能没有想象中那么大。</p><p>最后，作者提出YOLOF，这是一个不使用FPN的目标检测网络。其主要创新是：</p><ol><li>Dilated Encoder</li><li>Uniform Matching</li></ol><p>该网络在达到RetinaNet对等精度的情况下速度提升了2.5倍。</p><hr/><h2>讨论FPN的作用</h2><p>FPN是多尺度特征融合的经典设计，具有重大的启发意义。</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826204958764.png" alt="image-20210826204958764"/></p><p>上图：一个典型的FPN结构示意图。FPN结构接受来自骨干网络的多level输出作为输入，首先经过$1\times 1$卷积的侧向连接进行通道对齐，然后经过从高级特征到低级特征不断地Upsample和按位相加操作得到融合的特征图。这样的设计在启发意义上使人认为FPN是在进行多级特征融合并由此提升性能。</p><p>同时，FPN的另一个设计动机是为了让不同尺度的目标物体分配到不同level的特征图进行预测，称之为“分而治之”的策略。这样的策略在SSD、YOLOv3等目标检测器中被使用，即使用不同level的特征图做不同尺度的目标检测。</p><p>然而，FPN的代价是在推理时存在内存复制、融合的过程，这会在原网络输出的基础上占用两倍以上的显存空间，同时会导致运算的缓慢。这导致了使用FPN的网络在资源限制下无法处理超大分辨率（例如，2K、4K分辨率或以上）的图片。</p><p>在这里，作者称FPN是一个多输入多输出（Multiple-in-Multiple-out，以下简称MiMo）的编码器（encoder），MiMo使用来自骨干网络的多级特征进行融合，然后给后续的decoder（例如各种detection head）提供多级融合的特征。</p><h2>MiMo、SiMo、MiSo和SiSo</h2><p>在本文中，作者将目标检测网络分为三个组成部分：</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826220312496.png" alt="image-20210826220312496"/></p><p>如上图，这三个组成部分分别是骨干网络（backbone，例如ResNet50），编码器（encoder，例如FPN）以及解码器（decoder，例如yolo head）。</p><p>在本文中，作者将多输入多输出（以下简称MiMo）、单输入多输出（以下简称SiMo）、多输入单输出（以下简称MiSo）和单输入单输出（以下简称SiSo）的encoder进行了检测框AP的对比。</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826214506814.png" alt="image-20210826214506814"/></p><p>上图：四种encoder的输入输出方式示意以及检测框AP。其中C3、C4以及C5分别表示骨干网络下采样到8、16、32倍的特征图；P3~P7代表最终用于检测的输出特征图。实验中使用的输入均产生自ResNet-50，并且上图中MiMo的结构和RetinaNet中使用的FPN结构相同。</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826230926535.png" alt="image-20210826230926535"/></p><p>上图：MiMo、SiMo、MiSo、SiSo的具体结构。</p><table><thead><tr><th>英文缩写</th><th>中文释义</th><th>英文全拼</th></tr></thead><tbody><tr><td>MiMo</td><td>多输入多输出</td><td>Multiple-in-Multiple-out</td></tr><tr><td>SiMo</td><td>单输入多输出</td><td>Single-in-Multiple-out</td></tr><tr><td>MiSo</td><td>多输入单输出</td><td>Multiple-in-Single-out</td></tr><tr><td>SiSo</td><td>单输入单输出</td><td>Single-in-Single-out</td></tr></tbody></table><p>上表：四个缩写对照表。</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826220913659.png" alt="image-20210826220913659"/></p><p>上图：MiMo和SiSo在不同超参（channel数量等）下的性能对比。图中使用不同颜色的柱状图标记了网络的三个组成部分对性能的消耗情况，可以看出，encoder对网络的计算速度具有重大影响。<code>注：上图中最后的YOLOF和导数第二个RetinaNet是不同结构的，因此AP差异较大。</code></p><p>在本文的实验中，作者发现SiSo和MiSo的表现结果并不好，但令人惊奇的是SiMo这种并不会进行任何特征融合的结构却在AP上具有和MiMo对等的精度（相差不到1）因此作者才会提出要只使用一层特征图进行目标检测任务，即舍弃占用大量显存的FPN。</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826221541252.png" alt="image-20210826221541252"/></p><p>上图：SiMo使用的输入输出示意。根据上述实验，仅使用C5作为输出并且不进行任何特征融合的encoder具有和MiMo对等的精度。也就是说，从backbone那里得到的C5特征图已经包含了足够完成目标检测任务的上下文信息。</p><h2>YOLOF的设计</h2><p>根据上述实验，作者提出特征图C5有能力独自承担目标检测任务。于是作者希望仅使用来自backbone的单个特征图C5+SiSo的encoder完成目标检测任务。但是，仅使用C5也导致了两个问题：</p><ol><li><p>被限制了的感受野（receptive field is limited）</p><p>与 C5 特征的感受野匹配的尺度范围是有限的，这阻碍了对不同尺度的物体的检测性能。</p></li><li><p>正样本不均衡（imbalance problem on positive anchors）</p><p>单层特征中稀疏anchor导致的正负样本的不平衡问题，老话题了。</p></li></ol><p>因此，作者使用两种方法来解决这些问题。它们是使用空洞卷积的encoder（Dilated Encoder）和统一匹配解码器（Uniform Matching）。</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826230512567.png" alt="image-20210826230512567"/></p><p>上图：YOLOF的大致结构。可以看出，YOLOF仅使用C5进行目标检测。接下来我们聊一下使用空洞卷积的encoder（Dilated Encoder）和统一匹配解码器（Uniform Matching）的设计。</p><h3>Dilated Encoder 解决 Limited receptive field</h3><p>作者想要通过标准的空洞卷积<strong>（dilated convolution）</strong>提升特征图C5的感受野，但一直下采样虽然可以让 feature 覆盖大的物体，不能很好地捕获小尺寸物体。因此，本文加入残差，将原始的 scale 范围和扩大后的 scale 范围结合起来，得到一个可以覆盖所有物体的 scale 的具有多个感受野<strong>（multiple receptive field）</strong>的输出特征，构建的Dilated Encoder结构如下：</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826231550035.png" alt="image-20210826231550035"/></p><p>上图：本文构造的Dilated Encoder结构。其包括一个$1\times 1$卷积+$3\times 3$卷积的projector以及四个串联的残差块。其中，四个连续的 Residual 模块中对于3x3卷积采用空洞卷积（dilated convolution）且有着不同的 dilate rate。这样以来，encoder就从C5中获得了一个具有更大感受野且不丢失小物体的特征图。<code>个人疑惑：这样做和直接从backbone取出更深的特征图的区别仅仅是空洞卷积的加入吗？</code></p><h3>Uniform Matching 解决 positive anchors&#x27; Imbalance problem</h3><p>目标检测中的样本不平衡问题，是指对于目标检测模型，负样本的个数远多于正样本(标签为背景的检测框比标签为目标的检测框多得多)，同时，负样本中简单负样本的个数也远多于困难负样本。模型训练过程中，需要避免这种不平衡对模型性能的影响，以免模型将所有样本都判断为负样本。</p><ul><li>两阶段网络：两阶段网络中第一阶段的 RPN 与单阶段网络类似，但第二阶段 proposal 的不平衡问题得到了很大缓解，这主要是由于 RPN 处理后，proposal 中的简单负样本大大减少，从而同时减轻了正负样本和困难/简单负样本的不平衡问题。</li><li>单阶段网络：目前绝大多数单阶段网络仍然基于 anchor 。网络 anchor 的正负样本和困难/简单负样本不平衡问题十分突出。</li></ul><p>目前大家熟知的解决样本不均衡问题的解决方法有按比例随机采样（例如Faster R-CNN 中第一阶段 RPN 采用按比例随机采样）、在线难样本挖掘(OHEM, Online Hard Example Mining)、Focal Loss（这大概是最出名的专门处理不均衡的Loss设计的paper了）、IoU 平衡采样。</p><p>在anchor-based检测模型中，定义 positive anchor 的标准通常与 anchor 和 ground truth 的 IoU 有关，在 RetinaNet中，如果 anchor 与所有 ground truth 的最大 IoU 值大于 0.5，则该 anchor 是 positive anchor，此方法作者称之为 Max-IoU matching。</p><p>在 MiMo Encoder 中，anchor在 multi-level 特征图上密集的分布在整个图像上，并且ground truth会在其尺寸对应的 level 的特征图上产生 positive anchors，因此这种分治机制可以让每个尺寸的 ground truth 都能产生足够多的 positive anchors，但在 SiSo Encoder中，由于只有一个 level 特征图，且不采用分治机制（即 single out），会让 positive anchor 数量骤减。</p><p>因此，作者提出 Uniform Matching Strategy：对于每个 ground truth，采用k近邻（k nearest）的 anchors 作为 positive anchors ，这一步保证了所有的 ground truth 都能均匀的匹配到相同数量的 positive anchors 而不受 ground truth 自身 scale 的影响（正样本的平衡也会使得它们对训练的贡献相同），除此之外，在 Uniform Matching 中忽略 IoU&gt;0.7 的 negative anchors 和 IoU&lt;0.15 的 positive anchors。</p><h2>Encoder 设计</h2><p>参考YOLOF的结构：</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826230512567.png" alt="image-20210826230512567"/></p><p>在backbone作者还是采用经典的ResNet和ResNeXt，选取的特征图是C5，通道数为2048且下采样率为32。</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210826231550035.png" alt="image-20210826231550035"/></p><p>上图：本文的encoder结构。encoder中第一步和FPN类似，对backbone的输出使用投影层（由1x1卷积和3x3卷积组成），得到通道数为512的特征图<code>注：FPN中使用1x1卷积投影特征图用于对齐通道</code>。接着，为了获得全尺度感受野，作者这里使用了一种残差模块，它由三个卷积组成，第一个$1\times 1$卷积通道减少4倍，然后一个$3\times 3$膨胀卷积用于增大感受野，最后的$1\times 1$卷积恢复通道维度，这个残差块会重复四次。</p><h2>Decoder的设计</h2><p>decoder和RetinaNet类似，它包含两个并行的head分支，用于目标分类和边框回归任务。作者这里主要做了两个改动：参考DETR中FFN的设计使用两个预测头以及参考AutoAssign动态分配正负样本。</p><h3>使用两个预测头</h3><p>参照DETR（End-to-End Object Detection with Transformers）中FFN的设计，使得两个head卷积层数目不同。</p><p><img src="./src/You-Only-Look-One-level-Feature/v2-84679d261c01da7a28a67fb9509cc1ec_1440w.jpg" alt="End-to-End Object Detection with Transformers-DETR"/></p><p>上图：DETR结构示意。其主要贡献是将目标检测任务转化为一个集合预测（set prediction）的任务，使用transformer编码-解码器结构和双边匹配的方法，由输入图像直接得到预测结果集合。和SOTA的检测方法不同，没有proposal（Faster R-CNN），没有anchor（YOLO），没有center(CenterNet)，也没有繁琐的NMS，直接预测检测框和类别，利用二分图匹配的匈牙利算法，将CNN和transformer巧妙的结合，实现目标检测的任务。</p><p>其中的预测头部（Feed-forward network，FFN）是本文参考的一个设计：</p><p><img src="./src/You-Only-Look-One-level-Feature/image-20210827093728559.png" alt="image-20210827093728559"/></p><p>上图：FFN在DETR结构中出现的位置。在右上角的两个FFN结构不是一样的。这两个FFN通过不同的Loss分别优化，用于分别生成类别和检测框。</p><p>本文在decoder上的两个分支参考了这种设计，在回归（regression）分支中包含4个Conv-BN-ReLU操作，在分类（classification）分支中包含2个Conv-BN-ReLU操作构成两个“FFN”，使用相同的输入完成不同的生成任务。</p><h3>跳出非正即负的assign方式和监督原则</h3><p>依据AutoAssign，回归分支中的每个anchor都有一个objectness prediction，最终的分类得分由分类分支的输出乘以objectness prediction得到。</p><p>AutoAssign认为每个location众生平等（这里包括FPN各个level），每个location都有正样本属性和负样本属性。也就是说，在优化的过程中，有些样本会同时受到来自它为正样本的监督和负样本的监督。</p><p>并且，相较于大多数方法分开优化classification和regression，AutoAssign将两者进行联合，一方面可以更好地简化表示统一优化，另一方面在协助生成正样本置信度的时候可以综合考虑分类和定位的情况。即优化上regression和classification是统一的：
$$
L_i(\theta) = L_i^{cls}(\theta) + \lambda L_i^{loc}(\theta)<!-- -->\<!-- -->
= -\log(P_i(cls|\theta))+\lambda L_i^{loc}(\theta)<!-- -->\<!-- -->
= -log(P_i(cls|\theta)e^{-\lambda L_i^{loc}(\theta)})<!-- -->\<!-- -->
= -\log(P_i(cls|\theta)P_i(loc|\theta))<!-- -->\<!-- -->
= -\log(P_i(theta))
$$
这篇AutoAssign的内容是稍微有点多的，建议单独阅读一下。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-sensitive Fully Convolutional Networks]]></title>
        <id>Instance-sensitive Fully Convolutional Networks</id>
        <link href="https://ml.akasaki.space/blog/[39]Instance-sensitive-Fully-Convolutional-Networks"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai%2C+J">Jifeng Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K">Kaiming He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+Y">Yi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaoqing Ren</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a></p><blockquote><p>Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.</p></blockquote><p>这篇工作又名InstanceFCN。实例分割方面，由于网络难以同时进行分类和分割任务，因此首先流行的是二阶段实例分割网络，首先对输入找到实例的proposal，然后在其中进行密集预测（也就是先框框再分割）。本文从名称上看不是一篇讲实例分割的文章，是讲如何通过FCN获得实例级别的分割mask的的。</p><p>在阅读之前我想提醒一下，这篇工作的效果是比较差的，毕竟是早期工作。不过这篇工作具有不错的启发意义，值得读一读。后面的一篇工作FCIS（Fully Convolutional Instance-aware Semantic Segmentation）中就借鉴了本文中提出的instance-sensitive score maps（请不要弄混本篇工作和FCIS）。本文的一大贡献就是提出使用instance-sensitive score maps区分不同个体。</p><hr/><h2>InstanceFCN</h2><p>大家也称这篇工作为InstanceFCN。这篇工作的功能只是区分了实例，并没有区分类别。这是一篇”两不像“的工作。我们可以称之为”对象分割“：</p><table><thead><tr><th>名称</th><th>描述</th></tr></thead><tbody><tr><td>语义分割</td><td>区分各个像素类别的密集预测任务</td></tr><tr><td>实例分割</td><td>区分各个像素类别及其属于的对象实例的密集预测任务</td></tr><tr><td>对象分割（本篇工作）</td><td>区分各个像素属于的对象实例的密集预测任务</td></tr></tbody></table><p>这篇工作是使用全卷积网络区分像素所属的对象实例的：</p><p><img src="./src/Instance-sensitive-Fully-Convolutional-Networks/image-20210831174047097.png" alt="image-20210831174047097"/></p><p>上图是InstanceFCN的主要结构。可以看出，在特征提取网络之后（原论文中InstanceFCN使用在ImageNet上预训练的修改版VGG-16作为骨干特征提取网络，修改版是指作者对原版的VGG-16做了一些调整避免过度的下采样导致分辨率过度损失。具体可以看一下原论文中算法部分的相关描述），InstanceFCN在骨干特征提取网络之后主要分为两个分支，分别是instance分支（用于产生实例级的像素打分）以及objectness分支（用于产生某位置是不是一个实体的打分）。这两个分支的结果将相乘形成最终的得分。</p><p>接下来我们主要聊一下这两个分支如何工作。</p><h2>Instance分支</h2><p>这个分支的作用产生一个像素与实体关系的打分，可衡量一个像素应该属于哪个实例。这个分支分为两个阶段：</p><ol><li>通过卷积得到instance-sensitive score maps。</li><li>一个assembling module将它们组装称为最终的结果。</li></ol><p>接下来分别介绍这两个阶段。</p><h3>Instance-sensitive score maps</h3><p>instance-sensitive score maps是这份工作的核心贡献之一。InstanceFCN通过这一贡献实现区分不同对象。在语义分割模型中，通常将损失设定为交叉熵，因此每个像素在优化时仅表达一个语义。这对于很多个对象存在重叠的像素难以判断其所属。</p><p>相比于用于语义分割的FCN在最终输出一个类别，InstanceFCN最后的输出是一个像素于对象关系的打分。大致上去理解，就=就是一个像素越可能属于某个对象，这个打分就越高。进行打分的方式来源于一个简单的规则：不同对象应该处在不同位置。因此在原论文中，作者使用$k\times k$的矩阵定义一种”相对位置“：</p><p><img src="./src/Instance-sensitive-Fully-Convolutional-Networks/image-20210831165835721.png" alt="image-20210831165835721"/></p><p>例如在上图中，$k=3$，就会产生$k^2=9$个score map。这些score map被称为instance-sensitive score maps，可以用于对像素和对象的相对位置进行打分。</p><p>这个$k\times k$的矩阵我们暂且称之为”打分器“。这个打分器是一个在全图上移动的滑动窗口，将对全图的各个区域进行打分。还是刚才提到的，作者认为不同的对象处于不同的位置。所以只要打分器在所有位置都打一下分，使每个像素都具有一个跟位置相关的分数，就能区分这个像素到底属于哪个对象。例如上图中有两个类别为人的实体，打分器在左侧的人的位置产生的score maps最终仅拼接出了左侧的人，并没有拼接出右面的。</p><p>根据原论文，前置骨干特征提取网络输出特征图后，首先使用一个512-d的$1\times 1$卷积对特征进行一下转换，然后使用一个$3\times 3$的卷积层产生instance-sensitive score maps：</p><p><img src="./src/Instance-sensitive-Fully-Convolutional-Networks/image-20210831195822063.png" alt="image-20210831195822063"/></p><p>刚才我们提到论文中使用一个$k\times k$的网格打分器描述某种相对位置信息，所以这个用于形成score map的$3\times 3$卷积层卷积核维度就是$k^2$。也就是对于输出，分辨率上每个像素后方堆叠着$k^2$个数字，分别表示这个像素对$k^2$个相对位置的打分。</p><p>这里可能会产生一个小疑问，就是卷积核明明是逐像素的迭代，而且根据上面的描述只有$3\times 3$的大小，怎么可能覆盖图中的区域：</p><p><img src="./src/Instance-sensitive-Fully-Convolutional-Networks/image-20210831201633024.png" alt="image-20210831201633024"/></p><p>例如这幅示意图中，一个$k\times k,k=3$的打分器命名就是一个$3\times 3$的卷积核，如何覆盖一个人的像素范围？我自己的理解是VGG-16都进来卷过了，一个像素就已经能代表一定大小的感受野了。而且在$3\times 3$卷积之前还有一个$1\times 1$的卷积层，训练过程它学了啥全是玄学。这就导致了这部分可解释性比较牵强（也许是我才疏学浅解读失误）。<code>大家都常讲起这个笑话：“通过学习的卷积核能做到的话解释不通也是能解释通的”。如果您有心钻研深度神经网络的可解释性，那么祝您能在这条路上走得顺利👻。有任何实质性的进展请让我也了解一下您的工作</code>。</p><blockquote><p>插一嘴，我在想这样的结构在不同感受野的物体上表现应该不会均衡吧。这样设计实属是把对不同感受野的兼容性抛给了骨干网络和那个不知道为什么存在的$1\times 1$卷积。看了一眼原论文试验结果部分，确实效果一般。会不会换上Res-Net或者U结构的网络后性能会显著提升呢？</p></blockquote><p>这样我们就得到了一份instance-sensitive score maps。不过仅凭此还不能区分出对象，还需要通过一个称为组装（assembling）的过程生成对象级的输出。</p><h3>Instance assembling module</h3><p>instance-sensitive score maps还不足以直接产生对象实例。但是可以通过”组装“score maps来产生对象实例：</p><p><img src="./src/Instance-sensitive-Fully-Convolutional-Networks/image-20210831180137018.png" alt="image-20210831180137018"/></p><p>例如，在上图所示的两个类别为人的对象上，通过卷积打分产生instance-sensitive score maps。我们可以看出，当这个$k\times k ,k=3$的打分器位于左侧的人的位置时，在这个打分器内部的$k^2 = 9$个位置上输出产生了不同的score map（也就是上个阶段网络的输出是$k^2$维的，分别代表当前位置下和这$k^2$个区域的相对位置关系分数）。</p><p>接下来Instance assembling module会用这个$k^2$维的输出“组装”出一个像素级的实例输出。原论文将这个组装的模块描述为一个大小为$m\times m$的窗口（其中m是k的倍数），将这个窗口分为$\frac{m}{k}\times \frac{m}{k}$个子窗口，也就是每个窗口是$k\times k$这么大，每个子窗口直接从score map的相应位置复制对应的值，然后拼在一起（依据它们的相对位置拼在一起）形成$m^2$大小的输出。在原文中，作者选择了$m=21$，步长$stride = 8$。</p><p>作者在原文中重申，这个模块是没有参数的，其中仅包含复制粘贴的操作，不需要学习。但是这个模块仍然是产生$m^2$分辨率输出的“唯一指定组件”（the only component），是网络的重要组件，且特征图由它经手后得到的输出直接参与loss计算，所以其设计影响网络的训练优化过程。</p><blockquote><p>回看刚才我们在Instance-sensitive score maps那里提到的小疑问，InstanceFCN通过卷积产生“位置敏感的”输出仅通过卷积这件事确实难以解释。如果真的是这样，为什么不直接在组装部分也直接用一个可以学习的卷积解决呢？个人认为这个“复制粘贴”模块是解释的一部分。作者在重申的时候也提到了，这个模块影响网络的训练优化。也就是说，它是干预前序网络学习行为的。因为它“对着位置复制粘贴”的行为，在优化网络的时候导致了前面score map的产生是区域敏感的（以上纯属我自己的臆想）。</p></blockquote><p>由于这篇论文并没有提供具体的代码，所以这个assembling模块的具体设计无从知晓。猜测是一个<code>[0,0,...,1,...,0]</code>长相的卷积。</p><h2>Object分支</h2><p>这个分支用于产生Objectness score map，主要用于判别某个位置存在一个实体的可能性。</p><p><img src="./src/Instance-sensitive-Fully-Convolutional-Networks/image-20210902090944392.png" alt="image-20210902090944392"/></p><p>如上图，这个分支和Instance分支是并行的。在原论文的描述中，该分支使用一个$3\times 3$卷积和一个$1\times 1$卷积构成。其中$1\times 1$卷积是一个密集（逐像素）的logistic回归，回归为两类，一类是“背景”，一类是“实体”。</p><p><img src="./src/Instance-sensitive-Fully-Convolutional-Networks/image-20210831174047097.png" alt="image-20210831174047097"/></p><p>接下来这两个分支产生的结果直接相乘得到最终结果。注意，object分支的输出分辨率要和instace分支匹配，否则没法按位相乘。</p><h2>Loss设计</h2><p>$$
\sum<em>i (L(p_i,p_i^*)+\sum_j L(S</em>{i,j},S_{i,j}^*))
$$</p><p>其中$p$的部分是在计算object分支的loss，$S$的部分是在计算intance分支的loss。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning in the Frequency Domain]]></title>
        <id>Learning in the Frequency Domain</id>
        <link href="https://ml.akasaki.space/blog/[40]Learning-in-the-Frequency-Domain"/>
        <updated>2022-11-05T07:47:19.660Z</updated>
        <summary type="html"><![CDATA[Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, Fengbo Ren]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+K">Kai Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin%2C+M">Minghai Qin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+F">Fei Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yuhao Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yen-Kuang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren%2C+F">Fengbo Ren</a></p><blockquote><p>  Deep neural networks have achieved remarkable success in computer vision tasks. Existing neural networks mainly operate in the spatial domain with fixed input sizes. For practical applications, images are usually large and have to be downsampled to the predetermined input size of neural networks. Even though the downsampling operations reduce computation and the required communication bandwidth, it removes both redundant and salient information obliviously, which results in accuracy degradation. Inspired by digital signal processing theories, we analyze the spectral bias from the frequency perspective and propose a learning-based frequency selection method to identify the trivial frequency components which can be removed without accuracy loss. The proposed method of learning in the frequency domain leverages identical structures of the well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN, while accepting the frequency-domain information as the input. Experiment results show that learning in the frequency domain with static channel selection can achieve higher accuracy than the conventional spatial downsampling approach and meanwhile further reduce the input data size. Specifically for ImageNet classification with the same inpu t size, the proposed method achieves 1.41% and 0.66% top-1 accuracy improvements on ResNet-50 and MobileNetV2, respectively. Even with half input size, the proposed method still improves the top-1 accuracy on ResNet-50 by 1%. In addition, we observe a 0.8% average precision improvement on Mask R-CNN for instance segmentation on the COCO dataset.</p></blockquote><p><code>Comments</code>: Accepted to CVPR 2020</p><hr/><h2>概览</h2><p>在传统的CNN结构中，受限于设备的计算性能（也有的时候单纯是为了产生固定长度的输出以适应需要固定大小输入的全连接层），通常会使用固定大小的图像作为输入（例如：224$\times224$），因此通常会先将图片下采样到某个大小。这已经不能满足现代摄像机实际能达到的视觉画面精度了。加之传统CNN通常在空间上对图像进行操作，这样的行为有意无意中将图像的部分高频信息以及显著性信息移除，直接或间接地导致图像信息或精度的丢失（参考<a href="https://arxiv.org/abs/1810.05552">Effects of Image Degradations to CNN-based Image Classification</a>）。这篇文章在Introduction中提到以前的工作通过对特定的视觉任务控制这个downsample的过程来降低信息损失（例如 <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Heewon_Kim_Task-Aware_Image_Downscaling_ECCV_2018_paper.pdf">Task-Aware Image Downscaling</a>）。</p><p>本文的作者受到HVS（human visual system，人类视觉系统）中的一些研究（<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Deep_Learning_of_CVPR_2017_paper.pdf">Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework</a>）以及数字信号处理中一些方法的启发，首先将图像通过某种方法映射到频域内（例如离散余弦变换（discrete cosine transform）或其他方式），获得频域上的特征图后，训练一种“选择器”，其功能是筛选出对最终结果影响较大的频域信息，并移除无关紧要的部分，作为输入后续网络的数据的一部分。</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023123854207.png" alt="image-20211023123854207"/></p><p>上图(a)：一般CNN模型处理图片的流程；上图(b)：本文的方法处理图片的流程（值得一提的是，该方法作为前处理加入网络时，不需要对原来使用RGB图像作为输入的CNN结构做出什么更改）。实验证明，在相同的输入图像精度下，加入该方法的神经网络比直接在空间域上卷积完成的逐层下采样的方法达到了更高的精度，并且在二分之一输入大小的图像上依然如此。	</p><p>从代码上看，本篇论文的贡献主要是一个频域的前处理过程。作者在论文中说明了，将输入转换进频域的前处理几乎可以直接用于任何CNN模型。</p><blockquote><p>  关注到本文，是因为最近研究分割比较多，并且我觉得分割作为一个逐像素的密集预测问题，其所需的编码方式和类别数量巨大的分类问题以及目标检测问题可以分开来讲。抛开实例分割这种除了分类还需要区分实体的人物不谈，在单纯的语义分割中，分类图像的方式基本是通过编码后特征图每个像素对应channel上解码出的概率分布。在分类数量较少时，我认为所需要的编码复杂度并不需要很高。降低编码的复杂度，一方面能够加快参数收敛的速度，另一方面能够降低网络参数两。自从拜读了一些通道注意力相关的文章（例如<a href="./%5B23%5DSqueeze-and-Excitation-Networks">Squeeze and Excitation Networks</a>），我就隐约感到CNN在较长的channel上可能会隐式地编码一些频率信息，只是由于在网络的监督上没有针对这一点进行优化，所以表现得并不明显。此篇文章提出在频域上选择必要的信息，并不是像我想的那样在直接在监督上使网络显式编码频率信息，而是通过前处理使网络在输入上就表现出对频域的偏置。这和我一开始的想法有些出入，非常具有启发意义。</p></blockquote><p>本文的主要贡献如下：</p><ol><li>本文提出了一种不需要更改原有CNN网络结构的频域学习方法，其主要使用“某种频域像关系数”（DCT coefficients as input）作为输入而不是RGB图像。该方法在许多视觉任务上具有出色表现。</li><li>本文通过实验证明，使用在频域上的前处理比直接使用卷积操作进行下采样更能保留图像的信息。</li><li>本文从频域的角度对“光谱偏置”进行了分析并证明了使用RGB图像作为输入的CNN模型对低频信息更加敏感，这和人类的视觉系统表现是贴合的。</li><li>本文提出了一种可学习的动态channel选择器。该选择器通过学习在推理时静态地移除一部分相关性不大的频率分量（通过拦截部分通道）。</li></ol><hr/><h2>提出了频域的数据前处理过程</h2><p>很少有CNN模型会直接将高清图片直接放入GPU或任何计算设备中进行推理。这会导致设备内存、带宽或计算瓶颈。为了解决这个问题，高清图片通常被CPU提前处理成较小的图片输入进网络。这通常会导致一些信息的损失以及精度下降。</p><p>本文中的方法是将RGB色域的图像转换进YCbCr颜色空间，再转换进频域空间。</p><blockquote><p>  YCbCr或Y&#x27;CbCr有的时候会被写作：YCBCR或是Y&#x27;CBCR，是<a href="https://zh.wikipedia.org/wiki/%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93">色彩空间</a>的一种，通常会用于影片中的影像连续处理，或是数字摄影系统中。Y&#x27;和Y是不同的，Y就是所谓的<a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E6%98%8E">流明</a>（<a href="https://zh.wikipedia.org/w/index.php?title=Luminance&amp;action=edit&amp;redlink=1">luminance</a>），Y&#x27;表示光的浓度且为非线性，使用<a href="https://zh.wikipedia.org/wiki/%E4%BC%BD%E7%91%AA%E6%A0%A1%E6%AD%A3">伽马修正</a>（gamma correction）编码处理，而CB和CR则为蓝色和红色的浓度偏移量成分。</p><p>  <strong>YCbCr</strong>不是一种<a href="https://zh.wikipedia.org/wiki/%E7%B5%95%E5%B0%8D%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93">绝对色彩空间</a>，是<a href="https://zh.wikipedia.org/wiki/YUV">YUV</a>压缩和偏移的版本。YCbCr的Y与YUV中的Y含义一致，Cb和Cr与UV同样都指色彩，Cb指蓝色色度，Cr指红色色度，在应用上很广泛，JPEG、MPEG、DVD、<a href="https://zh.wikipedia.org/wiki/%E6%94%9D%E5%BD%B1%E6%A9%9F">摄影机</a>、<a href="https://zh.wikipedia.org/wiki/%E6%95%B8%E4%BD%8D%E9%9B%BB%E8%A6%96">数字电视</a>等皆采此一格式。因此<strong>一般俗称的YUV大多是指YCbCr</strong>。</p><p>  <strong>YUV</strong>，是一种<a href="https://zh.wikipedia.org/wiki/%E9%A1%8F%E8%89%B2">颜色</a><a href="https://zh.wikipedia.org/wiki/%E7%B7%A8%E7%A2%BC">编码</a>方法。常使用在各个影像处理组件中。 YUV在对照片或视频编码时，考虑到人类的感知能力，允许降低色度的带宽。YUV是编译true-color颜色空间（color space）的种类，Y&#x27;UV, YUV, <a href="https://zh.wikipedia.org/wiki/YCbCr">YCbCr</a>，<a href="https://zh.wikipedia.org/wiki/YPbPr">YPbPr</a>等专有名词都可以称为YUV，彼此有重叠。“Y”表示<strong><a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E6%98%8E">明亮度</a></strong>（Luminance、Luma），“U”和“V”则是<strong><a href="https://zh.wikipedia.org/wiki/%E8%89%B2%E5%BA%A6_(%E8%89%B2%E5%BD%A9%E5%AD%A6)">色度</a></strong>、<strong><a href="https://zh.wikipedia.org/wiki/%E6%BF%83%E5%BA%A6_(%E8%89%B2%E5%BD%A9%E5%AD%B8)">浓度</a></strong>（Chrominance、Chroma）</p></blockquote><p>上述内容摘自 Wikipedia 中关于 <a href="https://zh.wikipedia.org/wiki/YCbCr">YCbCr</a> 以及 <a href="https://zh.wikipedia.org/wiki/YUV">YUV </a>的相关内容。本篇论文的方法中没有直接将RGB色域的图像转化进频域空间，而是将数据先放入YCbCr颜色空间是有理由的。也许你会在多媒体技术或者数字图像处理课程上了解过早期电视机的相关内容，在该颜色空间下黑白视频是只有Y（Luma，Luminance）通道的视频，也就是灰阶值。到了彩色电视规格的制定，是以YUV/<a href="https://zh.wikipedia.org/wiki/YIQ">YIQ</a>的格式来处理彩色电视图像，把UV视作表示彩度的C（Chrominance或Chroma），如果忽略C信号，那么剩下的Y（Luma）信号就跟之前的黑白电视频号相同，这样一来便解决彩色电视机与黑白电视机的兼容问题。Y&#x27;UV最大的优点在于只需占用极少的带宽。</p><p>刚才提到“HVS中的一些研究”，作者在频域中对CNN的输入输出进行测试，通过实验分析发现，在分类、检测和分割任务中，CNN模型对低频率的channel更加敏感。这和HVS中的一些研究是贴合的。也就是说，在使用现在主流的数据集进行监督时，CNN在频域上表现出了和人类一样的“低频敏感性”。因此，对于RGB色域上的输入，并不是整个RGB空间内的所有值在CNN模型中都具有重要的作用。</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023123921777.png" alt="image-20211023123921777"/></p><p>上图：在本论文提出的方法中图像前处理的过程。该过程包含如下步骤：</p><ol><li>Spatial resize and crop：将图像转为输入大小。</li><li>DCT transform：将图像从RGB色域转入YCbCr颜色空间，再通过DCT变换转入频域。</li><li>DCT channel select：一个选择器选择的过程，选出通道中对推理具有更大影响的部分。</li><li>DCT concatenate：将刚才在YCbCr的三个通道里做出的频域选择结果拼接成一个张量。</li><li>DCT normalize：利用训练数据集计算的均值和方差对每个频率通道进行归一化处理。</li></ol><h2>提出频域的通道选择器</h2><p>在聊频域通道的选择（原文：Frequency Channel Selection）之前，先回忆起 <a href="./%5B23%5DSqueeze-and-Excitation-Networks">Squeeze and Excitation Networks</a> 中使用Excitation的过程选择channel，在频域的选择问题上，也许可以使用相似的思路。例如，将不同的频域信息堆叠在channel中，使用类似Squeeze的过程产生一组等于channel长度的选择器，决定某个channel是否进入输出。在CNN中是存在对光谱的偏置的（主要体现为输入为RGB图像），因此对于编码好的特征图，可以在channel中选出“比较重要的部分”。</p><blockquote><p>  刚才提到“HVS中的一些研究”，作者在频域中对CNN的输入输出进行测试，通过实验分析发现，在分类、检测和分割任务中，CNN模型对低频率的channel更加敏感。这和HVS中的一些研究是贴合的。也就是说，在使用现在主流的数据集进行监督时，CNN在频域上表现出了和人类一样的“低频敏感性”。</p></blockquote><p>基于上述思考，我猜对于频域上的特征图这样做的理由也是相同存在的。还真猜对了，在经过前处理后，特征图上不同的channel堆叠了不同的频域信息。本篇论文中提出通过学习筛除一部分对最终的结果及误差影响不大的通道。在刚才的前处理中，图像变为了$W\times H\times C$（本文中$C=192$）的频域特征图，输入频率选择器。这篇文章在通道选择上直接使用了类似SE Block的结构：</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023123801381.png" alt="image-20211023123801381"/></p><p>上图：选择器结构（下，文中称之为 $Gate\ Module$）和SE Block（上）的结构对比图。可以看到，除了图中标号1和2，其余部分和两者结构基本相同（注：本文的$F_{ex}(\cdot,W)$过程是$1\times 1$卷积，而不是SE Block中全连接）。在Gate Module的输出中标记为白色的通道代表被过滤的通道（查看<a href="./%5B23%5DSqueeze-and-Excitation-Networks">Squeeze and Excitation Networks</a> 以理解 SE Block ）。</p><p>论文的作者在文中说明了该结构与SE Block的不同之处：在 SE Block 中，对channel的采样是数值的，每个channel会得到一个数值的权重。而本文的Gate Module则对channel进行 $0-1$ 采样。采样的方式是 $Tensor 3$ 通过两套参数变为 $Tensor 4$（shape为$1\times 1\times C\times 2$），然后再进行伯努利分布的采样。原文中是这样描述的：</p><blockquote><p>  The decision is obtained by sampling a Bernoulli distribution $Bern(p)$, where $p$ is calculated by the 2 numbers in the $1 \times 1 \times C \times 2$ tensor mentioned above.</p></blockquote><p>在原SE Block的设计中经过“$F_{ex}$”过程得到 $Tensor 3$ 后直接 $softmax$ 。但是在本文给出的Gate Module中产生了一个问题。上面描述的选择方法中使用了伯努利分布采样，这会产生离散的通道选择决策。当我们采用梯度下降优化时，能够直接优化的是连续量。对于离散量以及在中间过程中出现离散量的网络是难以直接进行梯度下降优化的。因此，在Gate Module中，然后再进行 $Gumbel\ softmax$（ Gumbel softmax distribution 可参考论文<a href="https://arxiv.org/abs/1611.01144">Categorical Reparameterization with Gumbel-Softmax </a>）。$Gumbel\ softmax$ 允许了在具有离散采样的过程中进行反向传播，解决了这个问题。</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023123813633.png" alt="image-20211023123813633"/></p><p>上图：通过类似 SE Block 的结构选取重要的通道。</p><h2>探究哪些通道被选择了</h2><p>为了研究通道选择器的行为，作者使用相同数据集在图像分类和语义分割任务上进行训练，并对频域的选择器选出的channel信息进行了可视化：</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023123725921.png" alt="image-20211023123725921"/></p><p>上图：在 ImageNet（validation set）进行<strong>分类</strong>任务时对选出的通道 YCbCr 组成绘制可视化 heat map。</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023123729829.png" alt="image-20211023123729829"/></p><p>上图：在 COCO（validation set）进行<strong>分割</strong>任务时对选出的通道 YCbCr 组成绘制可视化 heat map。</p><blockquote><p>  阅读上述 heat map 的方法：在 heat map 中每个代表channel方块上具有的index编号表示该通道对应的frequency index。颜色越深表示在前向传播时该channel越容易被选中。</p></blockquote><p>通过观察上述可视化结果，论文的作者提出了以下几个结论：</p><ol><li>低频的（index较小的）通道比高频的（index较大的）通道更容易被选择。这说明在常见的视觉任务中低频信息比高频信息携带了更多对推理有效的信息。</li><li>在亮度分量Y（luma component Y）中的通道比Cb和Cr中的通道更容易被选择。这说明YCbCr空间内的输入在推理时图像的亮度分量Y包含了更多有用信息。</li><li>根据heat map，在分类任务和分割任务上，上述两点同时成立。这证明了这些结论并非task-specific，并且可能同样适用于更多高级的计算机视觉任务。</li><li>低频的通道并不是严格地比高频的通道容易被选中。例如，在Cb和Cr上，能够观察到$6,9$通道被选中而$3,4,5$通道没有被选中的情况，并且在COCO数据集上 $index=0$对应的通道被选择的可能性低于$index=1,2$的通道。也就是说，上述结论可能因为数据集分布的不同具有少量差异。</li></ol><p>这些结论也许说明了，CNN模型和人类的视觉一样喜欢低利用低频信息。JPEG标准在压缩图像时使用的时相似的策略。如果你想仔细研究以下JPEG标准，可以移步维基。</p><h2>什么数据输入了网络</h2><p>经过前处理得到频域特征图也并没有直接作为输入特征直接输入网络，而是与来自spatial-wise的特征图concatenate成为组合特征共同输入网络。这个过程可以表示为：</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023124014543.png" alt="image-20211023124014543"/></p><p>上图：前处理后的频域特征与直接来自图像的特征图拼接的方式。由于CNN中的卷积层对于spatial-wise的图像数据在设计上就表现出优化偏置，因此空域上的输入是必要的。上图中channel数量写为$64$仅为举例。根据论文描述，实际在代码中这个数字一般小等于$192$。在这片论文中，作者一直将这个数字写为$192$。</p><p><img src="./src/Learning-in-the-Frequency-Domain/image-20211023123829531.png" alt="image-20211023123829531"/></p><p>上图：真正被输入后续网络中的数据。在concatenate操作之后，空间域的特征图和频域的特征图被拼接。经过这样的前处理，使输入的特征包含了频域信息丰富了特征表达，并且降低了占用大量带宽的空域特征图大小。</p>]]></content>
        <author>
            <name>Gavin Gong</name>
            <uri>https://gong.host</uri>
        </author>
    </entry>
</feed>