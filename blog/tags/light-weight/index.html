<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">One post tagged with &quot;light-weight&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" property="og:title" content="One post tagged with &quot;light-weight&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/tags/light-weight"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/tags/light-weight"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/light-weight" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/light-weight" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">é­”æ³•éƒ¨æ—¥å¿—</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>One post tagged with &quot;light-weight&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è½»é‡çº§Trickçš„ä¼˜åŒ–ç»„åˆã€‚</p><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/pdf/2109.15099.pdf" target="_blank" rel="noopener noreferrer">PP-LCNet: A Lightweight CPU Convolutional Neural Network</a></p><p>ä½œè€…ï¼šCheng Cui, Tingquan Gao, Shengyu Wei,Yuning Du...</p><p>Codeï¼š<a href="https://github.com/PaddlePaddle/PaddleClas" target="_blank" rel="noopener noreferrer">https://github.com/PaddlePaddle/PaddleClas</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><ol><li>æ€»ç»“äº†ä¸€äº›åœ¨å»¶è¿Ÿï¼ˆlatencyï¼‰å‡ ä¹ä¸å˜çš„æƒ…å†µä¸‹ç²¾åº¦æé«˜çš„æŠ€æœ¯ï¼›</li><li>æå‡ºäº†ä¸€ç§åŸºäºMKLDNNåŠ é€Ÿç­–ç•¥çš„è½»é‡çº§CPUç½‘ç»œï¼Œå³PP-LCNetã€‚</li></ol><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20211007133525281.png" alt="image-20211007133525281"><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä»‹ç»">ä»‹ç»<a aria-hidden="true" class="hash-link" href="#ä»‹ç»" title="Direct link to heading">â€‹</a></h2><p>ç›®å‰çš„è½»é‡çº§ç½‘ç»œåœ¨å¯ç”¨<a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">MKLDNN</a>çš„Intel CPUä¸Šé€Ÿåº¦å¹¶ä¸ç†æƒ³ï¼Œè€ƒè™‘äº†ä¸€ä¸‹ä¸‰ä¸ªåŸºæœ¬é—®é¢˜ï¼š</p><ol><li>å¦‚ä½•ä¿ƒä½¿ç½‘ç»œå­¦ä¹ åˆ°æ›´å¼ºçš„ç‰¹å¾ï¼Œä½†ä¸å¢åŠ å»¶è¿Ÿï¼Ÿ</li><li>åœ¨CPUä¸Šæé«˜è½»é‡çº§æ¨¡å‹ç²¾åº¦çš„è¦ç´ æ˜¯ä»€ä¹ˆï¼Ÿ</li><li>å¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆä¸åŒçš„ç­–ç•¥æ¥è®¾è®¡CPUä¸Šçš„è½»é‡çº§æ¨¡å‹ï¼Ÿ</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/tricks">tricks</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about PP-LCNet - A Lightweight CPU Convolutional Neural Network" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§è½»é‡çº§ä¸»å¹²ç½‘ç»œçš„è®ºæ–‡ã€‚<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆMobileNetV2: Inverted Residuals and Linear Bottlenecksï¼‰</a>ã€‚</p><ul><li>æœ¬æ–‡ä¸»è¦é’ˆå¯¹è½»é‡ç‰¹å¾æå–ç½‘ç»œä¸­ç»“æ„ä¸Šçš„ä¸‰ä¸ªä¿®æ”¹æé«˜äº†ç½‘ç»œæ€§èƒ½ã€‚</li><li>æœ¬æ–‡æ€»æ€è·¯ï¼šä½¿ç”¨ä½ç»´åº¦çš„å¼ é‡å¾—åˆ°è¶³å¤Ÿå¤šçš„ç‰¹å¾</li></ul><p>æ‘˜è¦:</p><blockquote><p>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and bench- marks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottle- neck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demon- strate that this improves performance and provide an in- tuition that led to this design. Finally, our approach allows decoupling of the in- put/output domains from the expressiveness of the trans- formation, which provides a convenient framework for further analysis. We measure our performance on ImageNet classification, COCO object detection <!-- -->[2]<!-- -->, VOC image segmentation <!-- -->[3]<!-- -->. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MobileNetV2 - Inverted Residuals and Linear Bottlenecks" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§è½»é‡çº§ä¸»å¹²ç½‘ç»œçš„è®ºæ–‡ã€‚<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applicationsï¼‰</a>ã€‚</p><ul><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åº”ç”¨äºç§»åŠ¨æˆ–è€…åµŒå…¥å¼è®¾å¤‡çš„é«˜æ•ˆç¥ç»ç½‘ç»œ</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ“ä½œæ•°è¾ƒå°çš„å·ç§¯æ¨¡å—æ·±åº¦å¯åˆ†ç¦»å·ç§¯(Depthwise Separable Convolutionï¼Œä»¥ä¸‹ç§°DSC)</li></ul><p>æ‘˜è¦:</p><blockquote><p>We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>BiSeNetçš„ç›®æ ‡æ˜¯æ›´å¿«é€Ÿçš„å®æ—¶è¯­ä¹‰åˆ†å‰²ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç©ºé—´åˆ†è¾¨ç‡å’Œæ„Ÿå—é‡å¾ˆéš¾ä¸¤å…¨ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶è¯­ä¹‰åˆ†å‰²çš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸æ˜¯åˆ©ç”¨å°çš„è¾“å…¥å›¾åƒæˆ–è€…è½»é‡ä¸»å¹²æ¨¡å‹å®ç°åŠ é€Ÿã€‚ä½†æ˜¯å°å›¾åƒç›¸è¾ƒäºåŸå›¾åƒç¼ºå¤±äº†å¾ˆå¤šç©ºé—´ä¿¡æ¯ï¼Œè€Œè½»é‡çº§æ¨¡å‹åˆ™ç”±äºè£å‰ªé€šé“è€ŒæŸå®³äº†ç©ºé—´ä¿¡æ¯ã€‚BiSegNetæ•´åˆäº†Spatial Path (SP) å’Œ Context Path (CP)åˆ†åˆ«ç”¨æ¥è§£å†³ç©ºé—´ä¿¡æ¯ç¼ºå¤±å’Œæ„Ÿå—é‡ç¼©å°çš„é—®é¢˜ã€‚</p><blockquote><p>Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.</p></blockquote><p>è®ºæ–‡åŸæ–‡ï¼š<a href="https://arxiv.org/abs/1808.00897" target="_blank" rel="noopener noreferrer">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>ã€‚é˜…è¯»åä½ ä¼šå‘ç°ï¼Œè¿™ç¯‡è®ºæ–‡æœ‰å¾ˆå¤šæ€è·¯å—åˆ°<a href="/blog/tags/[23]Squeeze-and-Excitation-Networks">SENetï¼ˆSqueeze-and-Excitation Networksï¼‰</a>çš„å¯å‘ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan%2C+M" target="_blank" rel="noopener noreferrer">Mingyuan Fan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lai%2C+S" target="_blank" rel="noopener noreferrer">Shenqi Lai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+J" target="_blank" rel="noopener noreferrer">Junshi Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+X" target="_blank" rel="noopener noreferrer">Xiaoming Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chai%2C+Z" target="_blank" rel="noopener noreferrer">Zhenhua Chai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo%2C+J" target="_blank" rel="noopener noreferrer">Junfeng Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+X" target="_blank" rel="noopener noreferrer">Xiaolin Wei</a></p><p><img alt="image-20210719132305088" src="/assets/images/image-20210719132305088-c5d44ca09757a5d8bc0ba7e8d56e4611.png"></p><blockquote><p>BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images.</p></blockquote><p>åœ¨é˜…è¯»æœ¬æ–‡å‰ï¼Œè¯·å…ˆé˜…è¯»<a href="/blog/tags/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>ã€‚</p><p>è¯¥è®ºæ–‡æå‡º<a href="/blog/tags/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet</a>è¢«è¯æ˜æ˜¯ä¸é”™çš„åŒè·¯å®æ—¶åˆ†å‰²ç½‘ç»œã€‚ä¸è¿‡ï¼Œåœ¨BiSeNetä¸­ï¼š</p><ul><li>å•ç‹¬ä¸ºç©ºé—´ä¿¡æ¯å¼€è¾Ÿä¸€æ¡ç½‘ç»œè·¯å¾„åœ¨è®¡ç®—ä¸Šéå¸¸çš„è€—æ—¶</li><li>ç”¨äºspatial pathçš„é¢„è®­ç»ƒè½»é‡çº§éª¨å¹²ç½‘ç»œä»å…¶ä»–ä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚åˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ï¼‰ç›´æ¥æ‹¿æ¥ï¼Œç”¨åœ¨åˆ†å‰²ä¸Šæ•ˆç‡ä¸å¾ˆé«˜ã€‚</li></ul><p>å› æ­¤ï¼Œä½œè€…æå‡ºShort-Term Dense Concatenate networkï¼ˆ<strong>STDC</strong> networkï¼‰æ¥ä»£æ›¿BiSeNetä¸­çš„context pathã€‚å…¶æ ¸å¿ƒå†…å®¹æ˜¯ç§»é™¤å†—ä½™çš„ç»“æ„ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡å°†ç‰¹å¾å›¾çš„ç»´æ•°é€æ¸é™ä½ï¼Œå¹¶å°†ç‰¹å¾å›¾èšåˆèµ·æ¥è¿›è¡Œå›¾åƒè¡¨å¾ï¼Œå½¢æˆäº†STDCç½‘ç»œçš„åŸºæœ¬æ¨¡å—ã€‚åŒæ—¶ï¼Œåœ¨decoderä¸­æå‡ºDetail Aggregation moduleå°†ç©ºé—´ä¿¡æ¯çš„å­¦ä¹ ä»¥single-streamæ–¹å¼é›†æˆåˆ°low-level layersä¸­ï¼Œç”¨äºä»£æ›¿BiSeNetä¸­çš„spatial pathã€‚æœ€åï¼Œå°†low-level featureså’Œdeep featuresèåˆä»¥é¢„æµ‹æœ€ç»ˆçš„åˆ†å‰²ç»“æœã€‚</p><p><img alt="image-20210719100139212" src="/assets/images/image-20210719100139212-5b65daa15693025259898e861ccdf07c.png"></p><p>æ³¨ï¼šä¸Šå›¾ä¸­çº¢è‰²è™šçº¿æ¡†ä¸­çš„éƒ¨åˆ†æ˜¯æ–°æå‡ºçš„STDC networkï¼›ARMè¡¨ç¤ºæ³¨æ„åŠ›ä¼˜åŒ–æ¨¡å—ï¼ˆAttention Refinement Moduleï¼‰ï¼ŒFFMè¡¨ç¤ºç‰¹å¾èåˆæ¨¡å—ï¼ˆFeature Fusion Moduleï¼‰ã€‚è¿™ä¸¤ä¸ªæ¨¡å—æ˜¯åœ¨<a href="/blog/tags/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>å°±å·²ç»å­˜åœ¨çš„è®¾è®¡ã€‚</p><p>æœ‰å…´è¶£è¯·é˜…è¯»åŸè®ºæ–‡<a href="https://arxiv.org/abs/2104.13188" target="_blank" rel="noopener noreferrer">Rethinking BiSeNet For Real-time Semantic Segmentation</a>ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/inductive-bias">inductive-bias</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Rethinking BiSeNet For Real-time Semantic Segmentation" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Q" target="_blank" rel="noopener noreferrer">Qiang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y" target="_blank" rel="noopener noreferrer">Yingming Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+T" target="_blank" rel="noopener noreferrer">Tong Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+X" target="_blank" rel="noopener noreferrer">Xiangyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+J" target="_blank" rel="noopener noreferrer">Jian Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" target="_blank" rel="noopener noreferrer">Jian Sun</a></p><blockquote><p>This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - {\em utilizing only one-level feature for detection}. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5Ã— faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7Ã— less training epochs. With an image size of 608Ã—608, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4. Code is available at <a href="https://github.com/megvii-model/YOLOF" target="_blank" rel="noopener noreferrer">this https URL</a>.</p></blockquote><p>æœ¬æ–‡ç®€ç§°YOLOFã€‚æˆªè‡³åˆ°æœ¬æ–‡å†™ä½œæ—¶ï¼ŒäºŒé˜¶æ®µå’Œå•é˜¶æ®µç›®æ ‡æ£€æµ‹çš„SOTAæ–¹æ³•ä¸­å¹¿æ³›ä½¿ç”¨äº†å¤šå°ºåº¦ç‰¹å¾èåˆçš„æ–¹æ³•ã€‚FPNæ–¹æ³•å‡ ä¹å·²ç»ç§°ä¸ºäº†ç½‘ç»œä¸­ç†æ‰€åº”å½“çš„ä¸€ä¸ªç»„ä»¶ã€‚</p><p>æœ¬æ–‡ä¸­ä½œè€…é‡æ–°å›é¡¾äº†FPNæ¨¡å—ï¼Œå¹¶æŒ‡å‡ºFPNçš„ä¸¤ä¸ªä¼˜åŠ¿åˆ†åˆ«æ˜¯å…¶åˆ†æ²»ï¼ˆdivide-and-conquerï¼‰çš„è§£å†³æ–¹æ¡ˆã€ä»¥åŠå¤šå°ºåº¦ç‰¹å¾èåˆã€‚æœ¬æ–‡åœ¨å•é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨ä¸Šç ”ç©¶äº†FPNçš„è¿™ä¸¤ä¸ªä¼˜åŠ¿ï¼Œå¹¶åœ¨RetinaNetä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†ä¸Šè¿°ä¸¤ä¸ªä¼˜åŠ¿è§£è€¦ï¼Œåˆ†åˆ«ç ”ç©¶å…¶å‘æŒ¥çš„ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºï¼ŒFPNåœ¨å¤šå°ºåº¦ç‰¹å¾èåˆä¸Šå‘æŒ¥çš„ä½œç”¨å¯èƒ½æ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¤§ã€‚</p><p>æœ€åï¼Œä½œè€…æå‡ºYOLOFï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ä½¿ç”¨FPNçš„ç›®æ ‡æ£€æµ‹ç½‘ç»œã€‚å…¶ä¸»è¦åˆ›æ–°æ˜¯ï¼š</p><ol><li>Dilated Encoder</li><li>Uniform Matching</li></ol><p>è¯¥ç½‘ç»œåœ¨è¾¾åˆ°RetinaNetå¯¹ç­‰ç²¾åº¦çš„æƒ…å†µä¸‹é€Ÿåº¦æå‡äº†2.5å€ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fpn">fpn</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about You Only Look One-level Feature" href="/blog/[38]You-Only-Look-One-level-Feature"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>