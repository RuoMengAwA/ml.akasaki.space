<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">One post tagged with &quot;low-light&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" property="og:title" content="One post tagged with &quot;low-light&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/tags/low-light"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/tags/low-light"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/low-light" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/low-light" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">é­”æ³•éƒ¨æ—¥å¿—</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>One post tagged with &quot;low-light&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://arxiv.org/abs/1808.04560" target="_blank" rel="noopener noreferrer">Deep Retinex Decomposition for Low-Light Enhancement</a></p><p>è®ºæ–‡ä½œè€…: Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu</p><p>Code: <a href="https://github.com/weichen582/RetinexNet" target="_blank" rel="noopener noreferrer">https://github.com/weichen582/RetinexNet</a></p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>é‡‡ç”¨äº†åˆ†è§£ç½‘ç»œå’Œå¢å¼ºç½‘ç»œï¼Œä½¿ç”¨Retinexç†è®ºæ„å»ºåˆ†è§£ç½‘ç»œï¼Œåˆ†è§£åå†è¿›è¡Œå¢å¼ºã€‚</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deepRetinex-Netlearned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjusment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.</p></blockquote><p>Retinexæ¨¡å‹æ˜¯å¼±å…‰å›¾åƒå¢å¼ºçš„æœ‰æ•ˆå·¥å…·ã€‚å®ƒå‡è®¾è§‚å¯Ÿåˆ°çš„å›¾åƒ<strong>å¯ä»¥åˆ†è§£ä¸ºåå°„ç‡å’Œç…§åº¦</strong>ã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºè§†ç½‘è†œçš„æ–¹æ³•éƒ½ä¸ºè¿™ç§é«˜åº¦ä¸é€‚å®šçš„åˆ†è§£ç²¾å¿ƒè®¾è®¡äº†æ‰‹å·¥åˆ¶ä½œçš„çº¦æŸå’Œå‚æ•°ï¼Œå½“åº”ç”¨äºå„ç§åœºæ™¯æ—¶ï¼Œè¿™å¯èƒ½ä¼šå—åˆ°æ¨¡å‹å®¹é‡çš„é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå¯¹çš„å¼±å…‰æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šæå‡ºäº†ä¸€ä¸ªDeeprinex Netlearnï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºåˆ†è§£çš„Decom-Netå’Œä¸€ä¸ªç”¨äºå…‰ç…§è°ƒæ•´çš„å¢å¼º-Netã€‚åœ¨Decom-Netçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å­˜åœ¨åˆ†è§£åå°„ç‡å’Œå…‰ç…§çš„åŸºæœ¬äº‹å®ã€‚è¯¥ç½‘ç»œä»…åœ¨å…³é”®çº¦æŸæ¡ä»¶ä¸‹å­¦ä¹ ï¼ŒåŒ…æ‹¬æˆå¯¹çš„å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå…±äº«çš„ä¸€è‡´åå°„ç‡ä»¥åŠç…§æ˜çš„å¹³æ»‘åº¦ã€‚åœ¨åˆ†è§£çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¢å¼ºç½‘ç»œå¯¹å…‰ç…§è¿›è¡Œåç»­çš„äº®åº¦å¢å¼ºï¼Œå¯¹äºè”åˆå»å™ªï¼Œå¯¹åå°„ç‡è¿›è¡Œå»å™ªæ“ä½œã€‚RetinexNetæ˜¯ç«¯åˆ°ç«¯è®­ç»ƒçš„ï¼Œå› æ­¤å­¦ä¹ çš„åˆ†è§£æœ¬è´¨ä¸Šæœ‰åˆ©äºäº®åº¦è°ƒæ•´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¼±å…‰å¢å¼ºæ–¹é¢è·å¾—äº†è§†è§‰ä¸Šä»¤äººæ»¡æ„çš„è´¨é‡ï¼Œè€Œä¸”æä¾›äº†å›¾åƒåˆ†è§£çš„è‰¯å¥½è¡¨ç¤ºã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Deep Retinex Decomposition for Low-Light Enhancement" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220116171846.jpg" alt="RuoMengAwA"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">RuoMengAwA</span></a></div><small class="avatar__subtitle" itemprop="description">ä¸€ä¸ªå–œæ¬¢æ‘¸é±¼çš„èœç‹—ï¼Œç›®å‰ä¸»è¦åšä½ç…§åº¦å¢å¼ºæ–¹å‘çš„ç ”ç©¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p><em>é¬¼ç½‘ï¼</em></p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212350.png" alt="image-20210510203042875"></p></blockquote><hr><header><h1>GhostNetäº§ç”ŸåŸå› </h1></header><blockquote><p><em>mobileNetæˆ–è€…æ˜¯shuffleNetæå‡ºäº†ä½¿ç”¨depthwiseæˆ–è€…æ˜¯shuffleç­‰æ“ä½œï¼Œä½†æ˜¯å¼•å…¥çš„1x1å·ç§¯ä¾ç„¶ä¼šäº§ç”Ÿä¸€å®šçš„è®¡ç®—é‡</em></p><p>ä¸ºä»€ä¹ˆ1x1å·ç§¯ä¾ç„¶ä¼šäº§ç”Ÿè¾ƒå¤§çš„è®¡ç®—é‡?çœ‹å·ç§¯è®¡ç®—é‡çš„è®¡ç®—å…¬å¼$n âˆ— h âˆ— w âˆ— c âˆ— k âˆ— k $,å¯ä»¥å‘ç°ï¼Œç”±äºcå’Œnéƒ½æ˜¯æ¯”è¾ƒå¤§çš„ï¼Œæ‰€ä»¥ä¼šå¯¼è‡´è¿™ä¸ªè®¡ç®—é‡ä¹Ÿæ˜¯æ¯”è¾ƒå¤§çš„ï¼ˆåæ–‡å…·ä½“ç»“æ„å¤ç°æ—¶è¿˜ä¼šè§£é‡Šï¼‰</p><p>æ‰€ä»¥ï¼Œæˆ‘ä»¬å¦‚ä½•åœ¨è¿™ä¸ªåŸºç¡€ä¸Šå†å‡å°‘å‚æ•°ï¼Œä¼˜åŒ–ç½‘ç»œé€Ÿåº¦å‘¢ï¼Œä½œè€…ä»ä¸€ä¸ªç‹¬ç‰¹çš„è§’åº¦ï¼Œè§‚å¯Ÿäº†ResNet50ç¬¬ä¸€ä¸ªæ®‹å·®å—è¾“å‡ºçš„ç‰¹å¾å›¾ï¼Œå‘ç°æœ‰è®¸å¤šè¾“å‡ºç‰¹å¾å¾ˆç›¸ä¼¼ï¼ŒåŸºæœ¬åªè¦è¿›è¡Œç®€å•çš„çº¿æ€§å˜æ¢å°±èƒ½å¾—åˆ°ï¼Œè€Œä¸éœ€è¦è¿›è¡Œå¤æ‚çš„éçº¿æ€§å˜æ¢å¾—åˆ°ã€‚</p><p>å¦‚å›¾ï¼š</p><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212347.png" alt="image-20210510184255116"></p><p>ä»¥ä¸Šå›¾ç‰‡ä¸­åŒè‰²å›¾ç‰‡å¯ä»¥ä½¿ç”¨cheap operationsè¿›è¡Œç”Ÿæˆ</p><p>æ‰€ä»¥å¯ä»¥å…ˆé€šè¿‡ä¸€ä¸ªéçº¿æ€§å˜åŒ–å¾—åˆ°å…¶ä¸­ä¸€ä¸ªç‰¹å¾å›¾ï¼Œé’ˆå¯¹è¿™ä¸ªç‰¹å¾å›¾åšçº¿æ€§å˜åŒ–ï¼Œå¾—åˆ°åŸç‰¹å¾å›¾çš„å¹½çµç‰¹å¾å›¾ã€‚</p><p><em>ps:è¿™é‡Œè¯´çš„éçº¿æ€§å·ç§¯æ“ä½œæ˜¯å·ç§¯-æ‰¹å½’ä¸€åŒ–-éçº¿æ€§æ¿€æ´»å…¨å¥—ç»„åˆï¼Œè€Œæ‰€è°“çš„çº¿æ€§å˜æ¢æˆ–è€…å»‰ä»·æ“ä½œå‡æŒ‡æ™®é€šå·ç§¯ï¼Œä¸å«æ‰¹å½’ä¸€åŒ–å’Œéçº¿æ€§æ¿€æ´»</em></p></blockquote><p>â€‹		 æ‰€ä»¥ï¼Œæ€»ç»“å…¶<strong>æ ¸å¿ƒæ€æƒ³</strong>å°±æ˜¯ï¼šè®¾è®¡ä¸€ç§åˆ†é˜¶æ®µçš„å·ç§¯è®¡ç®—æ¨¡å—ï¼Œåœ¨å°‘é‡çš„éçº¿æ€§çš„å·ç§¯å¾—åˆ°çš„ç‰¹å¾å›¾åŸºç¡€ä¸Šï¼Œå†è¿›è¡Œä¸€æ¬¡çº¿æ€§å·ç§¯ï¼Œä»è€Œè·å–æ›´å¤šçš„ç‰¹å¾å›¾ï¼Œè€Œæ–°å¾—åˆ°çš„ç‰¹å¾å›¾ï¼Œå°±è¢«å«åšä¹‹å‰ç‰¹å¾å›¾çš„â€˜ghostâ€™ï¼Œä»¥æ­¤æ¥å®ç°æ¶ˆé™¤å†—ä½™ç‰¹å¾ï¼ˆä¹Ÿå¯ä»¥è¯´æ˜¯ä¸é¿å…å†—ä½™çš„ç‰¹å¾æ˜ å°„ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç§æ›´ä½æˆæœ¬æ•ˆç›Šçš„æ–¹å¼æ¥å—å®ƒï¼‰ï¼Œä½¿å¾—åœ¨ä¿æŒç›¸ä¼¼çš„è¯†åˆ«æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½é€šç”¨å·ç§¯å±‚çš„è®¡ç®—ä»£ä»·ï¼Œä»¥è·å–æ›´åŠ è½»é‡çš„æ¨¡å‹ï¼ˆéçº¿æ€§çš„æ“ä½œæ˜¯ <em>æ˜‚è´µçš„</em>ï¼Œçº¿æ€§æ“ä½œæ˜¯ <em>å»‰ä»·çš„</em>ï¼‰ï¼ˆè¿™æ“ä½œé¬¼æƒ³å¾—åˆ°ã€‚ã€‚ã€‚ï¼‰</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about GhostNet - More Features from Cheap Operations" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220116171846.jpg" alt="RuoMengAwA"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">RuoMengAwA</span></a></div><small class="avatar__subtitle" itemprop="description">ä¸€ä¸ªå–œæ¬¢æ‘¸é±¼çš„èœç‹—ï¼Œç›®å‰ä¸»è¦åšä½ç…§åº¦å¢å¼ºæ–¹å‘çš„ç ”ç©¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ç ”ç©¶èƒŒæ™¯">ç ”ç©¶èƒŒæ™¯ï¼š<a aria-hidden="true" class="hash-link" href="#ç ”ç©¶èƒŒæ™¯" title="Direct link to heading">â€‹</a></h2><p>å¯¹äºä¸€å¼ ä½å…‰å›¾åƒï¼Œä¸ä»…æ˜¯æš—ï¼Œè€Œä¸”ä¹Ÿä¼šä¼´éšç€å™ªå£°å’Œé¢œè‰²å¤±çœŸç­‰å¤šæ–¹é¢çš„å›¾åƒåŠŸèƒ½é€€åŒ–ï¼Œæ‰€ä»¥ä»…ä»…æé«˜äº®åº¦å°†æ— å¯é¿å…çš„æé«˜äººå·¥äº§ç”Ÿçš„å½±å“ï¼Œå¿…ç„¶ä¼šæ”¾å¤§éšè—çš„ä¼ªå½±</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ç‰¹ç‚¹">ç‰¹ç‚¹ï¼š<a aria-hidden="true" class="hash-link" href="#ç‰¹ç‚¹" title="Direct link to heading">â€‹</a></h2><p>è¿˜æ˜¯ä»retinexç†è®ºä¸­å¾—åˆ°çš„å¯å‘ï¼Œç»§è€Œå°†å¼±å…‰å›¾åƒåˆ†è§£ä¸ºå…‰ç…§ï¼ˆ<strong>illumination</strong>ï¼‰å’Œ åå°„ç‡ï¼ˆ<strong>reflectance</strong>ï¼‰ï¼›å‰è€…è´Ÿè´£äº®åº¦è°ƒæ•´ï¼Œåè€…ç”¨äºå»é™¤é™è´¨ï¼ˆå™ªå£°ï¼Œé¢œè‰²å¤±çœŸï¼‰ã€‚è¿™æ ·å›¾åƒåˆ†è§£çš„å¥½å¤„æ˜¯è®©æ¯ä¸€ä¸ªæ¨¡å—å¯ä»¥æ›´å¥½åœ°è¢«æ­£è§„åŒ–/å­¦ä¹ </p><p>è€Œå¯¹äºè¾“å…¥å›¾åƒï¼Œè¯¥ç½‘ç»œåªéœ€è¦ä½¿ç”¨ä¸¤å¼ ä¸åŒæ›å…‰æ¡ä»¶ä¸‹çš„å›¾åƒï¼ˆå³ä½¿ä»–ä»¬æ˜¯ä¸¤å¼ å¼±å…‰å›¾åƒä¹Ÿå¯ä»¥ï¼‰ï¼Œè€Œä¸æ˜¯å¼±å…‰å›¾åƒå’ŒçœŸå®å›¾åƒï¼ˆè¿™æ ·çš„å¥½å¤„æ˜¯ï¼Œå¾ˆéš¾å®šä¹‰å¤šäº®çš„å›¾åƒç®—æ˜¯çœŸå®å›¾åƒï¼‰</p><p>å¯¹äºä¸¥é‡çš„è§†è§‰ç¼ºé™·å›¾ç‰‡ä¹Ÿä¾æ—§æ‹¥æœ‰å¾ˆå¼ºçš„é²æ£’æ€§</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ•ˆæœ">æ•ˆæœï¼š<a aria-hidden="true" class="hash-link" href="#æ•ˆæœ" title="Direct link to heading">â€‹</a></h2><p>æ¨¡å‹åœ¨2080Tiä¸‹çš„è®­ç»ƒé€Ÿåº¦ä¸ºï¼Œå¤„ç†ä¸€å¼ VGAåˆ†è¾¨ç‡å›¾ç‰‡èŠ±è´¹çš„æ—¶é—´ä¸åˆ°50ms</p><p>ç”¨æˆ·å¯ä»¥è‡ªç”±çš„è°ƒèŠ‚å…‰ç…§æ°´å¹³ï¼ˆæš‚æ—¶æ²¡çœ‹åˆ°åœ¨å“ªä½“ç°ï¼‰</p><p>å…·ä½“æ•ˆæœå±•ç¤ºï¼ˆå®æœºæµ‹è¯•ï¼‰ï¼š</p><p>ä¸åŒå™ªåº¦ï¼š</p><blockquote><p>é«˜å…‰å›¾åƒå’Œä½å…‰å›¾åƒå¯¹ç…§ï¼ˆä¸åŒçš„ï¼‰</p></blockquote><p><img src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212643.png" alt="image-20210802180913986"></p><p>å¯ä»¥å¾—å‡ºï¼ŒKinDåœ¨å¤šæ¡ä»¶ä¸‹ï¼Œæ•ˆæœæš‚æ—¶éƒ½ä¼˜äºå…¶ä»–ä½ç…§åº¦ä¼˜åŒ–ç®—æ³•ï¼ˆæœ€ä¸»è¦çš„æ˜¯æ•ˆæœçœŸå®ï¼Œç›¸è¾ƒäºå…¶ä½™ç®—æ³•ï¼Œå¤±çœŸçš„æƒ…å†µä¼šå¤§å¤§å‡å°‘ï¼ˆä¸è¿‡ç°åœ¨è¿˜æœ‰ä¸€ä¸ªKinD++ï¼‰ï¼‰</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Kindling the Darkness - A Practical Low-light Image Enhancer" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²ä½å…‰ç…§äººè„¸æ£€æµ‹çš„è®ºæ–‡ã€‚<a href="https://arxiv.org/pdf/2104.01984.pdf" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆHLA-Face Joint High-Low Adaptation for Low Light Face Detectionï¼‰</a>ã€‚</p><ul><li>å……åˆ†åˆ©ç”¨ç°æœ‰çš„æ­£å¸¸å…‰æ•°æ®ï¼Œå¹¶æ¢ç´¢å¦‚ä½•å°†é¢éƒ¨æ¢æµ‹å™¨ä»æ­£å¸¸å…‰çº¿è°ƒæ•´åˆ°ä½å…‰ã€‚è¿™é¡¹ä»»åŠ¡çš„æŒ‘æˆ˜æ˜¯ï¼Œæ­£å¸¸å’Œä½å…‰ä¹‹é—´çš„å·®è·å¯¹äºåƒç´ çº§å’Œç‰©ä½“çº§åˆ«æ¥è¯´å¤ªå¤§è€Œå¤æ‚ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°ç°æœ‰çš„lowlighenhanceå’Œé€‚åº”æ–¹æ³•ä¸è¾¾åˆ°æ‰€éœ€çš„performanceã€‚</li><li>æœ¬æ–‡æ˜¯DARK FACEä¸ºåŸºå‡†ï¼Œé’ˆå¯¹ç°æœ‰çš„æ­£å¸¸ç…§åº¦å›¾åƒï¼Œå°†å›¾åƒè°ƒæ•´æˆä½ç…§åº¦å›¾åƒï¼Œä¸éœ€è¦æ ‡ç­¾</li><li>ä¸€ä¸ªæ˜¯åƒç´ çº§å¤–è§‚çš„å·®è·ï¼Œä¾‹å¦‚ä¸è¶³ï¼Œç…§æ˜ï¼Œç›¸æœºå™ªå£°å’Œé¢œè‰²åç½®ã€‚å¦ä¸€ä¸ªæ˜¯æ­£å¸¸å’Œä½å…‰åœºæ™¯ä¹‹é—´çš„ç‰©ä½“çº§è¯­ä¹‰å·®å¼‚ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè·¯ç¯çš„å­˜åœ¨ï¼Œè½¦è¾†å‰ç¯å’Œå¹¿å‘Šæ¿ã€‚ä¼ ç»Ÿçš„ä½å…‰å¢å¼ºæ–¹æ³•<!-- -->[5,6]<!-- -->è®¾è®¡ç”¨äºæé«˜è§†è§‰è´¨é‡ï¼Œå› æ­¤ä¸èƒ½å¡«å……è¯­ä¹‰å·®è·ï¼Œ</li><li>é€šè¿‡ä½¿ä½å…‰å›¾åƒäº®èµ·å¹¶æ‰­æ›²æ­£å¸¸å…‰å›¾åƒï¼Œæˆ‘ä»¬æ„å»ºä½äºæ­£å¸¸å’Œä½å…‰ä¹‹é—´çš„ä¸­é—´çŠ¶æ€ã€‚</li></ul><p>æ‘˜è¦:</p><blockquote><p>Face detection in low light scenarios is challenging but vital to many practical applications, e.g., surveillance video, autonomous driving at night. Most existing face detectors heavily rely on extensive annotations, while col- lecting data is time-consuming and laborious. To reduce the burden of building new datasets for low light condi- tions, we make full use of existing normal light data and explore how to adapt face detectors from normal light to low light. The challenge of this task is that the gap between normal and low light is too huge and complex for both pixel-level and object-level. Therefore, most existing low- light enhancement and adaptation methods do not achieve desirable performance. To address the issue, we propose a joint High-Low Adaptation (HLA) framework. Through a bidirectional low-level adaptation and multi-task high- level adaptation scheme, our HLA-Face outperforms state- of-the-art methods even without using dark face labels for training. Our project is publicly available at: <!-- -->[https: //daooshee.github.io/HLA-Face-Website/]<!-- -->(https: //daooshee.github.io/HLA-Face-Website/)</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about HLA-Face Joint High-Low Adaptation for Low Light Face Detection" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://arxiv.org/abs/1808.04560" target="_blank" rel="noopener noreferrer">Deep Retinex Decomposition for Low-Light Enhancement</a></p><p>è®ºæ–‡ä½œè€…: Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu</p><p>Code: <a href="https://github.com/weichen582/RetinexNet" target="_blank" rel="noopener noreferrer">https://github.com/weichen582/RetinexNet</a></p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>é‡‡ç”¨äº†åˆ†è§£ç½‘ç»œå’Œå¢å¼ºç½‘ç»œï¼Œä½¿ç”¨Retinexç†è®ºæ„å»ºåˆ†è§£ç½‘ç»œï¼Œåˆ†è§£åå†è¿›è¡Œå¢å¼ºã€‚</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deepRetinex-Netlearned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjusment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.</p></blockquote><p>Retinexæ¨¡å‹æ˜¯å¼±å…‰å›¾åƒå¢å¼ºçš„æœ‰æ•ˆå·¥å…·ã€‚å®ƒå‡è®¾è§‚å¯Ÿåˆ°çš„å›¾åƒ<strong>å¯ä»¥åˆ†è§£ä¸ºåå°„ç‡å’Œç…§åº¦</strong>ã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºè§†ç½‘è†œçš„æ–¹æ³•éƒ½ä¸ºè¿™ç§é«˜åº¦ä¸é€‚å®šçš„åˆ†è§£ç²¾å¿ƒè®¾è®¡äº†æ‰‹å·¥åˆ¶ä½œçš„çº¦æŸå’Œå‚æ•°ï¼Œå½“åº”ç”¨äºå„ç§åœºæ™¯æ—¶ï¼Œè¿™å¯èƒ½ä¼šå—åˆ°æ¨¡å‹å®¹é‡çš„é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå¯¹çš„å¼±å…‰æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šæå‡ºäº†ä¸€ä¸ªDeeprinex Netlearnï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºåˆ†è§£çš„Decom-Netå’Œä¸€ä¸ªç”¨äºå…‰ç…§è°ƒæ•´çš„å¢å¼º-Netã€‚åœ¨Decom-Netçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å­˜åœ¨åˆ†è§£åå°„ç‡å’Œå…‰ç…§çš„åŸºæœ¬äº‹å®ã€‚è¯¥ç½‘ç»œä»…åœ¨å…³é”®çº¦æŸæ¡ä»¶ä¸‹å­¦ä¹ ï¼ŒåŒ…æ‹¬æˆå¯¹çš„å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå…±äº«çš„ä¸€è‡´åå°„ç‡ä»¥åŠç…§æ˜çš„å¹³æ»‘åº¦ã€‚åœ¨åˆ†è§£çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¢å¼ºç½‘ç»œå¯¹å…‰ç…§è¿›è¡Œåç»­çš„äº®åº¦å¢å¼ºï¼Œå¯¹äºè”åˆå»å™ªï¼Œå¯¹åå°„ç‡è¿›è¡Œå»å™ªæ“ä½œã€‚RetinexNetæ˜¯ç«¯åˆ°ç«¯è®­ç»ƒçš„ï¼Œå› æ­¤å­¦ä¹ çš„åˆ†è§£æœ¬è´¨ä¸Šæœ‰åˆ©äºäº®åº¦è°ƒæ•´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¼±å…‰å¢å¼ºæ–¹é¢è·å¾—äº†è§†è§‰ä¸Šä»¤äººæ»¡æ„çš„è´¨é‡ï¼Œè€Œä¸”æä¾›äº†å›¾åƒåˆ†è§£çš„è‰¯å¥½è¡¨ç¤ºã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Deep Retinex Decomposition for Low-Light Enhancement" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://arxiv.org/abs/1711.02488" target="_blank" rel="noopener noreferrer">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></p><p>è®ºæ–‡ä½œè€…: Liang Shen, Zihan Y ue, Fan Feng, Quan Chen, Shihao Liu, Jie Ma</p><p>Code: None</p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£åŸºäºRetinexç†è®ºä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>åŸºäºMSRä¼ ç»Ÿç†è®ºæ„é€ å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹</li><li>ç›´æ¥å­¦ä¹ æš—å›¾åƒå’Œäº®å›¾åƒä¹‹é—´çš„ç«¯åˆ°ç«¯æ˜ å°„</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Images captured in low-light conditions usually suffer from very low contrast, which increases the difficulty of sub-sequent computer vision tasks in a great extent. In this paper, a low-light image enhancement model based on convolutional neural network and Retinex theory is proposed. Firstly, we show that multi-scale Retinex is equivalent to a feedforward convolutional neural network with different Gaussian convolution kernels. Motivated by this fact, we consider a Convolutional Neural Network(MSR-net) that directly learns an end-to-end mapping between dark and bright images. Different fundamentally from existing approaches, low-light image enhancement in this paper is regarded as a machine learning problem. In this model, most of the parameters are optimized by back-propagation, while the parameters of traditional models depend on the artificial setting. Experiments on a number of challenging images reveal the advantages of our method in comparison with other state-of-the-art methods from the qualitative and quantitative perspective.</p></blockquote><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå·ç§¯ç¥ç»ç½‘ç»œå’Œè§†ç½‘è†œç†è®º(Retinex Theory)çš„ä½ç…§åº¦å›¾åƒå¢å¼ºæ¨¡å‹ã€‚è¯æ˜äº†å¤šå°ºåº¦è§†ç½‘è†œç­‰ä»·äºä¸€ä¸ªå…·æœ‰ä¸åŒé«˜æ–¯å·ç§¯æ ¸çš„å‰é¦ˆå·ç§¯ç¥ç»ç½‘ç»œã€‚è€ƒè™‘ä¸€ç§å·ç§¯ç¥ç»ç½‘ç»œ(MSRç½‘ç»œ)ï¼Œå®ƒ<strong>ç›´æ¥å­¦ä¹ æš—å›¾åƒå’Œäº®å›¾åƒä¹‹é—´çš„ç«¯åˆ°ç«¯æ˜ å°„</strong>ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MSR-net - Low-light Image Enhancement Using Deep Convolutional Network" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://ieeexplore.ieee.org/abstract/document/8305143/" target="_blank" rel="noopener noreferrer">LLCNN: A convolutional neural network for low-light image enhancement</a></p><p>è®ºæ–‡ä½œè€…: Li Tao, Chuang Zhu, Guoqing Xiang, Yuan Li, Huizhu Jia, Xiaodong Xie</p><p>Code: <a href="https://github.com/BestJuly/LLCNN" target="_blank" rel="noopener noreferrer">https://github.com/BestJuly/LLCNN</a></p></blockquote><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯pommespeter">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/PommesPeter" target="_blank" rel="noopener noreferrer">PommesPeter</a>ã€‚<a aria-hidden="true" class="hash-link" href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯pommespeter" title="Direct link to heading">â€‹</a></h3><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>æœ¬æ–‡ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼º</li><li>ä½¿ç”¨SSIMæŸå¤±æ›´å¥½åœ°è¯„ä»·å›¾åƒå¥½åå’Œæ¢¯åº¦æ”¶æ•›</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>In this paper, we propose a CNN based method to perform low-light image enhancement. We design a special  module to utilize multiscale feature maps, which can avoid  gradient vanishing problem as well. In order to preserve image textures as much as possible, we use SSIM loss to train our model. The contrast of low-light images can be adaptively enhanced using our method. Results demonstrate that our CNN based method  outperforms other contrast enhancement methods. </p></blockquote><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCNNçš„ä½ç…§åº¦å›¾åƒå¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹æ®Šçš„æ¨¡å—æ¥<strong>åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾æ˜ å°„</strong>ï¼Œè¿™æ ·å¯ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚<strong>ä¸ºäº†å°½å¯èƒ½åœ°ä¿ç•™å›¾åƒçº¹ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨SSIMæŸå¤±æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹</strong>ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥<strong>è‡ªé€‚åº”åœ°å¢å¼ºå¼±å…‰å›¾åƒçš„å¯¹æ¯”åº¦</strong>ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about LLCNN - A convolutional neural network for low-light image enhancement" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>