<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">One post tagged with &quot;segmentation&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" property="og:title" content="One post tagged with &quot;segmentation&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/tags/segmentation"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/tags/segmentation"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/segmentation" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/segmentation" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">é­”æ³•éƒ¨æ—¥å¿—</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>One post tagged with &quot;segmentation&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>æå‡ºåå­—äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œä½¿ç”¨å¾ªç¯ç¨€ç–è¿æ¥ä»£æ›¿å¯†é›†è¿æ¥ï¼Œå®ç°æ€§èƒ½SOTA</p><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/pdf/1811.11721.pdf" target="_blank" rel="noopener noreferrer">CCNet: Criss-Cross Attention for Semantic Segmentation</a></p><p>ä½œè€…ï¼šZilong Huangï¼ŒXinggang Wang Yunï¼Œchao Weiï¼ŒLichao Huangï¼ŒWenyu Liuï¼ŒThomas S. Huang</p><p>Codeï¼š<a href="https://github.com/speedinghzl/CCNet" target="_blank" rel="noopener noreferrer">https://github.com/speedinghzl/CCNet</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><p>ä¸Šä¸‹æ–‡ä¿¡æ¯åœ¨è§†è§‰ç†è§£é—®é¢˜ä¸­è‡³å…³é‡è¦ï¼Œè­¬å¦‚è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ï¼›</p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åå­—äº¤å‰çš„ç½‘ç»œï¼ˆCriss-Cross Netï¼‰ä»¥éå¸¸é«˜æ•ˆçš„æ–¹å¼è·å–å®Œæ•´çš„å›¾åƒä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š</p><ol><li>å¯¹æ¯ä¸ªåƒç´ ä½¿ç”¨ä¸€ä¸ªåå­—æ³¨æ„åŠ›æ¨¡å—èšé›†å…¶è·¯å¾„ä¸Šæ‰€æœ‰åƒç´ çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›</li><li>é€šè¿‡å¾ªç¯æ“ä½œï¼Œæ¯ä¸ªåƒç´ æœ€ç»ˆéƒ½å¯ä»¥æ•è·å®Œæ•´çš„å›¾åƒç›¸å…³æ€§ï¼›</li><li>æå‡ºäº†ä¸€ç§ç±»åˆ«ä¸€è‡´æ€§æŸå¤±æ¥å¢å¼ºæ¨¡å—çš„è¡¨ç°ã€‚</li></ol><p>CCNetå…·æœ‰ä¸€ä¸‹ä¼˜åŠ¿ï¼š</p><ol><li>æ˜¾å­˜å‹å¥½ï¼šç›¸è¾ƒäºNon-Localå‡å°‘æ˜¾å­˜å ç”¨11å€</li><li>è®¡ç®—é«˜æ•ˆï¼šå¾ªç¯åå­—æ³¨æ„åŠ›å‡å°‘Non-Localçº¦85%çš„è®¡ç®—é‡</li><li>SOTA</li><li>Achieve the <strong>mIoU</strong> scores of 81.9%, 45.76% and 55.47% on the <strong>Cityscapes test set</strong>, the <strong>ADE20K validation set</strong> and the <strong>LIP validation set</strong> respectively</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attention-mechanism">attention-mechanism</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about CCNet - Criss-Cross Attention for Semantic Segmentation" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AndPuQing" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/AndPuQing.png" alt="PuQing"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AndPuQing" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PuQing</span></a></div><small class="avatar__subtitle" itemprop="description">intro * new</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_DCT-Mask_Discrete_Cosine_Transform_Mask_Representation_for_Instance_Segmentation_CVPR_2021_paper.pdf" target="_blank" rel="noopener noreferrer">DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation</a></p><p>ä½œè€…ï¼šXing Shen, Jirui Yang, Chunbo Wei, Bing Deng, Jianqiang Huang, Xiansheng Hua, Xiaoliang Cheng, Kewei Liang</p><p>ä»“åº“åœ°å€ï¼š<a href="https://github.com/calmevtime/DCTNet" target="_blank" rel="noopener noreferrer">https://github.com/calmevtime/DCTNet</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>r</mi><mi>y</mi><mtext>â€…â€Š</mtext><mi>g</mi><mi>r</mi><mi>i</mi><mi>d</mi><mtext>â€…â€Š</mtext><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Binary\; grid\; mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">ina</span><span class="mord mathnormal" style="margin-right:0.03588em">ry</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> å¹¿æ³›ç”¨äºå®ä¾‹åˆ†å‰²ã€‚å°±ä¾‹å¦‚ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>s</mi><mi>k</mi><mtext>Â </mtext><mi>R</mi><mo>âˆ’</mo><mi>C</mi><mi>N</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">Mask\ R-CNN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace">Â </span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">CNN</span></span></span></span></span><a href="#references"><sup>1</sup></a>ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç½‘ç»œåœ¨ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">28</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">28</span></span></span></span></span> çš„ç½‘æ ¼ä¸­é¢„æµ‹ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> ã€‚</p><p><img src="/assets/images/mask-9bdabbbb284b83aeddc7e383af2e840a.jpg"></p><p>ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œä½åˆ†è¾¨ç‡çš„ç½‘æ ¼ä¸è¶³ä»¥æ•æ‰ç»†èŠ‚ï¼Œè€Œé«˜åˆ†è¾¨ç‡ä¼šå¤§å¤§å¢åŠ è®­ç»ƒçš„å¤æ‚æ€§ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºä¸€ç§æ–°çš„ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> è¡¨è¾¾æ–¹å¼ï¼Œåˆ©ç”¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>C</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">DCT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mord mathnormal" style="margin-right:0.13889em">CT</span></span></span></span></span>ï¼‰å°†é«˜åˆ†è¾¨ç‡çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>r</mi><mi>y</mi><mtext>â€…â€Š</mtext><mi>g</mi><mi>r</mi><mi>i</mi><mi>d</mi><mtext>â€…â€Š</mtext><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Binary\; grid\; mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">ina</span><span class="mord mathnormal" style="margin-right:0.03588em">ry</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„å‘é‡ï¼Œè¿™ç§æ–¹æ³•ç§°ä¸º <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>C</mi><mi>T</mi><mo>âˆ’</mo><mi>M</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">DCT-Mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mord mathnormal" style="margin-right:0.13889em">CT</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>ã€‚</p><p>è¯¥æ–¹æ³•å¯ä»¥éå¸¸å®¹æ˜“é›†æˆåˆ°å¤§å¤šæ•°åŸºäºåƒç´ çš„å®ä¾‹åˆ†å‰²ä¸Šã€‚å®ƒä¸éœ€è¦ä»»ä½•é¢„å¤„ç†æˆ–é¢„è®­ç»ƒï¼Œè€Œä¸”å‡ ä¹å¯¹é€Ÿåº¦æ²¡æœ‰æŸå®³ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä»‹ç»">ä»‹ç»<a aria-hidden="true" class="hash-link" href="#ä»‹ç»" title="Direct link to heading">â€‹</a></h2><p>å°±å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>s</mi><mi>k</mi><mtext>Â </mtext><mi>R</mi><mo>âˆ’</mo><mi>C</mi><mi>N</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">Mask\ R-CNN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace">Â </span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">CNN</span></span></span></span></span> å°† <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">GT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">GT</span></span></span></span></span> é‡‡æ ·åˆ° <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">28</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">28</span></span></span></span></span> ï¼Œç„¶åä¸Šé‡‡æ ·é‡æ„å®ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä½åˆ†è¾¨ç‡çš„ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>r</mi><mi>y</mi><mtext>â€…â€Š</mtext><mi>g</mi><mi>r</mi><mi>i</mi><mi>d</mi><mtext>â€…â€Š</mtext><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Binary\; grid\; mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">ina</span><span class="mord mathnormal" style="margin-right:0.03588em">ry</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> ä¸è¶³ä»¥æ•è·ç»†èŠ‚ç‰¹å¾ï¼Œå¹¶åœ¨ä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­äº§ç”Ÿåå·®ã€‚</p><p><img src="/assets/images/vs-ebd78cd0583b979534aa501725d9fbd3.jpg"></p><p>å¦‚ä¸Šå›¾ä¸ºä½¿ç”¨ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>C</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">DCT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mord mathnormal" style="margin-right:0.13889em">CT</span></span></span></span></span> å’Œæœªä½¿ç”¨ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>C</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">DCT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mord mathnormal" style="margin-right:0.13889em">CT</span></span></span></span></span> æ–¹æ³•çš„æ¯”è¾ƒï¼Œå·¦è¾¹ä¸º <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">GT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">GT</span></span></span></span></span> ï¼›ä¹‹åæ˜¯ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Resize</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mord mathnormal">es</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span></span></span></span></span> åçš„ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">GT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">GT</span></span></span></span></span> ï¼›å†æ˜¯åŸºäº <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Resize</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mord mathnormal">es</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span></span></span></span></span> åçš„é‡å»ºå›¾ï¼›æœ€åæ˜¯é‡å»ºå›¾ä¸åŸæ¥çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">GT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">GT</span></span></span></span></span>å›¾çš„è¯¯å·®å€¼ã€‚</p><p>æ‰€ä»¥å°±ç®—é¢„æµ‹ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> æ˜¯æ­£ç¡®çš„ï¼Œé‡å»ºçš„ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> ä¹Ÿæœ‰ä¸€å®šçš„ç³»ç»Ÿè¯¯å·®ã€‚è§£å†³æ–¹å¼ä¹‹ä¸€æ˜¯æé«˜ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>r</mi><mi>y</mi><mtext>â€…â€Š</mtext><mi>g</mi><mi>r</mi><mi>i</mi><mi>d</mi><mtext>â€…â€Š</mtext><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Binary\; grid\; mask</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">ina</span><span class="mord mathnormal" style="margin-right:0.03588em">ry</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> çš„åˆ†è¾¨ç‡ï¼Œä½†æ˜¯å®éªŒæ˜¾ç¤ºæé«˜åˆ†è¾¨ç‡åå¹³å‡ç²¾åº¦ï¼ˆ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>P</mi></mrow><annotation encoding="application/x-tex">AP</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span></span>ï¼‰æ¯” <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">28</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">28</span></span></span></span></span> è¦å·®ï¼Œå…·ä½“è§ä¸‹å›¾ã€‚</p><p><img src="/assets/images/mask_size-bd27b2e04b710cf1b39375a75f20718b.jpg"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/frequency-domain">frequency-domain</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/inductive-bias">inductive-bias</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²å„ç§å„æ ·è§£ç å™¨çš„è®ºæ–‡ã€‚<a href="https://arxiv.org/pdf/1707.05847.pdf" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆThe Devil is in the Decoder: Classification, Regression and GANsï¼‰</a>ã€‚</p><p>ç”±äºâ€œè§£ç å™¨ï¼ˆdecoderï¼Œæœ‰äº›æ—¶å€™ä¹Ÿè¢«ç§°ä¸ºfeature extractorï¼‰â€çš„æ¦‚å¿µä¸åƒç´ çº§çš„åˆ†ç±»ã€å›å½’ç­‰é—®é¢˜å¤šå¤šå°‘å°‘éƒ½æœ‰ç“œè‘›ã€‚ä»¥ä¸‹æ˜¯decoderè¢«åº”ç”¨äºåƒç´ çº§çš„ä»»åŠ¡ï¼š</p><ul><li>åˆ†ç±»ï¼šè¯­ä¹‰åˆ†å‰²ã€è¾¹ç¼˜æ£€æµ‹ã€‚</li><li>å›å½’ï¼šäººä½“å…³é”®ç‚¹æ£€æµ‹ã€æ·±åº¦é¢„æµ‹ã€ç€è‰²ã€è¶…åˆ†è¾¨ã€‚</li><li>åˆæˆï¼šåˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”Ÿæˆå›¾åƒç­‰ã€‚</li></ul><p>æ‰€ä»¥decoderæ˜¯ç¨ å¯†é¢„æµ‹ï¼ˆDence predictionï¼Œåƒç´ çº§åˆ«çš„å¾ˆå¤šé—®é¢˜éƒ½å¯ä»¥å«åšç¨ å¯†çš„ï¼‰é—®é¢˜çš„å…³é”®ã€‚</p><header><h1>Abstractï¼ˆæ‘˜è¦ï¼‰</h1></header><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>æˆ‘çœ‹äº†è¿™ç¯‡ç»¼è¿°å—ç›ŠåŒªæµ…ï¼Œå¦‚æœæœ‰æ—¶é—´çš„è¯è¯·é˜…è¯»<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">åŸä½œ</a>ã€‚æœ¬æ–‡åªæ˜¯å¯¹åŸä½œé˜…è¯»çš„ç²—æµ…ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/survey">survey</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/decoder">decoder</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about The Devil is in the Decoder - Classification, Regression and GANs" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>åŸè®ºæ–‡ï¼š<a href="https://arxiv.org/pdf/2104.03778.pdf" target="_blank" rel="noopener noreferrer">Progressive Semantic Segmentation</a></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="é—®é¢˜æè¿°">é—®é¢˜æè¿°<a aria-hidden="true" class="hash-link" href="#é—®é¢˜æè¿°" title="Direct link to heading">â€‹</a></h2><p>å½“å¯¹å¤§å‹å›¾ç‰‡è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ˜¾å­˜ç‚¸æ‰ã€‚æ”¶åˆ°å†…å­˜é™åˆ¶ï¼Œå¯ä»¥é€‰æ‹©ä¸‹é‡‡æ ·ï¼Œæˆ–å°†å›¾åƒåˆ’åˆ†ä¸ºå±€éƒ¨å—ã€‚ä½†å‰è€…ä¼šä¸¢å¤±ç»†èŠ‚ï¼Œåè€…ä¼šå´åå…¨å±€è§†å›¾ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="åå¤„ç†æ”¹å–„åˆ†å‰²ç»†èŠ‚">åå¤„ç†æ”¹å–„åˆ†å‰²ç»†èŠ‚<a aria-hidden="true" class="hash-link" href="#åå¤„ç†æ”¹å–„åˆ†å‰²ç»†èŠ‚" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ç»å…¸æ–¹æ³•">ç»å…¸æ–¹æ³•<a aria-hidden="true" class="hash-link" href="#ç»å…¸æ–¹æ³•" title="Direct link to heading">â€‹</a></h3><p>æ¡ä»¶éšæœºåœº(CRF),å¼•å¯¼æ»¤æ³¢å™¨ï¼ˆGFï¼‰ï¼Œä¸¤ä¸ªé€Ÿåº¦æ…¢ï¼Œæ”¹è¿›æ˜¯æ¸è¿›çš„ã€‚</p><p>æ·±åº¦å­¦ä¹ çš„å¼•å¯¼è¿‡æ»¤å™¨(DGF)å¯ä»¥æé«˜æ¨ç†é€Ÿåº¦</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Progressive Semantic Segmentation" href="/blog/[03]Progressive-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[06]DeepLab-Series">DeepLab Series</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>DeepLabç³»åˆ—ä¸­åŒ…å«äº†ä¸‰ç¯‡è®ºæ–‡ï¼šDeepLab-v1ã€DeepLab-v2ã€DeepLab-v3ã€‚</p><p>DeepLab-v1ï¼š<a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener noreferrer">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>DeepLab-v2ï¼š<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener noreferrer">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>DeepLab-v3ï¼š<a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener noreferrer">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>åœ¨è¿™é‡Œæˆ‘ä»¬å°†è¿™ä¸‰ç¯‡æ”¾åœ¨ä¸€èµ·é˜…è¯»ã€‚</p><p>åæ¥ç”šè‡³è¿˜å‡ºç°äº†åç»­ï¼š</p><p>DeepLab-v3+ï¼š<a href="https://arxiv.org/abs/1802.02611" target="_blank" rel="noopener noreferrer">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>ä¸è¿‡æš‚æ—¶æ²¡æœ‰å†™è¿›æ¥çš„æ‰“ç®—ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/decoder">decoder</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/atrous-convolution">atrous-convolution</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about DeepLab Series" href="/blog/[06]DeepLab-Series"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/2103.11351" target="_blank" rel="noopener noreferrer">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></p><p>ä½œè€…ï¼š<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+L" target="_blank" rel="noopener noreferrer">Li Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+D" target="_blank" rel="noopener noreferrer">Dong Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu%2C+Y" target="_blank" rel="noopener noreferrer">Yousong Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian%2C+L" target="_blank" rel="noopener noreferrer">Lu Tian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shan%2C+Y" target="_blank" rel="noopener noreferrer">Yi Shan</a></p><p>æœŸåˆŠï¼šCVPR2021</p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä¸»è¦ç»“æ„">ä¸»è¦ç»“æ„<a aria-hidden="true" class="hash-link" href="#ä¸»è¦ç»“æ„" title="Direct link to heading">â€‹</a></h2><p>DABï¼šDataset-Aware Block(æ•°æ®é›†æ„ŸçŸ¥å—)</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#393A34"><span class="token plain">    ä½œä¸ºç½‘ç»œçš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œæœ‰åŠ©äºæ•è·oä¸åŒåŠŸèƒ½æ•°æ®é›†ä¹‹é—´çš„åŒè´¨è¡¨ç¤ºå’Œå¼‚æ„ç»Ÿè®¡ã€‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ä¸»è¦ç”±ï¼Œä¸€ä¸ªæ•°æ®é›†ä¸å˜çš„å·ç§¯å±‚ï¼Œå¤šä¸ªæ•°æ®é›†ç‰¹å®šçš„BatchNormalå’Œä¸€ä¸ªæ¿€æ´»å±‚æ„æˆã€‚</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>DATï¼šDataset Alternation Training(æ•°æ®é›†äº¤æ›¿è®­ç»ƒæœºåˆ¶)</p><p>åˆ†å‰²ç»“æœï¼š</p><p><img alt="image-20210505160138997" src="/assets/images/20210505160141image-20210505160138997-0005e31a81f1a3491314c4432158798b.png"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/cross-dataset-learning">cross-dataset-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Cross-Dataset Collaborative Learning for Semantic Segmentation" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡å…³äºç»¼è¿°è®ºæ–‡çš„è§£è¯»ã€‚<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆA Review on Deep Learning Techniques Applied to Semantic Segmentationï¼‰</a></p><p>æ‘˜è¦ï¼š</p><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>æˆ‘çœ‹äº†è¿™ç¯‡ç»¼è¿°å—ç›ŠåŒªæµ…ï¼Œå¦‚æœæœ‰æ—¶é—´çš„è¯è¯·é˜…è¯»<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">åŸä½œ</a>ã€‚æœ¬æ–‡åªæ˜¯å¯¹åŸä½œé˜…è¯»çš„ç²—æµ…ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/survey">survey</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about A Review on Deep Learning Techniques Applied to Semantic Segmentation" href="/blog/[10]Overview-Of-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§å¿«é€Ÿè¯­ä¹‰åˆ†å‰²çš„è®ºæ–‡ã€‚è®ºæ–‡å:<a href="https://arxiv.org/abs/1902.04502" target="_blank" rel="noopener noreferrer">Fast-SCNN: Fast Semantic Segmentation Network</a></p><ul><li>ä¸»è¦æ˜¯é‡‡ç”¨åŒæµæ¨¡å‹çš„æ¶æ„è®¾è®¡è¿™ä¸ªç½‘ç»œ</li><li>æœ¬æ–‡æ€»æ€è·¯ï¼šå‡å°‘å†—ä½™çš„å·ç§¯è¿‡ç¨‹ï¼Œä»è€Œæé«˜é€Ÿåº¦</li></ul><p>æ‘˜è¦ï¼š</p><blockquote><p>The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024 Ã— 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our â€˜learning to downsampleâ€™ module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Fast-SCNN - Fast Semantic Segmentation Network" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="image-20210601121147760" src="/assets/images/image-20210601121147760-2609d0f40692dce369b8041f749ff63b.png"></p><blockquote><p>â€œæˆ‘ä»¬å¸Œæœ›é¢„æµ‹åˆ†å‰²å›¾çš„è¾¹ç•ŒåŒºåŸŸæ›´åŠ å‡†ç¡®ï¼Œæˆ‘ä»¬å°±ä¸åº”è¯¥ä½¿ç”¨å‡åŒ€é‡‡æ ·ï¼Œè€Œåº”è¯¥æ›´åŠ å€¾å‘äºå›¾åƒè¾¹ç•ŒåŒºåŸŸã€‚â€</p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡ç”¨äºæ”¹å–„å›¾åƒåˆ†å‰²é—®é¢˜ä¸­è¾¹ç¼˜åˆ†å‰²æ•ˆæœçš„æ–¹æ³•çš„è®ºæ–‡çš„é˜…è¯»ç¬”è®°ã€‚è¯¥æ–¹æ³•â€œå°†åˆ†å‰²é—®é¢˜çœ‹ä½œæ¸²æŸ“é—®é¢˜â€ï¼Œè¾¾åˆ°äº†è¾ƒå¥½çš„æ•ˆæœã€‚è®ºæ–‡åŸæ–‡ï¼š<a href="https://arxiv.org/abs/1912.08193" target="_blank" rel="noopener noreferrer">PointRend: Image Segmentation as Rendering</a>ã€‚åœ¨é˜…è¯»è¿™ç¯‡ç¬”è®°ä¹‹å‰ï¼Œè¯·ç¡®ä¿å…ˆäº†è§£å›¾åƒåˆ†å‰²æŠ€æœ¯ã€‚å¯¹åˆ†å‰²çš„æŠ€æœ¯è¿›è¡Œç®€è¦çš„äº†è§£ï¼Œå¯ä»¥å‚è€ƒ<a href="/blog/tags/[10]Overview-Of-Semantic-Segmentation">å¦ä¸€ç¯‡ç¬”è®°</a>ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstractæ‘˜è¦">Abstractï¼ˆæ‘˜è¦ï¼‰<a aria-hidden="true" class="hash-link" href="#abstractæ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend&#x27;s efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend" target="_blank" rel="noopener noreferrer">this https URL</a>.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about PointRend - Image Segmentation as Rendering" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/2104.08569" target="_blank" rel="noopener noreferrer">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></p><p>ä½œè€…ï¼š<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+G" target="_blank" rel="noopener noreferrer">Gang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu%2C+X" target="_blank" rel="noopener noreferrer">Xin Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan%2C+J" target="_blank" rel="noopener noreferrer">Jingru Tan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J" target="_blank" rel="noopener noreferrer">Jianmin Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+Z" target="_blank" rel="noopener noreferrer">Zhaoxiang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+Q" target="_blank" rel="noopener noreferrer">Quanquan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu%2C+X" target="_blank" rel="noopener noreferrer">Xiaolin Hu</a></p><p>æœŸåˆŠï¼šCVPR2021</p><p>ä»£ç ï¼š<a href="https://github.com/zhanggang001/RefineMask" target="_blank" rel="noopener noreferrer">https://github.com/zhanggang001/RefineMask</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="åŸæ–‡æ‘˜è¦">åŸæ–‡æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#åŸæ–‡æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>The  two-stage  methods  for  instance  segmentation,  e.g.Mask  R-CNN,  have  achieved  excellent  performance  re-cently.  However, the segmented masks are still very coarsedue  to  the  downsampling  operations  in  both  the  featurepyramid and the instance-wise pooling process, especiallyfor large objects.  In this work, we propose a new methodcalled  RefineMask  for  high-quality  instance  segmentationof objects and scenes, which incorporates fine-grained fea-tures  during  the  instance-wise  segmenting  process  in  amulti-stage manner. Through fusing more detailed informa-tion stage by stage, RefineMask is able to refine high-qualitymasks  consistently.    RefineMask  succeeds  in  segmentinghard  cases  such  as  bent  parts  of  objects  that  are  over-smoothed by most previous methods and outputs accurateboundaries.  Without bells and whistles, RefineMask yieldssignificant gains of 2.6, 3.4, 3.8 AP over Mask R-CNN onCOCO, LVIS, and Cityscapes benchmarks respectively at asmall  amount  of  additional  computational  cost.   Further-more, our single-model result outperforms the winner of theLVIS Challenge 2020 by 1.3 points on the LVIS test-dev setand establishes a new state-of-the-art.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><p>å³ä½¿å¦‚Mask R-CNNè¿™æ ·äºŒé˜¶æ®µçš„å®ä¾‹åˆ†å‰²ç½‘è·¯å·²ç»æœ‰äº†ä¼˜ç§€çš„è¡¨ç°ï¼Œä½†å› ä¸ºåœ¨ç‰¹å¾é‡‘å­—å¡”å’Œå®ä¾‹æ± åŒ–è¿‡ç¨‹ä¸­ä½¿ç”¨äº†ä¸‹é‡‡æ ·æ“ä½œï¼Œä½¿å¾—åˆ†å‰²æ©ç ä»ç„¶éå¸¸ç²—ç³™ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤§å‹ç‰©ä½“ã€‚</p><p>åœ¨æœ¬æ–‡ä¸­ï¼Œæå‡ºäº†RefineMaskæ–¹æ³•ï¼Œç”¨äºå¯¹è±¡å’Œåœºæ™¯çš„é«˜è´¨é‡å®ä¾‹åˆ†å‰²ï¼Œå®ƒåœ¨å®åˆ†å‰²çš„è¿‡ç¨‹ä¸­ä»¥å¤šé˜¶æ®µçš„æ–¹å¼ç»“åˆäº†ç»†ç²’åº¦ç‰¹å¾ã€‚é€šè¿‡é€æ­¥èåˆæ›´ç»†èŠ‚çš„ä¿¡æ¯ï¼ŒRefineMaskèƒ½å¤Ÿå§‹ç»ˆå¦‚ä¸€åœ°æç‚¼å‡ºé«˜è´¨é‡çš„maskã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>BiSeNetçš„ç›®æ ‡æ˜¯æ›´å¿«é€Ÿçš„å®æ—¶è¯­ä¹‰åˆ†å‰²ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç©ºé—´åˆ†è¾¨ç‡å’Œæ„Ÿå—é‡å¾ˆéš¾ä¸¤å…¨ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶è¯­ä¹‰åˆ†å‰²çš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸æ˜¯åˆ©ç”¨å°çš„è¾“å…¥å›¾åƒæˆ–è€…è½»é‡ä¸»å¹²æ¨¡å‹å®ç°åŠ é€Ÿã€‚ä½†æ˜¯å°å›¾åƒç›¸è¾ƒäºåŸå›¾åƒç¼ºå¤±äº†å¾ˆå¤šç©ºé—´ä¿¡æ¯ï¼Œè€Œè½»é‡çº§æ¨¡å‹åˆ™ç”±äºè£å‰ªé€šé“è€ŒæŸå®³äº†ç©ºé—´ä¿¡æ¯ã€‚BiSegNetæ•´åˆäº†Spatial Path (SP) å’Œ Context Path (CP)åˆ†åˆ«ç”¨æ¥è§£å†³ç©ºé—´ä¿¡æ¯ç¼ºå¤±å’Œæ„Ÿå—é‡ç¼©å°çš„é—®é¢˜ã€‚</p><blockquote><p>Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.</p></blockquote><p>è®ºæ–‡åŸæ–‡ï¼š<a href="https://arxiv.org/abs/1808.00897" target="_blank" rel="noopener noreferrer">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>ã€‚é˜…è¯»åä½ ä¼šå‘ç°ï¼Œè¿™ç¯‡è®ºæ–‡æœ‰å¾ˆå¤šæ€è·¯å—åˆ°<a href="/blog/tags/[23]Squeeze-and-Excitation-Networks">SENetï¼ˆSqueeze-and-Excitation Networksï¼‰</a>çš„å¯å‘ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan%2C+M" target="_blank" rel="noopener noreferrer">Mingyuan Fan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lai%2C+S" target="_blank" rel="noopener noreferrer">Shenqi Lai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+J" target="_blank" rel="noopener noreferrer">Junshi Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+X" target="_blank" rel="noopener noreferrer">Xiaoming Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chai%2C+Z" target="_blank" rel="noopener noreferrer">Zhenhua Chai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo%2C+J" target="_blank" rel="noopener noreferrer">Junfeng Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+X" target="_blank" rel="noopener noreferrer">Xiaolin Wei</a></p><p><img alt="image-20210719132305088" src="/assets/images/image-20210719132305088-c5d44ca09757a5d8bc0ba7e8d56e4611.png"></p><blockquote><p>BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images.</p></blockquote><p>åœ¨é˜…è¯»æœ¬æ–‡å‰ï¼Œè¯·å…ˆé˜…è¯»<a href="/blog/tags/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>ã€‚</p><p>è¯¥è®ºæ–‡æå‡º<a href="/blog/tags/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet</a>è¢«è¯æ˜æ˜¯ä¸é”™çš„åŒè·¯å®æ—¶åˆ†å‰²ç½‘ç»œã€‚ä¸è¿‡ï¼Œåœ¨BiSeNetä¸­ï¼š</p><ul><li>å•ç‹¬ä¸ºç©ºé—´ä¿¡æ¯å¼€è¾Ÿä¸€æ¡ç½‘ç»œè·¯å¾„åœ¨è®¡ç®—ä¸Šéå¸¸çš„è€—æ—¶</li><li>ç”¨äºspatial pathçš„é¢„è®­ç»ƒè½»é‡çº§éª¨å¹²ç½‘ç»œä»å…¶ä»–ä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚åˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ï¼‰ç›´æ¥æ‹¿æ¥ï¼Œç”¨åœ¨åˆ†å‰²ä¸Šæ•ˆç‡ä¸å¾ˆé«˜ã€‚</li></ul><p>å› æ­¤ï¼Œä½œè€…æå‡ºShort-Term Dense Concatenate networkï¼ˆ<strong>STDC</strong> networkï¼‰æ¥ä»£æ›¿BiSeNetä¸­çš„context pathã€‚å…¶æ ¸å¿ƒå†…å®¹æ˜¯ç§»é™¤å†—ä½™çš„ç»“æ„ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡å°†ç‰¹å¾å›¾çš„ç»´æ•°é€æ¸é™ä½ï¼Œå¹¶å°†ç‰¹å¾å›¾èšåˆèµ·æ¥è¿›è¡Œå›¾åƒè¡¨å¾ï¼Œå½¢æˆäº†STDCç½‘ç»œçš„åŸºæœ¬æ¨¡å—ã€‚åŒæ—¶ï¼Œåœ¨decoderä¸­æå‡ºDetail Aggregation moduleå°†ç©ºé—´ä¿¡æ¯çš„å­¦ä¹ ä»¥single-streamæ–¹å¼é›†æˆåˆ°low-level layersä¸­ï¼Œç”¨äºä»£æ›¿BiSeNetä¸­çš„spatial pathã€‚æœ€åï¼Œå°†low-level featureså’Œdeep featuresèåˆä»¥é¢„æµ‹æœ€ç»ˆçš„åˆ†å‰²ç»“æœã€‚</p><p><img alt="image-20210719100139212" src="/assets/images/image-20210719100139212-5b65daa15693025259898e861ccdf07c.png"></p><p>æ³¨ï¼šä¸Šå›¾ä¸­çº¢è‰²è™šçº¿æ¡†ä¸­çš„éƒ¨åˆ†æ˜¯æ–°æå‡ºçš„STDC networkï¼›ARMè¡¨ç¤ºæ³¨æ„åŠ›ä¼˜åŒ–æ¨¡å—ï¼ˆAttention Refinement Moduleï¼‰ï¼ŒFFMè¡¨ç¤ºç‰¹å¾èåˆæ¨¡å—ï¼ˆFeature Fusion Moduleï¼‰ã€‚è¿™ä¸¤ä¸ªæ¨¡å—æ˜¯åœ¨<a href="/blog/tags/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a>å°±å·²ç»å­˜åœ¨çš„è®¾è®¡ã€‚</p><p>æœ‰å…´è¶£è¯·é˜…è¯»åŸè®ºæ–‡<a href="https://arxiv.org/abs/2104.13188" target="_blank" rel="noopener noreferrer">Rethinking BiSeNet For Real-time Semantic Segmentation</a>ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/inductive-bias">inductive-bias</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Rethinking BiSeNet For Real-time Semantic Segmentation" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+X" target="_blank" rel="noopener noreferrer">Xinlong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kong%2C+T" target="_blank" rel="noopener noreferrer">Tao Kong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen%2C+C" target="_blank" rel="noopener noreferrer">Chunhua Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang%2C+Y" target="_blank" rel="noopener noreferrer">Yuning Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+L" target="_blank" rel="noopener noreferrer">Lei Li</a></p><blockquote><p>We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the &#x27;detect-thensegment&#x27; strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of &quot;instance categories&quot;, which assigns categories to each pixel within an instance according to the instance&#x27;s location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.</p></blockquote><p>å®ä¾‹åˆ†å‰²ç›¸æ¯”äºè¯­ä¹‰åˆ†å‰²ï¼Œä¸ä»…éœ€è¦é¢„æµ‹å‡ºæ¯ä¸€ä¸ªåƒç´ ç‚¹çš„è¯­ä¹‰ç±»åˆ«ï¼Œè¿˜è¦åˆ¤æ–­å‡ºè¯¥åƒç´ ç‚¹å±äºå“ªä¸€ä¸ªå®ä¾‹ã€‚ä»¥å¾€<strong>äºŒé˜¶æ®µ</strong>çš„æ–¹æ³•ä¸»è¦æ˜¯ï¼š</p><ol><li>å…ˆæ£€æµ‹ååˆ†å‰²ï¼šä¾‹å¦‚Mask R-CNN ï¼Œå…ˆç”¨æ£€æµ‹çš„æ–¹æ³•åˆ°å¾—æ¯ä¸€ä¸ªå®ä¾‹ï¼Œç„¶åå¯¹è¯¥å®ä¾‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œåˆ†å‰²å¾—åˆ°çš„åƒç´ éƒ½å±äºæ­¤å®ä¾‹ã€‚</li><li>å…ˆåˆ†å‰²ååˆ†ç±»ï¼šå…ˆé‡‡ç”¨è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•å¯¹æ•´ä¸ªå›¾çš„æ‰€æœ‰åƒç´ ç‚¹åšè¯­ä¹‰ç±»åˆ«çš„é¢„æµ‹ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œä½¿ç”¨èšç±»æ–¹æ³•æ‹‰è¿‘å±äºåŒä¸€å®ä¾‹çš„åƒç´ ç‚¹ï¼Œä½¿å®ƒä»¬å±äºåŒä¸€ç±»ï¼ˆåŒä¸ªå®ä½“ï¼‰ã€‚</li></ol><p>å•é˜¶æ®µæ–¹æ³•ï¼ˆSingle Stage Instance Segmentationï¼‰æ–¹é¢çš„å·¥ä½œå—åˆ°å•é˜¶æ®µç›®æ ‡æ£€æµ‹çš„å½±å“å¤§ä½“ä¸Šä¹Ÿåˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç§æ˜¯å—one-stage, anchot-basedæ£€æµ‹æ¨¡å‹å¦‚YOLOï¼ŒRetinaNetå¯å‘ï¼Œä»£è¡¨ä½œæœ‰YOLACTå’ŒSOLOï¼›ä¸€ç§æ˜¯å—anchor-freeæ£€æµ‹æ¨¡å‹å¦‚ FCOS å¯å‘ï¼Œä»£è¡¨ä½œæœ‰PolarMaskå’ŒAdaptISã€‚ä¸Šè¿°è¿™äº›å®ä¾‹åˆ†å‰²çš„æ–¹æ³•éƒ½ä¸é‚£ä¹ˆç›´æ¥ï¼Œä¹Ÿä¸é‚£ä¹ˆç®€å•ã€‚SOLOçš„å‡ºå‘ç‚¹å°±æ˜¯åšæ›´ç®€å•ã€æ›´ç›´æ¥çš„å®ä¾‹åˆ†å‰²ã€‚</p><p>åŸºäºå¯¹MSCOCOæ•°æ®é›†çš„ç»Ÿè®¡ï¼Œä½œè€…æå‡ºï¼ŒéªŒè¯å­é›†ä¸­æ€»å…±æœ‰36780ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­98.3ï¼…çš„å¯¹è±¡å¯¹çš„ä¸­å¿ƒè·ç¦»å¤§äº30ä¸ªåƒç´ ã€‚è‡³äºå…¶ä½™çš„1.7ï¼…çš„å¯¹è±¡å¯¹ï¼Œå…¶ä¸­40.5ï¼…çš„å¤§å°æ¯”ç‡å¤§äº1.5å€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸è€ƒè™‘åƒXå½¢ä¸¤ä¸ªç‰©ä½“è¿™æ ·çš„å°‘æ•°æƒ…å†µã€‚æ€»ä¹‹ï¼Œ<strong>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå›¾åƒä¸­çš„ä¸¤ä¸ªå®ä¾‹è¦ä¹ˆå…·æœ‰ä¸åŒçš„ä¸­å¿ƒä½ç½®ï¼Œè¦ä¹ˆå…·æœ‰ä¸åŒçš„å¯¹è±¡å¤§å°</strong>ã€‚</p><p>äºæ˜¯ä½œè€…æå‡ºé€šè¿‡ç‰©ä½“åœ¨å›¾ç‰‡ä¸­çš„<strong>ä½ç½®</strong>å’Œ<strong>å½¢çŠ¶</strong>æ¥è¿›è¡Œå®ä¾‹çš„åŒºåˆ†ã€‚åŒä¸€å¼ å›¾ç‰‡ä¸­ï¼Œä½ç½®å’Œå½¢çŠ¶å®Œå…¨ç›¸åŒï¼Œå°±æ˜¯åŒä¸€ä¸ªå®ä¾‹ï¼Œç”±äºå½¢çŠ¶æœ‰å¾ˆå¤šæ–¹é¢ï¼Œæ–‡ç« ä¸­æœ´ç´ åœ°ä½¿ç”¨å°ºå¯¸æè¿°å½¢çŠ¶ã€‚</p><p>è¯¥æ–¹æ³•ä¸ Mask R-CNN å®ç°äº†åŒç­‰å‡†ç¡®åº¦ï¼Œå¹¶ä¸”åœ¨å‡†ç¡®åº¦ä¸Šä¼˜äºæœ€è¿‘çš„å•æ¬¡å®ä¾‹åˆ†å‰²å™¨ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-branch">multi-branch</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about SOLO - Segmenting Objects by Locations" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bolya%2C+D" target="_blank" rel="noopener noreferrer">Daniel Bolya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+C" target="_blank" rel="noopener noreferrer">Chong Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao%2C+F" target="_blank" rel="noopener noreferrer">Fanyi Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+Y+J" target="_blank" rel="noopener noreferrer">Yong Jae Lee</a></p><blockquote><p>We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn&#x27;t depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.</p></blockquote><p>YOLACTæ˜¯You Only Look At CoefficienTs çš„ç®€å†™ï¼Œå…¶ä¸­ coefficients æ˜¯è¿™ä¸ªæ¨¡å‹çš„è¾“å‡ºä¹‹ä¸€ï¼Œè¿™ä¸ªå‘½åé£æ ¼åº”æ˜¯è‡´æ•¬äº†å¦ä¸€ç›®æ ‡æ£€æµ‹æ¨¡å‹ YOLOã€‚</p><p><img alt="image-20210818180207356" src="/assets/images/image-20210818180207356-905e78fcdbb7ba866da030d933f9b7dc.png"></p><p>ä¸Šå›¾ï¼šYOLACTçš„ç½‘ç»œç»“æ„å›¾ã€‚<strong>YOLACTçš„ç›®æ ‡æ˜¯å°†æ©æ¨¡åˆ†æ”¯æ·»åŠ åˆ°ç°æœ‰çš„ä¸€é˜¶æ®µï¼ˆone-stageï¼‰ç›®æ ‡æ£€æµ‹æ¨¡å‹</strong>ã€‚æˆ‘ä¸ªäººè§‰å¾—è¿™æ˜¯å¤¹åœ¨ä¸€é˜¶æ®µå’ŒäºŒé˜¶æ®µä¸­é—´çš„äº§ç‰©ã€‚å°†å…¶åˆ†ä¸ºä¸€é˜¶æ®µçš„ä¾æ®æ˜¯å…¶å®ç°â€œå°†æ©æ¨¡åˆ†æ”¯æ·»åŠ åˆ°ç°æœ‰çš„ä¸€é˜¶æ®µç›®æ ‡æ£€æµ‹æ¨¡å‹â€çš„æ–¹å¼ä¸Mask R-CNNå¯¹ Faster-CNN æ“ä½œç›¸åŒï¼Œä½†æ²¡æœ‰è¯¸å¦‚feature repoolingå’ŒROI alignç­‰æ˜ç¡®çš„ç›®æ ‡å®šä½æ­¥éª¤ã€‚ä¹Ÿå°±æ˜¯ï¼Œ<code>å®šä½-åˆ†ç±»-åˆ†å‰²</code>çš„æ“ä½œè¢«å˜æˆäº†<code>åˆ†å‰²-å‰ªè£</code>ã€‚</p><p>æ ¹æ®è¯„ä¼°ï¼Œå½“YOLACT å¤„ç†<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>550</mn><mo>Ã—</mo><mn>550</mn></mrow><annotation encoding="application/x-tex">550\times 550</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">550</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">550</span></span></span></span></span>â€‹â€‹â€‹å¤§å°çš„å›¾ç‰‡æ—¶ï¼Œå…¶é€Ÿåº¦è¾¾åˆ°äº† 33FPSï¼Œè€Œäº’è”ç½‘ä¸Šå¤šæ•°è§†é¢‘ä¸€èˆ¬æ˜¯ 30FPS çš„ï¼Œè¿™ä¹Ÿå°±æ˜¯å®æ—¶çš„å«ä¹‰äº†ã€‚è¿™æ˜¯å•é˜¶æ®µæ¯”è¾ƒæ—©çš„ä¸€ä»½å·¥ä½œï¼Œè™½ç„¶è¿™ä¸ªé€Ÿåº¦ä¸å¿«ä½†ä¹Ÿè¿˜è¡Œäº†ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-branch">multi-branch</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about YOLACT - Real-time Instance Segmentation" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai%2C+J" target="_blank" rel="noopener noreferrer">Jifeng Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" target="_blank" rel="noopener noreferrer">Kaiming He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+Y" target="_blank" rel="noopener noreferrer">Yi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren%2C+S" target="_blank" rel="noopener noreferrer">Shaoqing Ren</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" target="_blank" rel="noopener noreferrer">Jian Sun</a></p><blockquote><p>Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.</p></blockquote><p>è¿™ç¯‡å·¥ä½œåˆåInstanceFCNã€‚å®ä¾‹åˆ†å‰²æ–¹é¢ï¼Œç”±äºç½‘ç»œéš¾ä»¥åŒæ—¶è¿›è¡Œåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ï¼Œå› æ­¤é¦–å…ˆæµè¡Œçš„æ˜¯äºŒé˜¶æ®µå®ä¾‹åˆ†å‰²ç½‘ç»œï¼Œé¦–å…ˆå¯¹è¾“å…¥æ‰¾åˆ°å®ä¾‹çš„proposalï¼Œç„¶ååœ¨å…¶ä¸­è¿›è¡Œå¯†é›†é¢„æµ‹ï¼ˆä¹Ÿå°±æ˜¯å…ˆæ¡†æ¡†å†åˆ†å‰²ï¼‰ã€‚æœ¬æ–‡ä»åç§°ä¸Šçœ‹ä¸æ˜¯ä¸€ç¯‡è®²å®ä¾‹åˆ†å‰²çš„æ–‡ç« ï¼Œæ˜¯è®²å¦‚ä½•é€šè¿‡FCNè·å¾—å®ä¾‹çº§åˆ«çš„åˆ†å‰²maskçš„çš„ã€‚</p><p>åœ¨é˜…è¯»ä¹‹å‰æˆ‘æƒ³æé†’ä¸€ä¸‹ï¼Œè¿™ç¯‡å·¥ä½œçš„æ•ˆæœæ˜¯æ¯”è¾ƒå·®çš„ï¼Œæ¯•ç«Ÿæ˜¯æ—©æœŸå·¥ä½œã€‚ä¸è¿‡è¿™ç¯‡å·¥ä½œå…·æœ‰ä¸é”™çš„å¯å‘æ„ä¹‰ï¼Œå€¼å¾—è¯»ä¸€è¯»ã€‚åé¢çš„ä¸€ç¯‡å·¥ä½œFCISï¼ˆFully Convolutional Instance-aware Semantic Segmentationï¼‰ä¸­å°±å€Ÿé‰´äº†æœ¬æ–‡ä¸­æå‡ºçš„instance-sensitive score mapsï¼ˆè¯·ä¸è¦å¼„æ··æœ¬ç¯‡å·¥ä½œå’ŒFCISï¼‰ã€‚æœ¬æ–‡çš„ä¸€å¤§è´¡çŒ®å°±æ˜¯æå‡ºä½¿ç”¨instance-sensitive score mapsåŒºåˆ†ä¸åŒä¸ªä½“ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fcn">fcn</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Instance-sensitive Fully Convolutional Networks" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>