<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">One post tagged with &quot;backbone&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" property="og:title" content="One post tagged with &quot;backbone&quot; | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/tags/backbone"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/tags/backbone"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/backbone" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/tags/backbone" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">é­”æ³•éƒ¨æ—¥å¿—</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>One post tagged with &quot;backbone&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/asthestarsfalll" target="_blank" rel="noopener noreferrer">AsTheStarsFall</a>ã€‚</p><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/2101.03697" target="_blank" rel="noopener noreferrer">RepVGG: Making VGG-style ConvNets Great Again</a></p><p>ä½œè€…ï¼šXiaohan Dingï¼ŒXiangyu Zhangï¼ŒNingning Maï¼ŒJungong Hanï¼ŒGuiguang Dingï¼ŒJian Sun</p><p>Codeï¼š<a href="https://github.com/DingXiaoH/RepVGG" target="_blank" rel="noopener noreferrer">https://github.com/DingXiaoH/RepVGG</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><ol><li>æå‡ºäº†ä¸€ç§ç®€å•å¼ºå¤§çš„CNNï¼Œæ¨ç†æ—¶å…¶æ‹¥æœ‰VGGç±»ä¼¼çš„plainç»“æ„ï¼Œä»…ç”±å·ç§¯å’ŒReLUç»„æˆï¼›è®­ç»ƒæ—¶æ‹¥æœ‰å¤šåˆ†æ”¯çš„æ‹“æ‰‘ç»“æ„</li><li>å¾—ç›Šäºç»“æ„é‡å‚åŒ–ï¼ˆre-parameterizationï¼‰æŠ€æœ¯ï¼ŒRepVGGè¿è¡Œé€Ÿåº¦æ¯”ResNet-50å¿«83%ï¼Œæ¯”ResNet-101å¿«101%ï¼Œå¹¶ä¸”å…·æœ‰æ›´é«˜çš„ç²¾åº¦ã€‚</li></ol><img src="https://i.loli.net/2021/09/14/H4NT17LA635BQgK.png"></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about RepVGG - Making VGG-style ConvNets Great Again" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[06]DeepLab-Series">DeepLab Series</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>DeepLabç³»åˆ—ä¸­åŒ…å«äº†ä¸‰ç¯‡è®ºæ–‡ï¼šDeepLab-v1ã€DeepLab-v2ã€DeepLab-v3ã€‚</p><p>DeepLab-v1ï¼š<a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener noreferrer">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>DeepLab-v2ï¼š<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener noreferrer">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>DeepLab-v3ï¼š<a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener noreferrer">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>åœ¨è¿™é‡Œæˆ‘ä»¬å°†è¿™ä¸‰ç¯‡æ”¾åœ¨ä¸€èµ·é˜…è¯»ã€‚</p><p>åæ¥ç”šè‡³è¿˜å‡ºç°äº†åç»­ï¼š</p><p>DeepLab-v3+ï¼š<a href="https://arxiv.org/abs/1802.02611" target="_blank" rel="noopener noreferrer">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>ä¸è¿‡æš‚æ—¶æ²¡æœ‰å†™è¿›æ¥çš„æ‰“ç®—ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/decoder">decoder</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/atrous-convolution">atrous-convolution</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about DeepLab Series" href="/blog/[06]DeepLab-Series"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust</a>ã€‚<a aria-hidden="true" class="hash-link" href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust" title="Direct link to heading">â€‹</a></h3><p>åŸè®ºæ–‡<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Networks for Object Detection</a>ã€‚</p><p>è¿™ç¯‡è®ºæ–‡å°±æ˜¯å¤§å®¶ç†ŸçŸ¥çš„FPNäº†ã€‚FPNæ˜¯<strong>æ¯”è¾ƒæ—©æœŸçš„ä¸€ä»½å·¥ä½œ</strong>ï¼ˆè¯·æ³¨æ„ï¼Œè¿™ç¯‡è®ºæ–‡åªæ˜¯å¤šå°ºåº¦ç‰¹å¾èåˆçš„ä¸€ç§æ–¹å¼ã€‚ä¸è¿‡è¿™ç¯‡è®ºæ–‡æå‡ºçš„æ¯”è¾ƒæ—©ï¼ˆCVPR2017ï¼‰ï¼Œåœ¨å½“æ—¶çœ‹æ¥æ˜¯éå¸¸å…ˆè¿›çš„ï¼‰ï¼Œåœ¨å½“æ—¶å…·æœ‰å¾ˆå¤šäº®ç‚¹ï¼šFPNä¸»è¦è§£å†³çš„æ˜¯ç‰©ä½“æ£€æµ‹ä¸­çš„å¤šå°ºåº¦é—®é¢˜ï¼Œé€šè¿‡ç®€å•çš„ç½‘ç»œè¿æ¥æ”¹å˜ï¼Œåœ¨åŸºæœ¬ä¸å¢åŠ åŸæœ‰æ¨¡å‹è®¡ç®—é‡æƒ…å†µä¸‹ï¼Œå¤§å¹…åº¦æå‡äº†å°ç‰©ä½“æ£€æµ‹çš„æ€§èƒ½ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstractæ‘˜è¦">Abstractï¼ˆæ‘˜è¦ï¼‰<a aria-hidden="true" class="hash-link" href="#abstractæ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></blockquote><p>è¿™ç¯‡è®ºæ–‡å¯¹ä»¥åçš„è®¸å¤šç½‘ç»œè®¾è®¡äº§ç”Ÿäº†è¾ƒå¤§çš„å½±å“ï¼Œæ¨èä½ é˜…è¯»<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">åŸæ–‡</a>ã€‚è¿™é‡Œåªæ˜¯å¯¹è¿™ç¯‡è®ºæ–‡çš„ç²—æµ…é˜…è¯»ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fpn">FPN</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Feature Pyramid Networks for Object Detection" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§è½»é‡çº§ä¸»å¹²ç½‘ç»œçš„è®ºæ–‡ã€‚<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆMobileNetV2: Inverted Residuals and Linear Bottlenecksï¼‰</a>ã€‚</p><ul><li>æœ¬æ–‡ä¸»è¦é’ˆå¯¹è½»é‡ç‰¹å¾æå–ç½‘ç»œä¸­ç»“æ„ä¸Šçš„ä¸‰ä¸ªä¿®æ”¹æé«˜äº†ç½‘ç»œæ€§èƒ½ã€‚</li><li>æœ¬æ–‡æ€»æ€è·¯ï¼šä½¿ç”¨ä½ç»´åº¦çš„å¼ é‡å¾—åˆ°è¶³å¤Ÿå¤šçš„ç‰¹å¾</li></ul><p>æ‘˜è¦:</p><blockquote><p>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and bench- marks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottle- neck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demon- strate that this improves performance and provide an in- tuition that led to this design. Finally, our approach allows decoupling of the in- put/output domains from the expressiveness of the trans- formation, which provides a convenient framework for further analysis. We measure our performance on ImageNet classification, COCO object detection <!-- -->[2]<!-- -->, VOC image segmentation <!-- -->[3]<!-- -->. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MobileNetV2 - Inverted Residuals and Linear Bottlenecks" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§å¿«é€Ÿè¯­ä¹‰åˆ†å‰²çš„è®ºæ–‡ã€‚è®ºæ–‡å:<a href="https://arxiv.org/abs/1902.04502" target="_blank" rel="noopener noreferrer">Fast-SCNN: Fast Semantic Segmentation Network</a></p><ul><li>ä¸»è¦æ˜¯é‡‡ç”¨åŒæµæ¨¡å‹çš„æ¶æ„è®¾è®¡è¿™ä¸ªç½‘ç»œ</li><li>æœ¬æ–‡æ€»æ€è·¯ï¼šå‡å°‘å†—ä½™çš„å·ç§¯è¿‡ç¨‹ï¼Œä»è€Œæé«˜é€Ÿåº¦</li></ul><p>æ‘˜è¦ï¼š</p><blockquote><p>The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024 Ã— 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our â€˜learning to downsampleâ€™ module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Fast-SCNN - Fast Semantic Segmentation Network" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§è½»é‡çº§ä¸»å¹²ç½‘ç»œçš„è®ºæ–‡ã€‚<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applicationsï¼‰</a>ã€‚</p><ul><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åº”ç”¨äºç§»åŠ¨æˆ–è€…åµŒå…¥å¼è®¾å¤‡çš„é«˜æ•ˆç¥ç»ç½‘ç»œ</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ“ä½œæ•°è¾ƒå°çš„å·ç§¯æ¨¡å—æ·±åº¦å¯åˆ†ç¦»å·ç§¯(Depthwise Separable Convolutionï¼Œä»¥ä¸‹ç§°DSC)</li></ul><p>æ‘˜è¦:</p><blockquote><p>We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://arxiv.org/abs/1808.04560" target="_blank" rel="noopener noreferrer">Deep Retinex Decomposition for Low-Light Enhancement</a></p><p>è®ºæ–‡ä½œè€…: Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu</p><p>Code: <a href="https://github.com/weichen582/RetinexNet" target="_blank" rel="noopener noreferrer">https://github.com/weichen582/RetinexNet</a></p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>é‡‡ç”¨äº†åˆ†è§£ç½‘ç»œå’Œå¢å¼ºç½‘ç»œï¼Œä½¿ç”¨Retinexç†è®ºæ„å»ºåˆ†è§£ç½‘ç»œï¼Œåˆ†è§£åå†è¿›è¡Œå¢å¼ºã€‚</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deepRetinex-Netlearned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjusment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.</p></blockquote><p>Retinexæ¨¡å‹æ˜¯å¼±å…‰å›¾åƒå¢å¼ºçš„æœ‰æ•ˆå·¥å…·ã€‚å®ƒå‡è®¾è§‚å¯Ÿåˆ°çš„å›¾åƒ<strong>å¯ä»¥åˆ†è§£ä¸ºåå°„ç‡å’Œç…§åº¦</strong>ã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºè§†ç½‘è†œçš„æ–¹æ³•éƒ½ä¸ºè¿™ç§é«˜åº¦ä¸é€‚å®šçš„åˆ†è§£ç²¾å¿ƒè®¾è®¡äº†æ‰‹å·¥åˆ¶ä½œçš„çº¦æŸå’Œå‚æ•°ï¼Œå½“åº”ç”¨äºå„ç§åœºæ™¯æ—¶ï¼Œè¿™å¯èƒ½ä¼šå—åˆ°æ¨¡å‹å®¹é‡çš„é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå¯¹çš„å¼±å…‰æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šæå‡ºäº†ä¸€ä¸ªDeeprinex Netlearnï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºåˆ†è§£çš„Decom-Netå’Œä¸€ä¸ªç”¨äºå…‰ç…§è°ƒæ•´çš„å¢å¼º-Netã€‚åœ¨Decom-Netçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å­˜åœ¨åˆ†è§£åå°„ç‡å’Œå…‰ç…§çš„åŸºæœ¬äº‹å®ã€‚è¯¥ç½‘ç»œä»…åœ¨å…³é”®çº¦æŸæ¡ä»¶ä¸‹å­¦ä¹ ï¼ŒåŒ…æ‹¬æˆå¯¹çš„å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå…±äº«çš„ä¸€è‡´åå°„ç‡ä»¥åŠç…§æ˜çš„å¹³æ»‘åº¦ã€‚åœ¨åˆ†è§£çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¢å¼ºç½‘ç»œå¯¹å…‰ç…§è¿›è¡Œåç»­çš„äº®åº¦å¢å¼ºï¼Œå¯¹äºè”åˆå»å™ªï¼Œå¯¹åå°„ç‡è¿›è¡Œå»å™ªæ“ä½œã€‚RetinexNetæ˜¯ç«¯åˆ°ç«¯è®­ç»ƒçš„ï¼Œå› æ­¤å­¦ä¹ çš„åˆ†è§£æœ¬è´¨ä¸Šæœ‰åˆ©äºäº®åº¦è°ƒæ•´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¼±å…‰å¢å¼ºæ–¹é¢è·å¾—äº†è§†è§‰ä¸Šä»¤äººæ»¡æ„çš„è´¨é‡ï¼Œè€Œä¸”æä¾›äº†å›¾åƒåˆ†è§£çš„è‰¯å¥½è¡¨ç¤ºã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Deep Retinex Decomposition for Low-Light Enhancement" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+X" target="_blank" rel="noopener noreferrer">Xinlong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kong%2C+T" target="_blank" rel="noopener noreferrer">Tao Kong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen%2C+C" target="_blank" rel="noopener noreferrer">Chunhua Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang%2C+Y" target="_blank" rel="noopener noreferrer">Yuning Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+L" target="_blank" rel="noopener noreferrer">Lei Li</a></p><blockquote><p>We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the &#x27;detect-thensegment&#x27; strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of &quot;instance categories&quot;, which assigns categories to each pixel within an instance according to the instance&#x27;s location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.</p></blockquote><p>å®ä¾‹åˆ†å‰²ç›¸æ¯”äºè¯­ä¹‰åˆ†å‰²ï¼Œä¸ä»…éœ€è¦é¢„æµ‹å‡ºæ¯ä¸€ä¸ªåƒç´ ç‚¹çš„è¯­ä¹‰ç±»åˆ«ï¼Œè¿˜è¦åˆ¤æ–­å‡ºè¯¥åƒç´ ç‚¹å±äºå“ªä¸€ä¸ªå®ä¾‹ã€‚ä»¥å¾€<strong>äºŒé˜¶æ®µ</strong>çš„æ–¹æ³•ä¸»è¦æ˜¯ï¼š</p><ol><li>å…ˆæ£€æµ‹ååˆ†å‰²ï¼šä¾‹å¦‚Mask R-CNN ï¼Œå…ˆç”¨æ£€æµ‹çš„æ–¹æ³•åˆ°å¾—æ¯ä¸€ä¸ªå®ä¾‹ï¼Œç„¶åå¯¹è¯¥å®ä¾‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œåˆ†å‰²å¾—åˆ°çš„åƒç´ éƒ½å±äºæ­¤å®ä¾‹ã€‚</li><li>å…ˆåˆ†å‰²ååˆ†ç±»ï¼šå…ˆé‡‡ç”¨è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•å¯¹æ•´ä¸ªå›¾çš„æ‰€æœ‰åƒç´ ç‚¹åšè¯­ä¹‰ç±»åˆ«çš„é¢„æµ‹ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œä½¿ç”¨èšç±»æ–¹æ³•æ‹‰è¿‘å±äºåŒä¸€å®ä¾‹çš„åƒç´ ç‚¹ï¼Œä½¿å®ƒä»¬å±äºåŒä¸€ç±»ï¼ˆåŒä¸ªå®ä½“ï¼‰ã€‚</li></ol><p>å•é˜¶æ®µæ–¹æ³•ï¼ˆSingle Stage Instance Segmentationï¼‰æ–¹é¢çš„å·¥ä½œå—åˆ°å•é˜¶æ®µç›®æ ‡æ£€æµ‹çš„å½±å“å¤§ä½“ä¸Šä¹Ÿåˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç§æ˜¯å—one-stage, anchot-basedæ£€æµ‹æ¨¡å‹å¦‚YOLOï¼ŒRetinaNetå¯å‘ï¼Œä»£è¡¨ä½œæœ‰YOLACTå’ŒSOLOï¼›ä¸€ç§æ˜¯å—anchor-freeæ£€æµ‹æ¨¡å‹å¦‚ FCOS å¯å‘ï¼Œä»£è¡¨ä½œæœ‰PolarMaskå’ŒAdaptISã€‚ä¸Šè¿°è¿™äº›å®ä¾‹åˆ†å‰²çš„æ–¹æ³•éƒ½ä¸é‚£ä¹ˆç›´æ¥ï¼Œä¹Ÿä¸é‚£ä¹ˆç®€å•ã€‚SOLOçš„å‡ºå‘ç‚¹å°±æ˜¯åšæ›´ç®€å•ã€æ›´ç›´æ¥çš„å®ä¾‹åˆ†å‰²ã€‚</p><p>åŸºäºå¯¹MSCOCOæ•°æ®é›†çš„ç»Ÿè®¡ï¼Œä½œè€…æå‡ºï¼ŒéªŒè¯å­é›†ä¸­æ€»å…±æœ‰36780ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­98.3ï¼…çš„å¯¹è±¡å¯¹çš„ä¸­å¿ƒè·ç¦»å¤§äº30ä¸ªåƒç´ ã€‚è‡³äºå…¶ä½™çš„1.7ï¼…çš„å¯¹è±¡å¯¹ï¼Œå…¶ä¸­40.5ï¼…çš„å¤§å°æ¯”ç‡å¤§äº1.5å€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸è€ƒè™‘åƒXå½¢ä¸¤ä¸ªç‰©ä½“è¿™æ ·çš„å°‘æ•°æƒ…å†µã€‚æ€»ä¹‹ï¼Œ<strong>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå›¾åƒä¸­çš„ä¸¤ä¸ªå®ä¾‹è¦ä¹ˆå…·æœ‰ä¸åŒçš„ä¸­å¿ƒä½ç½®ï¼Œè¦ä¹ˆå…·æœ‰ä¸åŒçš„å¯¹è±¡å¤§å°</strong>ã€‚</p><p>äºæ˜¯ä½œè€…æå‡ºé€šè¿‡ç‰©ä½“åœ¨å›¾ç‰‡ä¸­çš„<strong>ä½ç½®</strong>å’Œ<strong>å½¢çŠ¶</strong>æ¥è¿›è¡Œå®ä¾‹çš„åŒºåˆ†ã€‚åŒä¸€å¼ å›¾ç‰‡ä¸­ï¼Œä½ç½®å’Œå½¢çŠ¶å®Œå…¨ç›¸åŒï¼Œå°±æ˜¯åŒä¸€ä¸ªå®ä¾‹ï¼Œç”±äºå½¢çŠ¶æœ‰å¾ˆå¤šæ–¹é¢ï¼Œæ–‡ç« ä¸­æœ´ç´ åœ°ä½¿ç”¨å°ºå¯¸æè¿°å½¢çŠ¶ã€‚</p><p>è¯¥æ–¹æ³•ä¸ Mask R-CNN å®ç°äº†åŒç­‰å‡†ç¡®åº¦ï¼Œå¹¶ä¸”åœ¨å‡†ç¡®åº¦ä¸Šä¼˜äºæœ€è¿‘çš„å•æ¬¡å®ä¾‹åˆ†å‰²å™¨ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-branch">multi-branch</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about SOLO - Segmenting Objects by Locations" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bolya%2C+D" target="_blank" rel="noopener noreferrer">Daniel Bolya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+C" target="_blank" rel="noopener noreferrer">Chong Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao%2C+F" target="_blank" rel="noopener noreferrer">Fanyi Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+Y+J" target="_blank" rel="noopener noreferrer">Yong Jae Lee</a></p><blockquote><p>We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn&#x27;t depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.</p></blockquote><p>YOLACTæ˜¯You Only Look At CoefficienTs çš„ç®€å†™ï¼Œå…¶ä¸­ coefficients æ˜¯è¿™ä¸ªæ¨¡å‹çš„è¾“å‡ºä¹‹ä¸€ï¼Œè¿™ä¸ªå‘½åé£æ ¼åº”æ˜¯è‡´æ•¬äº†å¦ä¸€ç›®æ ‡æ£€æµ‹æ¨¡å‹ YOLOã€‚</p><p><img alt="image-20210818180207356" src="/assets/images/image-20210818180207356-905e78fcdbb7ba866da030d933f9b7dc.png"></p><p>ä¸Šå›¾ï¼šYOLACTçš„ç½‘ç»œç»“æ„å›¾ã€‚<strong>YOLACTçš„ç›®æ ‡æ˜¯å°†æ©æ¨¡åˆ†æ”¯æ·»åŠ åˆ°ç°æœ‰çš„ä¸€é˜¶æ®µï¼ˆone-stageï¼‰ç›®æ ‡æ£€æµ‹æ¨¡å‹</strong>ã€‚æˆ‘ä¸ªäººè§‰å¾—è¿™æ˜¯å¤¹åœ¨ä¸€é˜¶æ®µå’ŒäºŒé˜¶æ®µä¸­é—´çš„äº§ç‰©ã€‚å°†å…¶åˆ†ä¸ºä¸€é˜¶æ®µçš„ä¾æ®æ˜¯å…¶å®ç°â€œå°†æ©æ¨¡åˆ†æ”¯æ·»åŠ åˆ°ç°æœ‰çš„ä¸€é˜¶æ®µç›®æ ‡æ£€æµ‹æ¨¡å‹â€çš„æ–¹å¼ä¸Mask R-CNNå¯¹ Faster-CNN æ“ä½œç›¸åŒï¼Œä½†æ²¡æœ‰è¯¸å¦‚feature repoolingå’ŒROI alignç­‰æ˜ç¡®çš„ç›®æ ‡å®šä½æ­¥éª¤ã€‚ä¹Ÿå°±æ˜¯ï¼Œ<code>å®šä½-åˆ†ç±»-åˆ†å‰²</code>çš„æ“ä½œè¢«å˜æˆäº†<code>åˆ†å‰²-å‰ªè£</code>ã€‚</p><p>æ ¹æ®è¯„ä¼°ï¼Œå½“YOLACT å¤„ç†<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>550</mn><mo>Ã—</mo><mn>550</mn></mrow><annotation encoding="application/x-tex">550\times 550</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">550</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">550</span></span></span></span></span>â€‹â€‹â€‹å¤§å°çš„å›¾ç‰‡æ—¶ï¼Œå…¶é€Ÿåº¦è¾¾åˆ°äº† 33FPSï¼Œè€Œäº’è”ç½‘ä¸Šå¤šæ•°è§†é¢‘ä¸€èˆ¬æ˜¯ 30FPS çš„ï¼Œè¿™ä¹Ÿå°±æ˜¯å®æ—¶çš„å«ä¹‰äº†ã€‚è¿™æ˜¯å•é˜¶æ®µæ¯”è¾ƒæ—©çš„ä¸€ä»½å·¥ä½œï¼Œè™½ç„¶è¿™ä¸ªé€Ÿåº¦ä¸å¿«ä½†ä¹Ÿè¿˜è¡Œäº†ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-branch">multi-branch</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about YOLACT - Real-time Instance Segmentation" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Q" target="_blank" rel="noopener noreferrer">Qiang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y" target="_blank" rel="noopener noreferrer">Yingming Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+T" target="_blank" rel="noopener noreferrer">Tong Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+X" target="_blank" rel="noopener noreferrer">Xiangyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+J" target="_blank" rel="noopener noreferrer">Jian Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" target="_blank" rel="noopener noreferrer">Jian Sun</a></p><blockquote><p>This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - {\em utilizing only one-level feature for detection}. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5Ã— faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7Ã— less training epochs. With an image size of 608Ã—608, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4. Code is available at <a href="https://github.com/megvii-model/YOLOF" target="_blank" rel="noopener noreferrer">this https URL</a>.</p></blockquote><p>æœ¬æ–‡ç®€ç§°YOLOFã€‚æˆªè‡³åˆ°æœ¬æ–‡å†™ä½œæ—¶ï¼ŒäºŒé˜¶æ®µå’Œå•é˜¶æ®µç›®æ ‡æ£€æµ‹çš„SOTAæ–¹æ³•ä¸­å¹¿æ³›ä½¿ç”¨äº†å¤šå°ºåº¦ç‰¹å¾èåˆçš„æ–¹æ³•ã€‚FPNæ–¹æ³•å‡ ä¹å·²ç»ç§°ä¸ºäº†ç½‘ç»œä¸­ç†æ‰€åº”å½“çš„ä¸€ä¸ªç»„ä»¶ã€‚</p><p>æœ¬æ–‡ä¸­ä½œè€…é‡æ–°å›é¡¾äº†FPNæ¨¡å—ï¼Œå¹¶æŒ‡å‡ºFPNçš„ä¸¤ä¸ªä¼˜åŠ¿åˆ†åˆ«æ˜¯å…¶åˆ†æ²»ï¼ˆdivide-and-conquerï¼‰çš„è§£å†³æ–¹æ¡ˆã€ä»¥åŠå¤šå°ºåº¦ç‰¹å¾èåˆã€‚æœ¬æ–‡åœ¨å•é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨ä¸Šç ”ç©¶äº†FPNçš„è¿™ä¸¤ä¸ªä¼˜åŠ¿ï¼Œå¹¶åœ¨RetinaNetä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†ä¸Šè¿°ä¸¤ä¸ªä¼˜åŠ¿è§£è€¦ï¼Œåˆ†åˆ«ç ”ç©¶å…¶å‘æŒ¥çš„ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºï¼ŒFPNåœ¨å¤šå°ºåº¦ç‰¹å¾èåˆä¸Šå‘æŒ¥çš„ä½œç”¨å¯èƒ½æ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¤§ã€‚</p><p>æœ€åï¼Œä½œè€…æå‡ºYOLOFï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ä½¿ç”¨FPNçš„ç›®æ ‡æ£€æµ‹ç½‘ç»œã€‚å…¶ä¸»è¦åˆ›æ–°æ˜¯ï¼š</p><ol><li>Dilated Encoder</li><li>Uniform Matching</li></ol><p>è¯¥ç½‘ç»œåœ¨è¾¾åˆ°RetinaNetå¯¹ç­‰ç²¾åº¦çš„æƒ…å†µä¸‹é€Ÿåº¦æå‡äº†2.5å€ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fpn">fpn</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about You Only Look One-level Feature" href="/blog/[38]You-Only-Look-One-level-Feature"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai%2C+J" target="_blank" rel="noopener noreferrer">Jifeng Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" target="_blank" rel="noopener noreferrer">Kaiming He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+Y" target="_blank" rel="noopener noreferrer">Yi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren%2C+S" target="_blank" rel="noopener noreferrer">Shaoqing Ren</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" target="_blank" rel="noopener noreferrer">Jian Sun</a></p><blockquote><p>Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.</p></blockquote><p>è¿™ç¯‡å·¥ä½œåˆåInstanceFCNã€‚å®ä¾‹åˆ†å‰²æ–¹é¢ï¼Œç”±äºç½‘ç»œéš¾ä»¥åŒæ—¶è¿›è¡Œåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ï¼Œå› æ­¤é¦–å…ˆæµè¡Œçš„æ˜¯äºŒé˜¶æ®µå®ä¾‹åˆ†å‰²ç½‘ç»œï¼Œé¦–å…ˆå¯¹è¾“å…¥æ‰¾åˆ°å®ä¾‹çš„proposalï¼Œç„¶ååœ¨å…¶ä¸­è¿›è¡Œå¯†é›†é¢„æµ‹ï¼ˆä¹Ÿå°±æ˜¯å…ˆæ¡†æ¡†å†åˆ†å‰²ï¼‰ã€‚æœ¬æ–‡ä»åç§°ä¸Šçœ‹ä¸æ˜¯ä¸€ç¯‡è®²å®ä¾‹åˆ†å‰²çš„æ–‡ç« ï¼Œæ˜¯è®²å¦‚ä½•é€šè¿‡FCNè·å¾—å®ä¾‹çº§åˆ«çš„åˆ†å‰²maskçš„çš„ã€‚</p><p>åœ¨é˜…è¯»ä¹‹å‰æˆ‘æƒ³æé†’ä¸€ä¸‹ï¼Œè¿™ç¯‡å·¥ä½œçš„æ•ˆæœæ˜¯æ¯”è¾ƒå·®çš„ï¼Œæ¯•ç«Ÿæ˜¯æ—©æœŸå·¥ä½œã€‚ä¸è¿‡è¿™ç¯‡å·¥ä½œå…·æœ‰ä¸é”™çš„å¯å‘æ„ä¹‰ï¼Œå€¼å¾—è¯»ä¸€è¯»ã€‚åé¢çš„ä¸€ç¯‡å·¥ä½œFCISï¼ˆFully Convolutional Instance-aware Semantic Segmentationï¼‰ä¸­å°±å€Ÿé‰´äº†æœ¬æ–‡ä¸­æå‡ºçš„instance-sensitive score mapsï¼ˆè¯·ä¸è¦å¼„æ··æœ¬ç¯‡å·¥ä½œå’ŒFCISï¼‰ã€‚æœ¬æ–‡çš„ä¸€å¤§è´¡çŒ®å°±æ˜¯æå‡ºä½¿ç”¨instance-sensitive score mapsåŒºåˆ†ä¸åŒä¸ªä½“ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fcn">fcn</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Instance-sensitive Fully Convolutional Networks" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>