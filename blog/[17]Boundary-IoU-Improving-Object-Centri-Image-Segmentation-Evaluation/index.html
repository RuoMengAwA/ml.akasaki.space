<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation | 工具箱的深度学习记事簿</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="Boundary IoU - Improving Object-Centric Image Segmentation Evaluation | 工具箱的深度学习记事簿"><meta data-react-helmet="true" name="description" content="论文名称：Boundary IoU: Improving Object-Centric Image Segmentation Evaluation"><meta data-react-helmet="true" property="og:description" content="论文名称：Boundary IoU: Improving Object-Centric Image Segmentation Evaluation"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2022-11-05T07:47:19.660Z"><meta data-react-helmet="true" property="article:author" content="https://github.com/AsTheStarsFalll"><meta data-react-helmet="true" property="article:tag" content="loss-function,iou"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a aria-current="page" class="sidebarItemLink_zyXk sidebarItemLinkActive_wcJs" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_d4p0" itemprop="headline">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</h1><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022年11月5日</time> · <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>论文名称：<a href="https://arxiv.org/abs/2103.16562" target="_blank" rel="noopener noreferrer">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></p><p>作者：Bowen Cheng，Ross Girshick，Piotr Dollár，Alexander C. Berg，Alexander Kirillov</p><p>Code：<a href="https://github.com/bowenc0221/boundary-iou-api" target="_blank" rel="noopener noreferrer">https://github.com/bowenc0221/boundary-iou-api</a></p></blockquote><p>写在前面：</p><p>​	<strong>正如它的名字，Boundary IoU就是边界轮廓之间的IoU。</strong></p><p>​	重点为3.4节、5.1节，其他基本都是对比实验。</p><header><h1>摘要</h1></header><ul><li>提出了一种新的基于边界质量的分割评价方法——Boundary IoU；</li><li>Boundary IoU对大对象的边界误差比标准掩码IoU测量明显更敏感，并且不会过分惩罚较小对象的误差；</li><li>比其他方法更适合作为评价分割的指标。</li></ul><header><h1>介绍</h1></header><ul><li><p>对于分割任务，不同的评估指标对不同类型错误的敏感性不同，网络可以轻易解决对应敏感的类型，而其他错误类型的效果则不尽人意；</p></li><li><p>mask的边界质量是图像分割的一个重要指标，各种下游任务直接受益于更精确的目标分割；</p></li><li><p>目前的分割网络的预测不够保真，边缘也很粗糙，<strong>这种情况说明目前的评估指标可能对目标边界的预测误差具有有限的敏感性</strong>；</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210508214210.png" alt="image-20210508214206239"></p></li><li><p>在大量的论文中，AP最高可达到八九十，而很少有论文会提及他们mask的边界质量。</p></li><li><p>对于实例分割，本文提出<strong>Boundary Average Precision</strong> (Boundary AP)，对于全景分割，提出<strong>Boundary Panop-tic Quality</strong> (Boundary PQ)。</p></li></ul><header><h1>相关指标</h1></header><p>各种相关指标如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210508223158.png" alt="image-20210508222750295"></p><p>首先解释几个名词：</p><ol><li><p>对称（Symmetric）：GT（GroundTruth）和Pred（prediction）的交换是否改变测量值</p></li><li><p>倾向（Preference）：衡量方法是否偏向某一类型的预测。</p></li><li><p>不灵敏度（Insensitivity）：测量不太敏感的误差类型。</p></li><li><p>三分图（Trimap）：对给定图像的一种粗略划分将给定图像划分为前景、背景和待求未知区域。</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509151808.png" alt="img"></li><li><p>Mask-based Measure：考虑物体的所有像素</p></li><li><p>Boundary-based Measure：衡量预测边界的分割质量，不同于Mask-based Measure，该方法只评估边界及其邻近的像素。</p></li><li><p>d：边界窄带的像素宽度</p></li></ol><p>通过分析各种相关指标的缺点，我们得出Boundary IoU应该拥有的特性：<strong>同时考虑分类、定位和分割质量。</strong></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="mask-iou和pixel-accuracy">Mask IoU和Pixel Accuracy<a aria-hidden="true" class="hash-link" href="#mask-iou和pixel-accuracy" title="Direct link to heading">​</a></h2><p>所有像素对指标的贡献都是相同的，而物体内部的像素呈二次型增长，其边界仅会线性增长，因此<strong>对较大物体的边界不够敏感</strong>。</p><p>Mask IoU计算方式示意图：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510161356.png" alt="image-20210510161350211"><h2 class="anchor anchorWithStickyNavbar_y2LR" id="trimap-iou">Trimap IoU<a aria-hidden="true" class="hash-link" href="#trimap-iou" title="Direct link to heading">​</a></h2><p>基于边界的分割指标，其计算距离GT和pred边界d像素窄带内的IoU，计算方式示意图如下（方便起见，简化为矩形且只显示边界部分）：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509163907.png" alt="image-20210509163853163"><p><strong>需要注意分母的</strong><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">G_d\cap G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">G</span></span></span></span></span>。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="feature-measure">Feature Measure<a aria-hidden="true" class="hash-link" href="#feature-measure" title="Direct link to heading">​</a></h2><p>F-Measure最初被提出用于边缘检测，但它也被用于评价分割质量。在最初的公式中，使用二分图匹配来进行计算，对于高分辨率的图像来说计算成本很大；因此提出了一种允许重复匹配的近似算法，<strong>precision为pred轮廓中 \ 距离GT轮廓中像素 \ 在d个像素以内的 \ 像素 \ 所占pred的比例</strong>（已断句），recall同理。不是很理解，原文如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510151207.png" alt="image-20210510151147870"></p><p>Precision和Recall计算方式示意图如下（可能）：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510153516.png" alt="image-20210510152547915"><h2 class="anchor anchorWithStickyNavbar_y2LR" id="boundary--iou">Boundary  IoU<a aria-hidden="true" class="hash-link" href="#boundary--iou" title="Direct link to heading">​</a></h2><p>Boundary IoU对大物体边界误差更加敏感，并且不会过分惩罚小物体。</p><p>直观上就是GT和Pred轮廓的交集除以并集，但是<strong>这里的轮廓是在对象内部的</strong><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mtext>、</mtext><msub><mi>P</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">G_d、P_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>，不包括在对象外面的部分，详细请看9.1。</p><p>虽然看起来和Trimap IoU很相似，但个人认为它是Mask IoU的边界升级版本，去除了对象内部巨量像素对整体的影响（见5.1Mask IoU的分析），使其拥有更优秀的性质。 </p><p>完整的论文中给出的示意图如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510154511.png" alt="image-20210510153535293"></p><p>我画的：</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510154514.png" alt="image-20210510154509338"><header><h1>敏感性分析</h1></header><p>为了进行系统的比较，本文对GT进行处理形成伪预测，通过<strong>模拟</strong>不同的误差类型来尽可能的模拟真实误差类型。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="尺度误差">尺度误差<a aria-hidden="true" class="hash-link" href="#尺度误差" title="Direct link to heading">​</a></h2><p>通过对GT进行膨胀和腐蚀操作，误差严重程度由运算核半径控制。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185613.png" alt="image-20210509185608432"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="边界定位误差">边界定位误差<a aria-hidden="true" class="hash-link" href="#边界定位误差" title="Direct link to heading">​</a></h2><p>将随机高斯噪声添加到GT上每一个多边形顶点的<strong>坐标</strong>上，误差严重程度由高斯噪声的标准差确定。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185617.png" alt="image-20210509185545908"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="物体定位误差">物体定位误差<a aria-hidden="true" class="hash-link" href="#物体定位误差" title="Direct link to heading">​</a></h2><p>将GT中的对象随机偏移一些像素，误差严重程度由位移像素长度控制。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185622.png" alt="image-20210509185530435"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="边界近似误差">边界近似误差<a aria-hidden="true" class="hash-link" href="#边界近似误差" title="Direct link to heading">​</a></h2><p>利用Sharply的简化公式来删除多边形顶点，同时保持简化多边形尽可能接近原始图像，误差严重程度由函数的容错参数控制。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185624.png" alt="image-20210509185108649"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="内部掩码错误">内部掩码错误<a aria-hidden="true" class="hash-link" href="#内部掩码错误" title="Direct link to heading">​</a></h2><p>向GT中添加随机性形状的孔，虽然这种误差类型并不常见，但是本文将其包含进来，用以评估内部掩膜误差的影响。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185630.png" alt="image-20210509185508685"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="实现细节">实现细节<a aria-hidden="true" class="hash-link" href="#实现细节" title="Direct link to heading">​</a></h2><p><strong>数据集</strong>：作者从LVIS V0.5验证集中随机抽取实例掩码，因为该数据集拥有高质量的注释。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509190212.png"></p><p><strong>实现过程</strong>：通过改变误差类型和误差的严重程度，记录每种类型的平均值和标准差，此外，还通过划分不同的区域，来比较对不同大小物体的指标评价。</p><p>其中d设置为图像对角线的2%。</p><header><h1>现有方法分析</h1></header><h2 class="anchor anchorWithStickyNavbar_y2LR" id="mask-iou">Mask IoU<a aria-hidden="true" class="hash-link" href="#mask-iou" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="理论分析">理论分析<a aria-hidden="true" class="hash-link" href="#理论分析" title="Direct link to heading">​</a></h3><p><strong>尺度不变性</strong>（自己取的）：即对于一个<strong>固定</strong>的Mask IoU值，分割对象面积越大，则其错误像素越多，二者之间的变化关系成正比，其比例即为Mask IoU的值。</p><p><strong>惩罚差异性</strong>（自己取的）：然而，当缩放一个对象时，内部像素数量呈二次增长，边界像素仅为线性增长，二者不同的增长率导致Mask IoU容忍更大的对象边界上的更多错误分类。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="实证分析">实证分析<a aria-hidden="true" class="hash-link" href="#实证分析" title="Direct link to heading">​</a></h3><p><strong>尺度不变性</strong>基于一个假设，即GT标注中的边界误差也随着对象的大小而增长。</p><p>然而已有研究表明，不论物体大小，被不同标注器标记的同一个对象的两个轮廓之间的像素距离很少超过图像对角线的1%。（就叫它<strong>标注相似性</strong>吧）</p><p>本文通过研究LVIS提供的双标注图像来证实这一点，如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509203230.png" alt="image-20210509201406961"></p><p>其中冰箱的面积是机翼面积的100倍，但在相同分辨率的区域内，注释之间的差异在视觉上十分相似。</p><p>两者的两个轮廓的Mask IoU分别为0.97,0.81，而它们的Boundary IoU则更为接近，分别为0.87，0.81。说明Mask IoU<strong>对小尺寸图片的“惩罚”更大</strong>。</p><p><strong>实验</strong>：通过严重程度相同的膨胀/腐蚀来模拟<strong>尺度误差</strong>，其显著降低了小物体的Mask IoU，而Mask IoU随物体大小的增加而增加，见下图：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510093856.png" alt="image-20210510093853486"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="总结">总结<a aria-hidden="true" class="hash-link" href="#总结" title="Direct link to heading">​</a></h3><ul><li>Mask IoU的主要不足在于对大物体边界的不敏感性。</li><li>相比之下，Boundary IoU更注重物体的边界。</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="trimap-iou-1">Trimap IoU<a aria-hidden="true" class="hash-link" href="#trimap-iou-1" title="Direct link to heading">​</a></h2><p>Trimap IoU是不对称的，交换GT和Pred将会得到不同的值。下图显示了其更倾向于比GT更大的pred：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510095941.png" alt="image-20210510095821668"></p><p>可以看到：</p><ul><li>不论膨胀的严重程度是多少，其值总会大于某个正值，对小物体的“惩罚”依然过大。</li><li>腐蚀则会下降到零。</li></ul><p>简单的证明：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114830.png" alt="image-20210510165235885"></p><p>蓝色部分为pseudo-predictions （伪预测），红色方框为GT轮廓，可以看到，当pseudo-predictions 完全包含了GT时，其值不会再改变</p><p>同理，当伪预测完全被GT所包含，分子为0，最终值为0。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="f-measure">F-measure<a aria-hidden="true" class="hash-link" href="#f-measure" title="Direct link to heading">​</a></h2><p>F-measure完全忽略了小的轮廓误差，但是表现效果很差，会在很短的严重程度中快速下降到0：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114828.png" alt="image-20210510170006064"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="总结-1">总结<a aria-hidden="true" class="hash-link" href="#总结-1" title="Direct link to heading">​</a></h2><p>综上可知，F-measure和Trimap IoU都不能代替Mask IoU，而Mask IoU也有着不能忽视的缺陷，因此，本文提出Boundary IoU。</p><header><h1>Boundary IoU</h1></header><h2 class="anchor anchorWithStickyNavbar_y2LR" id="公式">公式<a aria-hidden="true" class="hash-link" href="#公式" title="Direct link to heading">​</a></h2><p>一个简化的IoU公式</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><msub><mi>P</mi><mi>d</mi></msub></mrow><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∪</mo><msub><mi>P</mi><mi>d</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">IoU = \frac{G_d\cap P_d}{G_d\cup P_d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.1963em;vertical-align:-0.836em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>该公式直接使用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mtext>、</mtext><msub><mi>P</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">G_d、P_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>,丢失了边缘的尖锐部分的信息</p><p>Boundary IoU公式如下：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>B</mi><mi>o</mi><mi>u</mi><mi>d</mi><mi>a</mi><mi>r</mi><mi>y</mi><mo>−</mo><mi>I</mi><mi>o</mi><mi>U</mi><mo stretchy="false">(</mo><mi>G</mi><mo separator="true">,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi><mo stretchy="false">)</mo><mo>∩</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mi>d</mi></msub><mo>∩</mo><mi>P</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi><mo stretchy="false">)</mo><mo>∪</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mi>d</mi></msub><mo>∩</mo><mi>P</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Boudary-IoU(G,P)=\frac{|(G_d\cap G)\cap(P_d\cap P)|}{|(G_d\cap G)\cup(P_d\cap P)|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em">ry</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="mopen">(</span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∣</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">G</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mclose">)</span><span class="mord">∣</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∣</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">G</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mclose">)</span><span class="mord">∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>其中参数d控制了测量的灵敏性，当d足够大时，Boundary IoU就相当于Mask IoU;若使用较小的d，Boundary IoU则会忽略内部像素，使其对边界像素更加敏感。</p><p>此外，对于较小的对象，Boundary IoU十分接近甚至等价于Mask IoU，这主要取决于参数d。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="mask-iou-vs-boundary-iou敏感性分析">Mask IoU vs Boundary IoU：敏感性分析<a aria-hidden="true" class="hash-link" href="#mask-iou-vs-boundary-iou敏感性分析" title="Direct link to heading">​</a></h2><p>本文对比了Mask IoU和Boundary IoU在面积大于<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><msup><mn>6</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">96^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">9</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>的物体的不同误差类型下的表现：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181123.png" alt="image-20210510173824215"></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181124.png" alt="image-20210510173839905"></p><p>对于每种误差类型，Boundary IoU都能更好的利用0-1的范围</p><p>使用的固定的误差严重程度，对大小不同的对象使用伪预测，以<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>6</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">16^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>为增量划分区域，二者表现如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181127.png" alt="image-20210510181102929"></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181129.png" alt="image-20210510181118302"></p><p>可以看到：</p><ul><li>对于较大的对象，Boundary IoU在相同严重程度下保持平缓，而Mask IoU则明显的偏向于大物体；</li><li>对于较小的对象，二者拥有相似的指标，说明他们都没有对其进行过度惩罚。</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="boundary-iou-vs--trimap-iou">Boundary IoU vs  Trimap IoU<a aria-hidden="true" class="hash-link" href="#boundary-iou-vs--trimap-iou" title="Direct link to heading">​</a></h2><p>二者具有一定的相似性，Boundary IoU将Pred和GT边缘上的像素都考虑了进来，这个简单的改进改变了Trimap IoU两点不足，一是不对称，二见4.2。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="boundary-iou-vs-f-measure">Boundary IoU vs F-measure<a aria-hidden="true" class="hash-link" href="#boundary-iou-vs-f-measure" title="Direct link to heading">​</a></h2><p>F-measure对轮廓之间使用了硬预测——如果轮廓之间的像素在距离d内那么Precision和Recall都是完美的，然而当它们都位于d之外，则不会发生任何匹配（见4.3 ，其值会很快的降为0）。</p><p>而Boundary IoU使用一种软分割，变化平缓。</p><p>在附录中将会进行详细分析。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="像素距离参数d">像素距离参数d<a aria-hidden="true" class="hash-link" href="#像素距离参数d" title="Direct link to heading">​</a></h2><p>上文提过，当d足够大时，Boundary IoU等价于Mask IoU，当d过小，Boundary IoU则会出现严重惩罚的情况。</p><p>为了选择合适的参数d，本文在COCO和ASE20K两个数据集（它们拥有相似的分辨率）上进行实验，发现当d为图像<strong>对角线的2%（大约为15个像素）</strong>时，两数据集的Boundary IoU的中位数超过0.9。</p><p>对于Cityscapes中更大分辨率的图像，作者也建议使用相同的像素距离（15个左右），设置d为对角线的0.5%</p><p>对于其他数据集，作者建议考虑两个因素（<strong>没看懂</strong>：</p><ol><li>将注释一致性将下界设为d</li><li>D应根据当前方法的性能选择，并随着性能的提高而降低。</li></ol><h2 class="anchor anchorWithStickyNavbar_y2LR" id="boundary-iou的局限">Boundary IoU的局限<a aria-hidden="true" class="hash-link" href="#boundary-iou的局限" title="Direct link to heading">​</a></h2><p>Boundary IoU不评估距离轮廓超过d的像素，例如一个圆形Mask和一个环形Mask：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114822.png" alt="image-20210510190200706"></p><p>显然，其Boundary Iou值极高为1</p><p>为了惩罚这种情况，作者建议组合Boundary IoU和Mask IoU，并取他们的最小值。</p><p>此外，在实验中还发现，99.9%的情况Boundary IoU都是小于等于Mask IoU的，极少数情况如上图会出现Boundary IoU大于Mask IoU。</p><header><h1>应用</h1></header><p>如上文所说，作者将两种IoU组合，取其最小。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="boundary-ap-for-instance-segmentation">Boundary AP for instance segmentation<a aria-hidden="true" class="hash-link" href="#boundary-ap-for-instance-segmentation" title="Direct link to heading">​</a></h2><p>实例分割任务的目标是用像素级掩码描绘每个对象，其评估指标是同时评估多个方面，如分类、定位和分割质量。</p><p>本文通过（Synthetic predictions，Synthetic，综合的；合成的，人造的，结合上下文个人感觉应该取“人造”之意） 合成预测与真实模型来进行实验。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="合成预测">合成预测<a aria-hidden="true" class="hash-link" href="#合成预测" title="Direct link to heading">​</a></h3><blockquote><p>综合预测允许我们单独的评估分割质量。</p></blockquote><ul><li><p><strong>具体方法</strong>：</p><p>使用COCO数据集，将GT缩小为28X28的连续值掩码，使用双线性插值upscale it back，最后将其二值化。如下图所示</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510230303.png" alt="image-20210510230301360"></p><p>这种合成Mask十分接近GT，但这种差异随着物体大小的增大而增大，因此越大的物体经过处理后的IoU值应该越低。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122838.png" alt="image-20210510223226154"></p><p>下标表示物体的大小，可以看到，对于越大的物体，Boundary IoU的值越低，而Mask IoU的值则维持在高水平，<strong>这进一步显示了Boundary IoU对于大物体边界的敏感性</strong>。</p></li><li><p>实验结果：在Mask RCNN、PointRend、以及BMask RCNN模型上进行实验，结果如下：</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122836.png" alt="image-20210510224102719"></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122835.png" alt="image-20210510224120918"></p><p>众所周知，Mask RCNN对大物体的分割表现不尽人意（我不知道），从上表可以看出Boundary Ap的优越性</p><p>此外，上表还证明了相较于BMask RCNN，PointRend对较大对象的表现更好。</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122833.png" alt="image-20210510224713604"></p><p>上表显示了更深的主干网络并不能带来分割质量的显著提升。</p></li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="真实预测">真实预测<a aria-hidden="true" class="hash-link" href="#真实预测" title="Direct link to heading">​</a></h3><blockquote><p>利用现有的分割模型得到的真实预测进一步实验，可以进一步了解Boundary IoU在实例分割任务各个方面的表现。</p></blockquote><ul><li><p><strong>具体方法</strong>：</p><p>为了将分割质量与分类和定位错误分离开，作者为这些方法提供了Ground Truth Box，并为其分配随机置信度。</p></li><li><p><strong>实验结果</strong>：</p><p>模型在COCO数据集上训练，在LVIS v0.5上验证</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115505.png" alt="image-20210511115502161"></p><p>模型在Cityscapes上训练和验证</p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115657.png" alt="image-20210511115655795"></p></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="boundary--pq">Boundary  PQ<a aria-hidden="true" class="hash-link" href="#boundary--pq" title="Direct link to heading">​</a></h2><p>下图为标准PQ的公式</p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115040.png" alt="image-20210511115032369"><p>将其中的Mask IoU替换为Mask IoU与Boundary IoU的组合，取其最小值。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="合成预测-1">合成预测<a aria-hidden="true" class="hash-link" href="#合成预测-1" title="Direct link to heading">​</a></h3><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122826.png" alt="image-20210511120047274"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="真实预测-1">真实预测<a aria-hidden="true" class="hash-link" href="#真实预测-1" title="Direct link to heading">​</a></h3><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122824.png" alt="image-20210511120131765"></p><header><h1>总结</h1></header><p>​		不同于Mask IoU，Boundary IoU提供了一个明确的，定量的梯度，奖励改善边界分割质量。作者希望Boundary IoU可以鼓励更多人开发高保真Mask预测新方法。此外，Boundary  IoU允许对复杂的任务(如实例和全景分割)的分割相关错误进行更细粒度的分析。在性能分析工具(如TIDE<!-- -->[2]<!-- -->)中结合度量可以更好地洞察实例分段模型的特定错误类型。（<strong>直接翻译的</strong>）</p><header><h1>补充</h1></header><h2 class="anchor anchorWithStickyNavbar_y2LR" id="g_d和g_dcap-g"><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">G_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>和<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">G_d\cap G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">G</span></span></span></span></span><a aria-hidden="true" class="hash-link" href="#g_d和g_dcap-g" title="Direct link to heading">​</a></h2><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511121300.png" alt="image-20210511121230297"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="代码复现">代码复现<a aria-hidden="true" class="hash-link" href="#代码复现" title="Direct link to heading">​</a></h2><p>对于二分类图像的Boundary Iou</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 将二值Mask转化为Boundary mask</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">mask_to_boundary</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mask</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dilation_ratio</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.01</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    h</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> mask</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    img_diag </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">sqrt</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">h </span><span class="token operator" style="color:#393A34">**</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> w </span><span class="token operator" style="color:#393A34">**</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dilation </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">int</span><span class="token punctuation" style="color:#393A34">(</span><span class="token builtin">round</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dilation_ratio </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> img_diag</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> dilation </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dilation </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Pad image so mask truncated by the image border is also considered as boundary.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 将mask使用0填充一圈，防止dilation为1时</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    new_mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">copyMakeBorder</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        mask</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">BORDER_CONSTANT</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> value</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    kernel </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ones</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">uint8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 对mask进行腐蚀操作</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    new_mask_erode </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">erode</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">new_mask</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> iterations</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">dilation</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mask_erode </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> new_mask_erode</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> h </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> w </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># G_d intersects G</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> mask </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> mask_erode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">boundary_iou</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mask</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pred</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    intersect </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> mask</span><span class="token operator" style="color:#393A34">*</span><span class="token plain">pred</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ite </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">sum</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">intersect </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    un </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> mask</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">pred</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    union </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">sum</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">un </span><span class="token operator" style="color:#393A34">&gt;=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> ite</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">union</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091830.png" alt="image-20210519091807762"></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091840.png" alt="image-20210519091815466"></p><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091833.png" alt="image-20210519091826066"></p></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_xD8n"><div class="col"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/loss-function">loss-function</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/iou">iou</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/[16]Convolutional-Block-Attention-Module"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« <!-- -->Convolutional Block Attention Module</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Involution - Inverting the Inherence of Convolution for Visual Recognition<!-- --> »</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#mask-iou和pixel-accuracy" class="table-of-contents__link toc-highlight">Mask IoU和Pixel Accuracy</a></li><li><a href="#trimap-iou" class="table-of-contents__link toc-highlight">Trimap IoU</a></li><li><a href="#feature-measure" class="table-of-contents__link toc-highlight">Feature Measure</a></li><li><a href="#boundary--iou" class="table-of-contents__link toc-highlight">Boundary  IoU</a></li><li><a href="#尺度误差" class="table-of-contents__link toc-highlight">尺度误差</a></li><li><a href="#边界定位误差" class="table-of-contents__link toc-highlight">边界定位误差</a></li><li><a href="#物体定位误差" class="table-of-contents__link toc-highlight">物体定位误差</a></li><li><a href="#边界近似误差" class="table-of-contents__link toc-highlight">边界近似误差</a></li><li><a href="#内部掩码错误" class="table-of-contents__link toc-highlight">内部掩码错误</a></li><li><a href="#实现细节" class="table-of-contents__link toc-highlight">实现细节</a></li><li><a href="#mask-iou" class="table-of-contents__link toc-highlight">Mask IoU</a><ul><li><a href="#理论分析" class="table-of-contents__link toc-highlight">理论分析</a></li><li><a href="#实证分析" class="table-of-contents__link toc-highlight">实证分析</a></li><li><a href="#总结" class="table-of-contents__link toc-highlight">总结</a></li></ul></li><li><a href="#trimap-iou-1" class="table-of-contents__link toc-highlight">Trimap IoU</a></li><li><a href="#f-measure" class="table-of-contents__link toc-highlight">F-measure</a></li><li><a href="#总结-1" class="table-of-contents__link toc-highlight">总结</a></li><li><a href="#公式" class="table-of-contents__link toc-highlight">公式</a></li><li><a href="#mask-iou-vs-boundary-iou敏感性分析" class="table-of-contents__link toc-highlight">Mask IoU vs Boundary IoU：敏感性分析</a></li><li><a href="#boundary-iou-vs--trimap-iou" class="table-of-contents__link toc-highlight">Boundary IoU vs  Trimap IoU</a></li><li><a href="#boundary-iou-vs-f-measure" class="table-of-contents__link toc-highlight">Boundary IoU vs F-measure</a></li><li><a href="#像素距离参数d" class="table-of-contents__link toc-highlight">像素距离参数d</a></li><li><a href="#boundary-iou的局限" class="table-of-contents__link toc-highlight">Boundary IoU的局限</a></li><li><a href="#boundary-ap-for-instance-segmentation" class="table-of-contents__link toc-highlight">Boundary AP for instance segmentation</a><ul><li><a href="#合成预测" class="table-of-contents__link toc-highlight">合成预测</a></li><li><a href="#真实预测" class="table-of-contents__link toc-highlight">真实预测</a></li></ul></li><li><a href="#boundary--pq" class="table-of-contents__link toc-highlight">Boundary  PQ</a><ul><li><a href="#合成预测-1" class="table-of-contents__link toc-highlight">合成预测</a></li><li><a href="#真实预测-1" class="table-of-contents__link toc-highlight">真实预测</a></li></ul></li><li><a href="#g_d和g_dcap-g" class="table-of-contents__link toc-highlight">G_d和G_dcap G</a></li><li><a href="#代码复现" class="table-of-contents__link toc-highlight">代码复现</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>