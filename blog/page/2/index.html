<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">Blog | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" property="og:title" content="Blog | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="description" content="Blog"><meta data-react-helmet="true" property="og:description" content="Blog"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/page/2"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_posts_list"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/page/2"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/page/2" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/page/2" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">é­”æ³•éƒ¨æ—¥å¿—</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.664Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AndPuQing" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/AndPuQing.png" alt="PuQing"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AndPuQing" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PuQing</span></a></div><small class="avatar__subtitle" itemprop="description">intro * new</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sundararajan%2C+M" target="_blank" rel="noopener noreferrer">Mukund Sundararajan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Taly%2C+A" target="_blank" rel="noopener noreferrer">Ankur Taly</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan%2C+Q" target="_blank" rel="noopener noreferrer">Qiqi Yan</a></p><blockquote><p>We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ç®€ä»‹">ç®€ä»‹<a aria-hidden="true" class="hash-link" href="#ç®€ä»‹" title="Direct link to heading">â€‹</a></h2><p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç¥ç»ç½‘ç»œçš„å¯è§†åŒ–æ–¹æ³•ï¼š<code>ç§¯åˆ†æ¢¯åº¦ï¼ˆIntegrated Gradientsï¼‰</code>ï¼Œæ˜¯ä¸€ç¯‡ 2016-2017 å¹´é—´çš„å·¥ä½œã€‚</p><p>æ‰€è°“å¯è§†åŒ–ï¼Œç®€å•æ¥è¯´å°±æ˜¯å¯¹äºç»™å®šçš„è¾“å…¥ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> ä»¥åŠæ¨¡å‹ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>ï¼Œæƒ³åŠæ³•æŒ‡å‡º <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> çš„å“ªäº›åˆ†é‡å¯¹æ¨¡å‹çš„é¢„æµ‹æœ‰è¾ƒå¤§çš„å½±å“ï¼Œæˆ–è€…è¯´ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> å„ä¸ªåˆ†é‡çš„é‡è¦æ€§åšä¸ªæ’åºï¼Œè€Œä¸“ä¸šçš„è¯æœ¯å°±æ˜¯<code>å½’å› </code>(Attribution)ã€‚ä¸€ä¸ªæœ´ç´ çš„æ€è·¯æ˜¯ç›´æ¥ä½¿ç”¨æ¢¯åº¦ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">âˆ‡</mi><mi>x</mi></msub><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla _{x}F(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord">âˆ‡</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> æ¥ä½œä¸º <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>å„ä¸ªåˆ†é‡çš„é‡è¦æ€§æŒ‡æ ‡ï¼Œè€Œç§¯åˆ†æ¢¯åº¦æ˜¯å¯¹å®ƒçš„æ”¹è¿›ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attribution">attribution</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/visualization">visualization</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Axiomatic Attribution for Deep Networks" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>å¦‚æœä½ çœ‹åˆ°äº†è¿™é‡Œï¼Œè¯´æ˜ä½ å·²ç»å‡†å¤‡å¥½å¼€å§‹æ¢æ±‚è¿™ä¸ªé¢†åŸŸäº‹ç‰©çš„è§„å¾‹ä»¥åŠè¿™äº›è§„å¾‹çš„æœ¬æºäº†ã€‚ä½œä¸ºä¸€ä¸ªæœ¬ç§‘ç”Ÿï¼Œæœ€é€‚åˆä½ å…¥å‘çš„å°±æ˜¯å¼€å§‹ä¹ æƒ¯æ€§é˜…è¯»é¢†åŸŸå†…è®ºæ–‡ã€‚å¤§çº¦åœ¨æˆ‘çš„å¤§äºŒçš„ä¸‹åŠå­¦æœŸï¼Œæˆ‘å’Œæˆ‘çš„æœ‹å‹ä»¬å¼€å§‹å…±åŒé˜…è¯»è®ºæ–‡å¹¶å†™ä¸‹ç¬”è®°ã€‚è¿™äº›ç¬”è®°ç²—æµ…ã€å¹¼ç¨šï¼Œç”šè‡³ä¼šå‡ºç°ä¸€äº›ç†è§£ä¸Šçš„é”™è¯¯â€”â€”ä¸‡äº‹å¼€å¤´éš¾ã€‚ä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯æƒ³æŠŠè¿™äº›ç¬”è®°æ•´ç†èµ·æ¥â€”â€”è¿™ä¾¿æ˜¯é­”æ³•éƒ¨æ—¥å¿—çš„å¼€å§‹ã€‚åœ¨æˆ‘æ–°å»ºæ–‡ä»¶å¤¹çš„æ—¶å€™ï¼Œé­”æ³•éƒ¨æ—¥å¿—çš„æ–‡ä»¶å¤¹åç§°æ˜¯â€œunlimited paper worksâ€ï¼Œåœ¨æˆä¸ºç†æ€§çš„æ€€ç–‘è€…ä¹‹å‰ï¼Œåº”è¯¥å…ˆæŒæ¡è¿™ä¸ªç§‘ç ”é¢†åŸŸã€‚æˆ‘ä»¬åšå¥½äº†é•¿æœŸæŠ•å…¥çš„å‡†å¤‡ï¼Œå¹¶å¸Œæœ›æŠŠç®€å•çš„äº‹æƒ…åšåˆ°å‡ºäººæ„æ–™å¾—ç²¾å½©ã€‚</p><p>åŠ å…¥é­”æ³•éƒ¨æ—¥å¿—ä¹Ÿä¸æ˜¯ä»€ä¹ˆéš¾äº‹ï¼Œä½ åªéœ€è¦çƒ­èº«ä¸€ä¸‹ï¼Œè¯»å®Œä¸‹é¢çš„ä¸€ç¯‡å¼•å¯¼ï¼Œå°±å¯ä»¥å¼€å§‹äº†(ä»¥ä¸‹å†…å®¹å·²é€šè¿‡è¯­æ³•æ£€æŸ¥å·¥å…·<a href="https://github.com/PaperCube" target="_blank" rel="noopener noreferrer">PaperCube</a>çš„æ£€æŸ¥)ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="how-to-read-and-comprehend-scientific-research-articles">How to Read and Comprehend Scientific Research Articles<a aria-hidden="true" class="hash-link" href="#how-to-read-and-comprehend-scientific-research-articles" title="Direct link to heading">â€‹</a></h2><p>Scientific articles are how scholars and researchers communicate with each other. Reading scientific articles helps you to participate in your comprehension by wondering how the researchers explain their ideas. Books, websites, papers, scientific magazines are general places to start with.</p><p>This tutorial will discuss:</p><ul><li>How to read a scientific article</li><li>How to find the main points of an article</li><li>How to take effective notes</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col text--right"><a aria-label="Read more about æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—" href="/blog/[00]unlimited-paper-works"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²å„ç§å„æ ·è§£ç å™¨çš„è®ºæ–‡ã€‚<a href="https://arxiv.org/pdf/1707.05847.pdf" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆThe Devil is in the Decoder: Classification, Regression and GANsï¼‰</a>ã€‚</p><p>ç”±äºâ€œè§£ç å™¨ï¼ˆdecoderï¼Œæœ‰äº›æ—¶å€™ä¹Ÿè¢«ç§°ä¸ºfeature extractorï¼‰â€çš„æ¦‚å¿µä¸åƒç´ çº§çš„åˆ†ç±»ã€å›å½’ç­‰é—®é¢˜å¤šå¤šå°‘å°‘éƒ½æœ‰ç“œè‘›ã€‚ä»¥ä¸‹æ˜¯decoderè¢«åº”ç”¨äºåƒç´ çº§çš„ä»»åŠ¡ï¼š</p><ul><li>åˆ†ç±»ï¼šè¯­ä¹‰åˆ†å‰²ã€è¾¹ç¼˜æ£€æµ‹ã€‚</li><li>å›å½’ï¼šäººä½“å…³é”®ç‚¹æ£€æµ‹ã€æ·±åº¦é¢„æµ‹ã€ç€è‰²ã€è¶…åˆ†è¾¨ã€‚</li><li>åˆæˆï¼šåˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”Ÿæˆå›¾åƒç­‰ã€‚</li></ul><p>æ‰€ä»¥decoderæ˜¯ç¨ å¯†é¢„æµ‹ï¼ˆDence predictionï¼Œåƒç´ çº§åˆ«çš„å¾ˆå¤šé—®é¢˜éƒ½å¯ä»¥å«åšç¨ å¯†çš„ï¼‰é—®é¢˜çš„å…³é”®ã€‚</p><header><h1>Abstractï¼ˆæ‘˜è¦ï¼‰</h1></header><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>æˆ‘çœ‹äº†è¿™ç¯‡ç»¼è¿°å—ç›ŠåŒªæµ…ï¼Œå¦‚æœæœ‰æ—¶é—´çš„è¯è¯·é˜…è¯»<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">åŸä½œ</a>ã€‚æœ¬æ–‡åªæ˜¯å¯¹åŸä½œé˜…è¯»çš„ç²—æµ…ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/survey">survey</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/decoder">decoder</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about The Devil is in the Decoder - Classification, Regression and GANs" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AndSonder" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/AndSonder.png" alt="Sonder"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AndSonder" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sonder</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡ç¥ç»å¯¹æŠ—çš„ç»¼è¿°æ–‡ç« ï¼Œéå¸¸éå¸¸éå¸¸è¯¦ç»†çš„ä»‹ç»äº†å½“å‰ç¥ç»å¯¹æŠ—æ”»å‡»çš„å‘å±•æƒ…å†µå’Œå·²æœ‰çš„æ”»å‡»å’Œé˜²å¾¡ç®—æ³•ã€‚åŸè®ºæ–‡ï¼š<a href="https://arxiv.org/pdf/1801.00553.pdf" target="_blank" rel="noopener noreferrer">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></p><blockquote><p>Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.</p></blockquote><p>æœ¬æ–‡ä¸»è¦å¯¹æ–‡ç« è¿›è¡Œç¿»è¯‘ï¼Œè¿˜åŠ å…¥äº†ä¸ªäººå¯¹ä¸€äº›ç®—æ³•çš„ç†è§£ä¸è§£é‡Šã€‚è¿™ç¯‡æ–‡ç« æˆ‘å¤§æ¦‚çœ‹äº†ä¸€ä¸ªæ˜ŸæœŸã€‚çœŸçš„æ˜¯ä¸€ç¯‡éå¸¸ä¸é”™çš„ç»¼è¿°è®ºæ–‡ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/adversarial-attacks">adversarial attacks</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/survey">survey</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/robustness">robustness</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>åŸè®ºæ–‡ï¼š<a href="https://arxiv.org/pdf/2104.03778.pdf" target="_blank" rel="noopener noreferrer">Progressive Semantic Segmentation</a></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="é—®é¢˜æè¿°">é—®é¢˜æè¿°<a aria-hidden="true" class="hash-link" href="#é—®é¢˜æè¿°" title="Direct link to heading">â€‹</a></h2><p>å½“å¯¹å¤§å‹å›¾ç‰‡è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ˜¾å­˜ç‚¸æ‰ã€‚æ”¶åˆ°å†…å­˜é™åˆ¶ï¼Œå¯ä»¥é€‰æ‹©ä¸‹é‡‡æ ·ï¼Œæˆ–å°†å›¾åƒåˆ’åˆ†ä¸ºå±€éƒ¨å—ã€‚ä½†å‰è€…ä¼šä¸¢å¤±ç»†èŠ‚ï¼Œåè€…ä¼šå´åå…¨å±€è§†å›¾ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="åå¤„ç†æ”¹å–„åˆ†å‰²ç»†èŠ‚">åå¤„ç†æ”¹å–„åˆ†å‰²ç»†èŠ‚<a aria-hidden="true" class="hash-link" href="#åå¤„ç†æ”¹å–„åˆ†å‰²ç»†èŠ‚" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ç»å…¸æ–¹æ³•">ç»å…¸æ–¹æ³•<a aria-hidden="true" class="hash-link" href="#ç»å…¸æ–¹æ³•" title="Direct link to heading">â€‹</a></h3><p>æ¡ä»¶éšæœºåœº(CRF),å¼•å¯¼æ»¤æ³¢å™¨ï¼ˆGFï¼‰ï¼Œä¸¤ä¸ªé€Ÿåº¦æ…¢ï¼Œæ”¹è¿›æ˜¯æ¸è¿›çš„ã€‚</p><p>æ·±åº¦å­¦ä¹ çš„å¼•å¯¼è¿‡æ»¤å™¨(DGF)å¯ä»¥æé«˜æ¨ç†é€Ÿåº¦</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Progressive Semantic Segmentation" href="/blog/[03]Progressive-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡å…³äºæ•°æ®ä¾èµ–å‹è§£ç å™¨çš„ç†è®ºå’Œæµ‹è¯•å·¥ä½œçš„è®ºæ–‡ã€‚åŸè®ºæ–‡æ˜¯<a href="https://arxiv.org/pdf/1903.02120.pdf" target="_blank" rel="noopener noreferrer">Decoders Matter for Semantic Segmentation : Data-Dependent Decoding Enables Flexible Feature Aggregation</a>ã€‚</p><p>è¿‘å¹´æ¥ï¼Œå¸¸è§çš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•åˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„è¿›è¡Œé€åƒç´ çš„é¢„æµ‹ä»»åŠ¡ã€‚åœ¨è¿™äº›è§£ç å™¨æ¯ä¸€å±‚çš„æœ€åé€šå¸¸æ˜¯ä¸€å±‚åŒçº¿æ€§ä¸Šé‡‡æ ·çš„è¿‡ç¨‹ï¼Œç”¨äºå°†åƒç´ æ¢å¤è‡³åŸæœ‰åƒç´ å¤§å°ã€‚æœ¬è®ºæ–‡çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§ä¸æ•°æ®æ— å…³çš„åŒçº¿æ€§ä¸Šé‡‡æ ·æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´ç»“æœå¹¶ä¸å®Œç¾ã€‚</p><p>æ‰€ä»¥ï¼Œæœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä¾èµ–äºè¾“å…¥æ•°æ®çš„ä¸Šé‡‡æ ·å–ä»£åŒçº¿æ€§ä¸Šé‡‡æ ·ï¼Œç§°ä¸ºâ€œDUpsamplingâ€ã€‚è¿™ä¸ªæ–°çš„æ–¹æ³•åˆ©ç”¨åœ¨è¯­ä¹‰åˆ†æ®µæ ‡ç­¾ä¸­çš„ç©ºé—´å†—ä½™ï¼Œèƒ½å¤Ÿä»ä½åˆ†è¾¨ç‡çš„CNNè¾“å‡ºä¸­æ¢å¤åˆ†è¾¨ç‡å¹¶å®ç°é€åƒç´ é¢„æµ‹ã€‚è¯¥æ–¹æ³•åœ¨åˆ†è¾¨ç‡ç›¸å¯¹è¾ƒä½çš„è¾“å…¥ä¸Šèƒ½è·å¾—æ›´åŠ ç²¾ç¡®çš„åˆ†å‰²æ•ˆæœï¼Œå¹¶ä¸”æ˜¾è‘—é™ä½äº†è®¡ç®—çš„å¤æ‚åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š</p><ul><li>è¿™ç§æ–°çš„ä¸Šé‡‡æ ·å±‚é‡å»ºèƒ½åŠ›éå¸¸å¼º</li><li>è¿™ç§æ–¹æ³•å¯¹ä»»ä½•CNNç¼–ç å™¨çš„ç»„åˆå’Œä½¿ç”¨è¡¨ç°å‡ºå¾ˆå¥½çš„å…¼å®¹æ€§</li></ul><p>æœ¬è®ºæ–‡è¿˜é€šè¿‡å®éªŒæ ‡æ˜äº†ï¼ŒDUpsamplingæ€§èƒ½ä¼˜è¶Šï¼Œå¹¶ä¸”æ— éœ€ä»»ä½•åå¤„ç†ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstractæ‘˜è¦">Abstractï¼ˆæ‘˜è¦ï¼‰<a aria-hidden="true" class="hash-link" href="#abstractæ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Recent semantic segmentation methods exploit encoder-decoder architectures to produce the desired pixel-wise segmentation prediction. The last layer of the decoders is typically a bilinear upsampling procedure to recover the final pixel-wise prediction. We empirically show that this oversimple and data-independent bilinear upsampling may lead to sub-optimal results.
In this work, we propose a data-dependent upsampling (DUpsampling) to replace bilinear, which takes advantages of the redundancy in the label space of semantic segmentation and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. The main advantage of the new upsampling layer lies in that with a relatively lower-resolution feature map such as 1/16 or 1/32 of the input size, we can achieve even better segmentation accuracy, significantly reducing computation complexity. This is made possible by 1) the new upsampling layer&#x27;s much improved reconstruction capability; and more importantly 2) the DUpsampling based decoder&#x27;s flexibility in leveraging almost arbitrary combinations of the CNN encoders&#x27; features. Experiments demonstrate that our proposed decoder outperforms the state-of-the-art decoder, with only 20% of computation. Finally, without any post-processing, the framework equipped with our proposed decoder achieves new state-of-the-art performance on two datasets: 88.1% mIOU on PASCAL VOC with 30% computation of the previously best model; and 52.5% mIOU on PASCAL Context.     </p></blockquote><p>å¦‚æœæœ‰æ—¶é—´çš„è¯è¯·é˜…è¯»<a href="https://arxiv.org/pdf/1903.02120.pdf" target="_blank" rel="noopener noreferrer">åŸä½œ</a>ã€‚æœ¬æ–‡åªæ˜¯å¯¹åŸä½œé˜…è¯»çš„ç²—æµ…ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/survey">survey</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/decoder">decoder</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²ä½å…‰ç…§äººè„¸æ£€æµ‹çš„è®ºæ–‡ã€‚<a href="https://arxiv.org/pdf/2104.01984.pdf" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆHLA-Face Joint High-Low Adaptation for Low Light Face Detectionï¼‰</a>ã€‚</p><ul><li>å……åˆ†åˆ©ç”¨ç°æœ‰çš„æ­£å¸¸å…‰æ•°æ®ï¼Œå¹¶æ¢ç´¢å¦‚ä½•å°†é¢éƒ¨æ¢æµ‹å™¨ä»æ­£å¸¸å…‰çº¿è°ƒæ•´åˆ°ä½å…‰ã€‚è¿™é¡¹ä»»åŠ¡çš„æŒ‘æˆ˜æ˜¯ï¼Œæ­£å¸¸å’Œä½å…‰ä¹‹é—´çš„å·®è·å¯¹äºåƒç´ çº§å’Œç‰©ä½“çº§åˆ«æ¥è¯´å¤ªå¤§è€Œå¤æ‚ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°ç°æœ‰çš„lowlighenhanceå’Œé€‚åº”æ–¹æ³•ä¸è¾¾åˆ°æ‰€éœ€çš„performanceã€‚</li><li>æœ¬æ–‡æ˜¯DARK FACEä¸ºåŸºå‡†ï¼Œé’ˆå¯¹ç°æœ‰çš„æ­£å¸¸ç…§åº¦å›¾åƒï¼Œå°†å›¾åƒè°ƒæ•´æˆä½ç…§åº¦å›¾åƒï¼Œä¸éœ€è¦æ ‡ç­¾</li><li>ä¸€ä¸ªæ˜¯åƒç´ çº§å¤–è§‚çš„å·®è·ï¼Œä¾‹å¦‚ä¸è¶³ï¼Œç…§æ˜ï¼Œç›¸æœºå™ªå£°å’Œé¢œè‰²åç½®ã€‚å¦ä¸€ä¸ªæ˜¯æ­£å¸¸å’Œä½å…‰åœºæ™¯ä¹‹é—´çš„ç‰©ä½“çº§è¯­ä¹‰å·®å¼‚ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè·¯ç¯çš„å­˜åœ¨ï¼Œè½¦è¾†å‰ç¯å’Œå¹¿å‘Šæ¿ã€‚ä¼ ç»Ÿçš„ä½å…‰å¢å¼ºæ–¹æ³•<!-- -->[5,6]<!-- -->è®¾è®¡ç”¨äºæé«˜è§†è§‰è´¨é‡ï¼Œå› æ­¤ä¸èƒ½å¡«å……è¯­ä¹‰å·®è·ï¼Œ</li><li>é€šè¿‡ä½¿ä½å…‰å›¾åƒäº®èµ·å¹¶æ‰­æ›²æ­£å¸¸å…‰å›¾åƒï¼Œæˆ‘ä»¬æ„å»ºä½äºæ­£å¸¸å’Œä½å…‰ä¹‹é—´çš„ä¸­é—´çŠ¶æ€ã€‚</li></ul><p>æ‘˜è¦:</p><blockquote><p>Face detection in low light scenarios is challenging but vital to many practical applications, e.g., surveillance video, autonomous driving at night. Most existing face detectors heavily rely on extensive annotations, while col- lecting data is time-consuming and laborious. To reduce the burden of building new datasets for low light condi- tions, we make full use of existing normal light data and explore how to adapt face detectors from normal light to low light. The challenge of this task is that the gap between normal and low light is too huge and complex for both pixel-level and object-level. Therefore, most existing low- light enhancement and adaptation methods do not achieve desirable performance. To address the issue, we propose a joint High-Low Adaptation (HLA) framework. Through a bidirectional low-level adaptation and multi-task high- level adaptation scheme, our HLA-Face outperforms state- of-the-art methods even without using dark face labels for training. Our project is publicly available at: <!-- -->[https: //daooshee.github.io/HLA-Face-Website/]<!-- -->(https: //daooshee.github.io/HLA-Face-Website/)</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about HLA-Face Joint High-Low Adaptation for Low Light Face Detection" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[06]DeepLab-Series">DeepLab Series</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>DeepLabç³»åˆ—ä¸­åŒ…å«äº†ä¸‰ç¯‡è®ºæ–‡ï¼šDeepLab-v1ã€DeepLab-v2ã€DeepLab-v3ã€‚</p><p>DeepLab-v1ï¼š<a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener noreferrer">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>DeepLab-v2ï¼š<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener noreferrer">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>DeepLab-v3ï¼š<a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener noreferrer">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>åœ¨è¿™é‡Œæˆ‘ä»¬å°†è¿™ä¸‰ç¯‡æ”¾åœ¨ä¸€èµ·é˜…è¯»ã€‚</p><p>åæ¥ç”šè‡³è¿˜å‡ºç°äº†åç»­ï¼š</p><p>DeepLab-v3+ï¼š<a href="https://arxiv.org/abs/1802.02611" target="_blank" rel="noopener noreferrer">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>ä¸è¿‡æš‚æ—¶æ²¡æœ‰å†™è¿›æ¥çš„æ‰“ç®—ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/decoder">decoder</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/atrous-convolution">atrous-convolution</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about DeepLab Series" href="/blog/[06]DeepLab-Series"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/2103.11351" target="_blank" rel="noopener noreferrer">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></p><p>ä½œè€…ï¼š<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+L" target="_blank" rel="noopener noreferrer">Li Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+D" target="_blank" rel="noopener noreferrer">Dong Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu%2C+Y" target="_blank" rel="noopener noreferrer">Yousong Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian%2C+L" target="_blank" rel="noopener noreferrer">Lu Tian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shan%2C+Y" target="_blank" rel="noopener noreferrer">Yi Shan</a></p><p>æœŸåˆŠï¼šCVPR2021</p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä¸»è¦ç»“æ„">ä¸»è¦ç»“æ„<a aria-hidden="true" class="hash-link" href="#ä¸»è¦ç»“æ„" title="Direct link to heading">â€‹</a></h2><p>DABï¼šDataset-Aware Block(æ•°æ®é›†æ„ŸçŸ¥å—)</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#393A34"><span class="token plain">    ä½œä¸ºç½‘ç»œçš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œæœ‰åŠ©äºæ•è·oä¸åŒåŠŸèƒ½æ•°æ®é›†ä¹‹é—´çš„åŒè´¨è¡¨ç¤ºå’Œå¼‚æ„ç»Ÿè®¡ã€‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ä¸»è¦ç”±ï¼Œä¸€ä¸ªæ•°æ®é›†ä¸å˜çš„å·ç§¯å±‚ï¼Œå¤šä¸ªæ•°æ®é›†ç‰¹å®šçš„BatchNormalå’Œä¸€ä¸ªæ¿€æ´»å±‚æ„æˆã€‚</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>DATï¼šDataset Alternation Training(æ•°æ®é›†äº¤æ›¿è®­ç»ƒæœºåˆ¶)</p><p>åˆ†å‰²ç»“æœï¼š</p><p><img alt="image-20210505160138997" src="/assets/images/20210505160141image-20210505160138997-0005e31a81f1a3491314c4432158798b.png"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/cross-dataset-learning">cross-dataset-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Cross-Dataset Collaborative Learning for Semantic Segmentation" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™ç¯‡è®ºæ–‡æ˜¯ä¸€ç¯‡å¯¹åŠ¨æ€ç¥ç»ç½‘ç»œçš„ç»¼è¿°ï¼ŒåŸè®ºæ–‡<a href="http://arxiv.org/abs/2102.04906" target="_blank" rel="noopener noreferrer">&quot;Dynamic Neural Networks: A Survey&quot;</a>ä¸»è¦è®²äº†ï¼š</p><ul><li>æ¦‚å¿µï¼ˆIntroductionï¼‰</li><li>å¸¸è§çš„åŠ¨æ€ç¥ç»ç½‘ç»œ<ul><li>Instance-wise Dynamic Networks</li><li>Spatial-wise Dynamic Networks</li><li>Temporal-wise Dynamic Network</li></ul></li><li>æ¨ç†å’Œè®­ç»ƒï¼ˆInference and Trainingï¼‰</li><li>å¸¸è§åº”ç”¨å’Œä»£è¡¨æ€§å·¥ä½œï¼ˆApplicationsï¼‰</li></ul><p>è¿™ç¯‡è®ºæ–‡å¯¹è¿‘äº›å¹´å¸å¼•äº†å¾ˆå¤šç ”ç©¶è€…çš„åŠ¨æ€ç¥ç»ç½‘ç»œè¿›è¡Œäº†è¾ƒä¸ºç³»ç»Ÿçš„æ€»ç»“æ¦‚æ‹¬ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstractæ‘˜è¦">Abstractï¼ˆæ‘˜è¦ï¼‰<a aria-hidden="true" class="hash-link" href="#abstractæ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Dynamic neural network is an emerging research topic in deep learning. Compared to static models which have fixed computational graphs and parameters at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to notable advantages in terms of accuracy, computational efficiency, adaptiveness, etc. In this survey, we comprehensively review this rapidly developing area by dividing dynamic networks into three main categories: 1) instance-wise dynamic models that process each instance with data-dependent architectures or parameters; 2) spatial-wise dynamic networks that conduct adaptive computation with respect to different spatial locations of image data and 3) temporal-wise dynamic models that perform adaptive inference along the temporal dimension for sequential data such as videos and texts. The important research problems of dynamic networks, e.g., architecture design, decision making scheme, optimization technique and applications, are reviewed systematically. Finally, we discuss the open problems in this field together with interesting future research directions.</p></blockquote><p>åŠ¨æ€ç¥ç»ç½‘ç»œè¿‘äº›å¹´çš„ç›¸å…³ç ”ç©¶é€æ¸å˜å¤šï¼Œæ¯”èµ·å›ºå®šè®¡ç®—å›¾çš„ä¼ ç»Ÿçš„é™æ€ç¥ç»ç½‘ç»œï¼ŒåŠ¨æ€ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¯ä»¥æ ¹æ®è¾“å…¥çš„å…·ä½“æ•°æ®è°ƒæ•´å®ƒä»¬çš„ç»“æ„æˆ–æ˜¯å‚æ•°ï¼ŒåŒæ—¶åœ¨é€Ÿåº¦å’Œç²¾åº¦æ–¹é¢å æœ‰ä¼˜åŠ¿ã€‚ä¸€ç§æ¯”å–»æ˜¯ï¼šâ€œåœ¨è¾“å…¥è¾ƒä¸ºç®€å•æ—¶ï¼ŒåŠ¨æ€ç¥ç»ç½‘ç»œå¯ä»¥å¾ˆå¿«ï¼›åœ¨è¾“å…¥è¾ƒä¸ºå¤æ‚æ—¶ï¼ŒåŠ¨æ€ç¥ç»ç½‘ç»œå¯ä»¥ç²¾åº¦å¾ˆé«˜â€ã€‚</p><p>è¿™ç¯‡è®ºæ–‡æ¦‚æ‹¬åœ°ä»‹ç»äº†åŠ¨æ€ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•â€œåŠ¨æ€â€çš„ï¼Œä»¥åŠåŠ¨æ€å¸¦æ¥äº†æ€æ ·çš„ä¼˜åŠ¿ã€‚ </p><p>æˆ‘çœ‹äº†è¿™ç¯‡ç»¼è¿°å—ç›ŠåŒªæµ…ï¼Œå¦‚æœæœ‰æ—¶é—´çš„è¯è¯·é˜…è¯»<a href="http://arxiv.org/abs/2102.04906" target="_blank" rel="noopener noreferrer">åŸä½œ</a>ã€‚æœ¬æ–‡åªæ˜¯å¯¹åŸä½œé˜…è¯»çš„ç²—æµ…ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/survey">survey</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/dynamic-neural-network">dynamic-neural-network</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Dynamic Neural Networks - A Survey" href="/blog/[08]Dynamic-Neural-Networks-A-Survey"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog"><div class="pagination-nav__label">Â«<!-- --> <!-- -->Newer Entries</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/page/3"><div class="pagination-nav__label">Older Entries<!-- --> <!-- -->Â»</div></a></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>