<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">Blog | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" property="og:title" content="Blog | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="description" content="Blog"><meta data-react-helmet="true" property="og:description" content="Blog"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/page/5"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_posts_list"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/page/5"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/page/5" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/page/5" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">é­”æ³•éƒ¨æ—¥å¿—</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://arxiv.org/abs/1808.04560" target="_blank" rel="noopener noreferrer">Deep Retinex Decomposition for Low-Light Enhancement</a></p><p>è®ºæ–‡ä½œè€…: Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu</p><p>Code: <a href="https://github.com/weichen582/RetinexNet" target="_blank" rel="noopener noreferrer">https://github.com/weichen582/RetinexNet</a></p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>é‡‡ç”¨äº†åˆ†è§£ç½‘ç»œå’Œå¢å¼ºç½‘ç»œï¼Œä½¿ç”¨Retinexç†è®ºæ„å»ºåˆ†è§£ç½‘ç»œï¼Œåˆ†è§£åå†è¿›è¡Œå¢å¼ºã€‚</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deepRetinex-Netlearned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjusment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.</p></blockquote><p>Retinexæ¨¡å‹æ˜¯å¼±å…‰å›¾åƒå¢å¼ºçš„æœ‰æ•ˆå·¥å…·ã€‚å®ƒå‡è®¾è§‚å¯Ÿåˆ°çš„å›¾åƒ<strong>å¯ä»¥åˆ†è§£ä¸ºåå°„ç‡å’Œç…§åº¦</strong>ã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºè§†ç½‘è†œçš„æ–¹æ³•éƒ½ä¸ºè¿™ç§é«˜åº¦ä¸é€‚å®šçš„åˆ†è§£ç²¾å¿ƒè®¾è®¡äº†æ‰‹å·¥åˆ¶ä½œçš„çº¦æŸå’Œå‚æ•°ï¼Œå½“åº”ç”¨äºå„ç§åœºæ™¯æ—¶ï¼Œè¿™å¯èƒ½ä¼šå—åˆ°æ¨¡å‹å®¹é‡çš„é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå¯¹çš„å¼±å…‰æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šæå‡ºäº†ä¸€ä¸ªDeeprinex Netlearnï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºåˆ†è§£çš„Decom-Netå’Œä¸€ä¸ªç”¨äºå…‰ç…§è°ƒæ•´çš„å¢å¼º-Netã€‚åœ¨Decom-Netçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å­˜åœ¨åˆ†è§£åå°„ç‡å’Œå…‰ç…§çš„åŸºæœ¬äº‹å®ã€‚è¯¥ç½‘ç»œä»…åœ¨å…³é”®çº¦æŸæ¡ä»¶ä¸‹å­¦ä¹ ï¼ŒåŒ…æ‹¬æˆå¯¹çš„å¼±å…‰/æ­£å¸¸å…‰å›¾åƒå…±äº«çš„ä¸€è‡´åå°„ç‡ä»¥åŠç…§æ˜çš„å¹³æ»‘åº¦ã€‚åœ¨åˆ†è§£çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¢å¼ºç½‘ç»œå¯¹å…‰ç…§è¿›è¡Œåç»­çš„äº®åº¦å¢å¼ºï¼Œå¯¹äºè”åˆå»å™ªï¼Œå¯¹åå°„ç‡è¿›è¡Œå»å™ªæ“ä½œã€‚RetinexNetæ˜¯ç«¯åˆ°ç«¯è®­ç»ƒçš„ï¼Œå› æ­¤å­¦ä¹ çš„åˆ†è§£æœ¬è´¨ä¸Šæœ‰åˆ©äºäº®åº¦è°ƒæ•´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¼±å…‰å¢å¼ºæ–¹é¢è·å¾—äº†è§†è§‰ä¸Šä»¤äººæ»¡æ„çš„è´¨é‡ï¼Œè€Œä¸”æä¾›äº†å›¾åƒåˆ†è§£çš„è‰¯å¥½è¡¨ç¤ºã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Deep Retinex Decomposition for Low-Light Enhancement" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://arxiv.org/abs/1711.02488" target="_blank" rel="noopener noreferrer">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></p><p>è®ºæ–‡ä½œè€…: Liang Shen, Zihan Y ue, Fan Feng, Quan Chen, Shihao Liu, Jie Ma</p><p>Code: None</p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£åŸºäºRetinexç†è®ºä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>åŸºäºMSRä¼ ç»Ÿç†è®ºæ„é€ å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹</li><li>ç›´æ¥å­¦ä¹ æš—å›¾åƒå’Œäº®å›¾åƒä¹‹é—´çš„ç«¯åˆ°ç«¯æ˜ å°„</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Images captured in low-light conditions usually suffer from very low contrast, which increases the difficulty of sub-sequent computer vision tasks in a great extent. In this paper, a low-light image enhancement model based on convolutional neural network and Retinex theory is proposed. Firstly, we show that multi-scale Retinex is equivalent to a feedforward convolutional neural network with different Gaussian convolution kernels. Motivated by this fact, we consider a Convolutional Neural Network(MSR-net) that directly learns an end-to-end mapping between dark and bright images. Different fundamentally from existing approaches, low-light image enhancement in this paper is regarded as a machine learning problem. In this model, most of the parameters are optimized by back-propagation, while the parameters of traditional models depend on the artificial setting. Experiments on a number of challenging images reveal the advantages of our method in comparison with other state-of-the-art methods from the qualitative and quantitative perspective.</p></blockquote><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå·ç§¯ç¥ç»ç½‘ç»œå’Œè§†ç½‘è†œç†è®º(Retinex Theory)çš„ä½ç…§åº¦å›¾åƒå¢å¼ºæ¨¡å‹ã€‚è¯æ˜äº†å¤šå°ºåº¦è§†ç½‘è†œç­‰ä»·äºä¸€ä¸ªå…·æœ‰ä¸åŒé«˜æ–¯å·ç§¯æ ¸çš„å‰é¦ˆå·ç§¯ç¥ç»ç½‘ç»œã€‚è€ƒè™‘ä¸€ç§å·ç§¯ç¥ç»ç½‘ç»œ(MSRç½‘ç»œ)ï¼Œå®ƒ<strong>ç›´æ¥å­¦ä¹ æš—å›¾åƒå’Œäº®å›¾åƒä¹‹é—´çš„ç«¯åˆ°ç«¯æ˜ å°„</strong>ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MSR-net - Low-light Image Enhancement Using Deep Convolutional Network" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°: <a href="https://ieeexplore.ieee.org/abstract/document/8305143/" target="_blank" rel="noopener noreferrer">LLCNN: A convolutional neural network for low-light image enhancement</a></p><p>è®ºæ–‡ä½œè€…: Li Tao, Chuang Zhu, Guoqing Xiang, Yuan Li, Huizhu Jia, Xiaodong Xie</p><p>Code: <a href="https://github.com/BestJuly/LLCNN" target="_blank" rel="noopener noreferrer">https://github.com/BestJuly/LLCNN</a></p></blockquote><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯pommespeter">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/PommesPeter" target="_blank" rel="noopener noreferrer">PommesPeter</a>ã€‚<a aria-hidden="true" class="hash-link" href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯pommespeter" title="Direct link to heading">â€‹</a></h3><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼ºçš„è®ºæ–‡ã€‚</p><ul><li>æœ¬æ–‡ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œä½ç…§åº¦å¢å¼º</li><li>ä½¿ç”¨SSIMæŸå¤±æ›´å¥½åœ°è¯„ä»·å›¾åƒå¥½åå’Œæ¢¯åº¦æ”¶æ•›</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-æ‘˜è¦">Abstract (æ‘˜è¦)<a aria-hidden="true" class="hash-link" href="#abstract-æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>In this paper, we propose a CNN based method to perform low-light image enhancement. We design a special  module to utilize multiscale feature maps, which can avoid  gradient vanishing problem as well. In order to preserve image textures as much as possible, we use SSIM loss to train our model. The contrast of low-light images can be adaptively enhanced using our method. Results demonstrate that our CNN based method  outperforms other contrast enhancement methods. </p></blockquote><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCNNçš„ä½ç…§åº¦å›¾åƒå¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹æ®Šçš„æ¨¡å—æ¥<strong>åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾æ˜ å°„</strong>ï¼Œè¿™æ ·å¯ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚<strong>ä¸ºäº†å°½å¯èƒ½åœ°ä¿ç•™å›¾åƒçº¹ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨SSIMæŸå¤±æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹</strong>ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥<strong>è‡ªé€‚åº”åœ°å¢å¼ºå¼±å…‰å›¾åƒçš„å¯¹æ¯”åº¦</strong>ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/low-light">low-light</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about LLCNN - A convolutional neural network for low-light image enhancement" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼šVOLO: Vision Outlooker for Visual Recognition</p><p>ä½œè€…ï¼šLi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, Shuicheng Yan</p><p>Codeï¼š <a href="https://github.com/sail-sg/volo" target="_blank" rel="noopener noreferrer">https://github.com/sail-sg/volo</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><ul><li>è§†è§‰è¯†åˆ«ä»»åŠ¡å·²è¢«<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>N</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">CNN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">CNN</span></span></span></span></span>ä¸»å®°å¤šå¹´ã€‚åŸºäºè‡ªæ³¨æ„åŠ›çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>i</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">ViT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">Vi</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span>åœ¨<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">ImageNet</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span></span>åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºäº†æå¤§çš„æ½œåŠ›ï¼Œåœ¨æ²¡æœ‰é¢å¤–æ•°æ®å‰æä¸‹ï¼Œ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>e</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">Transformer</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">an</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.02778em">er</span></span></span></span></span>çš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>N</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">CNN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">CNN</span></span></span></span></span>æ¨¡å‹ä»å…·æœ‰å·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç¼©å°è¿™ä¸¤è€…ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå¹¶ä¸”è¯æ˜äº†åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç¡®å®èƒ½å¤Ÿæ¯”<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>N</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">CNN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">CNN</span></span></span></span></span>è¡¨ç°æ›´å¥½ã€‚</li><li>ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬å‘ç°é™åˆ¶<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>i</mi><mi>T</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">ViTs</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">Vi</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mord mathnormal">s</span></span></span></span></span>åœ¨<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">ImageNet</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span></span>åˆ†ç±»ä¸­çš„æ€§èƒ½çš„ä¸»è¦å› ç´ æ˜¯å…¶åœ¨å°†ç»†ç²’åº¦çº§åˆ«çš„ç‰¹å¾ç¼–ç ä¹˜<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">Token</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span></span>è¡¨ç¤ºè¿‡ç¨‹ä¸­æ¯”è¾ƒä½æ•ˆï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">outlook</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em">tl</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>æ³¨æ„åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç®€å•è€Œé€šç”¨çš„æ¶æ„ï¼Œç§°ä¸º<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">Vision</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">Vi</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span></span></span></span></span> <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">outlooker</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em">tl</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal" style="margin-right:0.02778em">er</span></span></span></span></span> (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>O</mi><mi>L</mi><mi>O</mi></mrow><annotation encoding="application/x-tex">VOLO</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.02778em">O</span></span></span></span></span>)ã€‚<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">outlook</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em">tl</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>æ³¨æ„åŠ›ä¸»è¦å°†<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>i</mi><mi>n</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">fine</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal">in</span><span class="mord mathnormal">e</span></span></span></span></span>â€‹-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">level</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span></span></span></span></span>çº§åˆ«çš„ç‰¹å¾å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ›´é«˜æ•ˆåœ°ç¼–ç åˆ°<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">token</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span></span>è¡¨ç¤ºä¸­ï¼Œè¿™äº›<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">token</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span></span>å¯¹è¯†åˆ«æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†å¾€å¾€è¢«è‡ªæ³¨æ„åŠ›æ‰€å¿½è§†ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸ä½¿ç”¨ä»»ä½•é¢å¤–è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>O</mi><mi>L</mi><mi>O</mi></mrow><annotation encoding="application/x-tex">VOLO</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.02778em">O</span></span></span></span></span>åœ¨<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">ImageNet</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span></span>-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>K</mi></mrow><annotation encoding="application/x-tex">1K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">1</span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span>åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†87.1%çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>o</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">top</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span></span></span></span></span>-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>å‡†ç¡®ç‡ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¶…è¿‡87%çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒå¥½çš„VOLOæ¨¡å‹è¿˜å¯ä»¥å¾ˆå¥½åœ°è¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬åœ¨<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>s</mi><mi>c</mi><mi>a</mi><mi>p</mi><mi>e</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">Cityscapes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ysc</span><span class="mord mathnormal">a</span><span class="mord mathnormal">p</span><span class="mord mathnormal">es</span></span></span></span></span>éªŒè¯é›†ä¸Šè·å¾—äº†84.3% <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>I</mi><mi>o</mi><mi>U</mi></mrow><annotation encoding="application/x-tex">mIoU</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10903em">U</span></span></span></span></span>ï¼Œåœ¨<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>D</mi><mi>E</mi><mn>20</mn><mi>K</mi></mrow><annotation encoding="application/x-tex">ADE20K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mord">20</span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span>éªŒè¯é›†ä¸Šè·å¾—äº†54.3%çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>I</mi><mi>o</mi><mi>U</mi></mrow><annotation encoding="application/x-tex">mIoU</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10903em">U</span></span></span></span></span>ï¼Œå‡åˆ›ä¸‹äº†æœ€æ–°è®°å½•ã€‚</li></ul><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210730004322.png"><p><strong>æ€»ç»“</strong>ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ³¨æ„åŠ›æœºåˆ¶â€”â€”<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mi>u</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>k</mi><mtext>Â </mtext><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">Outlook\ Attention</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em">tl</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace">Â </span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span></span></span></span></span>ï¼Œä¸ç²—ç•¥å»ºæ¨¡å…¨å±€é•¿è·ç¦»å…³ç³»çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>e</mi><mi>l</mi><mi>f</mi><mtext>Â </mtext><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">Self\ Attention</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace">Â </span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span></span></span></span></span>ä¸åŒï¼Œ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mi>u</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">Outlook</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em">tl</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>èƒ½åœ¨é‚»åŸŸä¸Šæ›´ç²¾ç»†åœ°ç¼–ç é¢†åŸŸç‰¹å¾ï¼Œå¼¥è¡¥äº†<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>e</mi><mi>l</mi><mi>f</mi><mtext>Â </mtext><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">Self\ Attention</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace">Â </span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span></span></span></span></span>å¯¹æ›´ç²¾ç»†ç‰¹å¾ç¼–ç çš„ä¸è¶³ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="outlooker-attention">OutLooker Attention<a aria-hidden="true" class="hash-link" href="#outlooker-attention" title="Direct link to heading">â€‹</a></h2><p><strong>OutLooker</strong>æ¨¡å—å¯è§†ä½œæ‹¥æœ‰ä¸¤ä¸ªç‹¬ç«‹é˜¶æ®µçš„ç»“æ„ï¼Œç¬¬ä¸€ä¸ªéƒ¨åˆ†åŒ…å«ä¸€å †<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mi>u</mi><mi>t</mi><mi>L</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">OutLooker</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">L</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal" style="margin-right:0.02778em">er</span></span></span></span></span>ç”¨äºç”Ÿæˆç²¾ç»†åŒ–çš„è¡¨ç¤ºï¼ˆ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">Token</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span></span></span></span></span> <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">representations</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal">re</span><span class="mord mathnormal">p</span><span class="mord mathnormal">rese</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">s</span></span></span></span></span>ï¼‰ï¼Œç¬¬äºŒä¸ªéƒ¨åˆ†éƒ¨ç½²ä¸€ç³»åˆ—çš„è½¬æ¢å™¨æ¥èšåˆå…¨å±€ä¿¡æ¯ã€‚åœ¨æ¯ä¸ªéƒ¨åˆ†ä¹‹å‰ï¼Œéƒ½æœ‰å—åµŒå…¥æ¨¡å—ï¼ˆ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">patch</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span></span></span></span></span> <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">embedding</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">mb</span><span class="mord mathnormal">e</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span></span> <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>u</mi><mi>l</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">module</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">e</span></span></span></span></span>ï¼‰å°†è¾“å…¥æ˜ å°„åˆ°æŒ‡å®šå½¢çŠ¶ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attention-mechanism">attention-mechanism</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/non-convolution">non-convolution</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about VOLO - Vision Outlooker for Visual Recognition" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/pdf/2107.00782.pdf" target="_blank" rel="noopener noreferrer">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</a></p><p>ä½œè€…ï¼šHuajun Liu,  Fuqiang Liu, Xinyi Fan</p><p>Codeï¼š<a href="https://github.com/DeLightCMU/PSA" target="_blank" rel="noopener noreferrer">https://github.com/DeLightCMU/PSA</a></p></blockquote><p>è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/asthestarsfalll" target="_blank" rel="noopener noreferrer">AsTheStarsFall</a>ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><p>ç»†ç²’åº¦çš„åƒç´ çº§ä»»åŠ¡ï¼ˆæ¯”å¦‚è¯­ä¹‰åˆ†å‰²ï¼‰ä¸€ç›´éƒ½æ˜¯è®¡ç®—æœºè§†è§‰ä¸­éå¸¸é‡è¦çš„ä»»åŠ¡ã€‚ä¸åŒäºåˆ†ç±»æˆ–è€…æ£€æµ‹ï¼Œç»†ç²’åº¦çš„åƒç´ çº§ä»»åŠ¡è¦æ±‚æ¨¡å‹åœ¨ä½è®¡ç®—å¼€é”€ä¸‹ï¼Œèƒ½å¤Ÿå»ºæ¨¡é«˜åˆ†è¾¨ç‡è¾“å…¥/è¾“å‡ºç‰¹å¾çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œè¿›è€Œæ¥ä¼°è®¡é«˜åº¦éçº¿æ€§çš„åƒç´ è¯­ä¹‰ã€‚<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>N</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">CNN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">CNN</span></span></span></span></span>â€‹â€‹â€‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæ•è·é•¿è·ç¦»çš„ä¾èµ–å…³ç³»ï¼Œä½†æ˜¯è¿™ç§æ–¹å¼ååˆ†å¤æ‚ä¸”<strong>å¯¹å™ªå£°æ•æ„Ÿ</strong>ã€‚</p><p>æœ¬æ–‡æå‡ºäº†å³æ’å³ç”¨çš„æåŒ–è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—åŒ…å«ä¸¤ä¸ªå…³é”®è®¾è®¡ï¼Œä»¥ä¿è¯é«˜è´¨é‡çš„åƒç´ å›å½’ï¼š</p><ol><li>æåŒ–æ»¤æ³¢ï¼ˆ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>o</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>z</mi><mi>e</mi><mi>d</mi><mtext>Â </mtext><mi>f</mi><mi>i</mi><mi>l</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">Polarized\ filtering</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mord mathnormal">d</span><span class="mspace">Â </span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal">i</span><span class="mord mathnormal">lt</span><span class="mord mathnormal" style="margin-right:0.02778em">er</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span></span>â€‹ï¼‰ï¼šåœ¨é€šé“å’Œç©ºé—´ç»´åº¦ä¿æŒæ¯”è¾ƒé«˜çš„åˆ†è¾¨ç‡ï¼ˆåœ¨é€šé“ä¸Šä¿æŒ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">C/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mord">/2</span></span></span></span></span>â€‹çš„ç»´åº¦ï¼Œåœ¨ç©ºé—´ä¸Šä¿æŒ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[H,W]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose">]</span></span></span></span></span>â€‹çš„ç»´åº¦ ï¼‰ï¼Œè¿›ä¸€æ­¥å‡å°‘ä½åˆ†è¾¨ç‡ã€ä½é€šé“æ•°å’Œä¸Šé‡‡æ ·é€ æˆçš„ä¿¡æ¯æŸå¤±ã€‚</li><li>å¢å¼ºï¼ˆ<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>n</mi><mi>h</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">Enhancement</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mord mathnormal">nhan</span><span class="mord mathnormal">ce</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span></span>â€‹ï¼‰ï¼šé‡‡ç”¨ç»†ç²’åº¦å›å½’è¾“å‡ºåˆ†å¸ƒçš„éçº¿æ€§å‡½æ•°ã€‚</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attention-mechanism">attention-mechanism</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Polarized Self-Attention - Towards High-quality Pixel-wise Regression" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="http://proceedings.mlr.press/v139/yang21o/yang21o.pdf" target="_blank" rel="noopener noreferrer">SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></p><p>ä½œè€…ï¼š<a href="https://zjjconan.github.io/" target="_blank" rel="noopener noreferrer"><strong>Lingxiao Yang</strong></a>, <a href="https://ruyuanzhang.github.io/" target="_blank" rel="noopener noreferrer">Ru-Yuan Zhang</a>, <a href="https://github.com/lld533" target="_blank" rel="noopener noreferrer">Lida Li</a>, <a href="http://sdcs.sysu.edu.cn/content/2478" target="_blank" rel="noopener noreferrer">Xiaohua Xie</a></p><p>Codeï¼š<a href="https://github.com/ZjjConan/SimAM" target="_blank" rel="noopener noreferrer">https://github.com/ZjjConan/SimAM</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä»‹ç»">ä»‹ç»<a aria-hidden="true" class="hash-link" href="#ä»‹ç»" title="Direct link to heading">â€‹</a></h2><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„3Dæ³¨æ„åŠ›æ¨¡å—ï¼ŒåŸºäºè‘—åçš„ç¥ç»ç§‘å­¦ç†è®ºï¼Œæå‡ºäº†ä¸€ç§èƒ½é‡å‡½æ•°ï¼Œå¹¶ä¸”æ¨å¯¼å‡ºå…¶å¿«é€Ÿè§£æè§£ï¼Œèƒ½å¤Ÿä¸ºæ¯ä¸€ä¸ªç¥ç»å…ƒåˆ†é…æƒé‡ã€‚ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼š</p><ul><li>å—äººè„‘æ³¨æ„æœºåˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰3Dæƒé‡çš„æ³¨æ„æ¨¡å—ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªèƒ½é‡å‡½æ•°æ¥è®¡ç®—æƒé‡ï¼›</li><li>æ¨å¯¼äº†èƒ½é‡å‡½æ•°çš„å°é—­å½¢å¼çš„è§£ï¼ŒåŠ é€Ÿäº†æƒé‡è®¡ç®—ï¼Œå¹¶ä¿æŒæ•´ä¸ªæ¨¡å—çš„è½»é‡ï¼›</li><li>å°†è¯¥æ¨¡å—åµŒå…¥åˆ°ç°æœ‰ConvNetä¸­åœ¨ä¸åŒä»»åŠ¡ä¸Šè¿›è¡Œäº†çµæ´»æ€§ä¸æœ‰æ•ˆæ€§çš„éªŒè¯ã€‚</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attention-mechanism">attention-mechanism</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/param-less">param-less</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/non-convolution">non-convolution</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+X" target="_blank" rel="noopener noreferrer">Xinlong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kong%2C+T" target="_blank" rel="noopener noreferrer">Tao Kong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen%2C+C" target="_blank" rel="noopener noreferrer">Chunhua Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang%2C+Y" target="_blank" rel="noopener noreferrer">Yuning Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+L" target="_blank" rel="noopener noreferrer">Lei Li</a></p><blockquote><p>We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the &#x27;detect-thensegment&#x27; strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of &quot;instance categories&quot;, which assigns categories to each pixel within an instance according to the instance&#x27;s location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.</p></blockquote><p>å®ä¾‹åˆ†å‰²ç›¸æ¯”äºè¯­ä¹‰åˆ†å‰²ï¼Œä¸ä»…éœ€è¦é¢„æµ‹å‡ºæ¯ä¸€ä¸ªåƒç´ ç‚¹çš„è¯­ä¹‰ç±»åˆ«ï¼Œè¿˜è¦åˆ¤æ–­å‡ºè¯¥åƒç´ ç‚¹å±äºå“ªä¸€ä¸ªå®ä¾‹ã€‚ä»¥å¾€<strong>äºŒé˜¶æ®µ</strong>çš„æ–¹æ³•ä¸»è¦æ˜¯ï¼š</p><ol><li>å…ˆæ£€æµ‹ååˆ†å‰²ï¼šä¾‹å¦‚Mask R-CNN ï¼Œå…ˆç”¨æ£€æµ‹çš„æ–¹æ³•åˆ°å¾—æ¯ä¸€ä¸ªå®ä¾‹ï¼Œç„¶åå¯¹è¯¥å®ä¾‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œåˆ†å‰²å¾—åˆ°çš„åƒç´ éƒ½å±äºæ­¤å®ä¾‹ã€‚</li><li>å…ˆåˆ†å‰²ååˆ†ç±»ï¼šå…ˆé‡‡ç”¨è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•å¯¹æ•´ä¸ªå›¾çš„æ‰€æœ‰åƒç´ ç‚¹åšè¯­ä¹‰ç±»åˆ«çš„é¢„æµ‹ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œä½¿ç”¨èšç±»æ–¹æ³•æ‹‰è¿‘å±äºåŒä¸€å®ä¾‹çš„åƒç´ ç‚¹ï¼Œä½¿å®ƒä»¬å±äºåŒä¸€ç±»ï¼ˆåŒä¸ªå®ä½“ï¼‰ã€‚</li></ol><p>å•é˜¶æ®µæ–¹æ³•ï¼ˆSingle Stage Instance Segmentationï¼‰æ–¹é¢çš„å·¥ä½œå—åˆ°å•é˜¶æ®µç›®æ ‡æ£€æµ‹çš„å½±å“å¤§ä½“ä¸Šä¹Ÿåˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç§æ˜¯å—one-stage, anchot-basedæ£€æµ‹æ¨¡å‹å¦‚YOLOï¼ŒRetinaNetå¯å‘ï¼Œä»£è¡¨ä½œæœ‰YOLACTå’ŒSOLOï¼›ä¸€ç§æ˜¯å—anchor-freeæ£€æµ‹æ¨¡å‹å¦‚ FCOS å¯å‘ï¼Œä»£è¡¨ä½œæœ‰PolarMaskå’ŒAdaptISã€‚ä¸Šè¿°è¿™äº›å®ä¾‹åˆ†å‰²çš„æ–¹æ³•éƒ½ä¸é‚£ä¹ˆç›´æ¥ï¼Œä¹Ÿä¸é‚£ä¹ˆç®€å•ã€‚SOLOçš„å‡ºå‘ç‚¹å°±æ˜¯åšæ›´ç®€å•ã€æ›´ç›´æ¥çš„å®ä¾‹åˆ†å‰²ã€‚</p><p>åŸºäºå¯¹MSCOCOæ•°æ®é›†çš„ç»Ÿè®¡ï¼Œä½œè€…æå‡ºï¼ŒéªŒè¯å­é›†ä¸­æ€»å…±æœ‰36780ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­98.3ï¼…çš„å¯¹è±¡å¯¹çš„ä¸­å¿ƒè·ç¦»å¤§äº30ä¸ªåƒç´ ã€‚è‡³äºå…¶ä½™çš„1.7ï¼…çš„å¯¹è±¡å¯¹ï¼Œå…¶ä¸­40.5ï¼…çš„å¤§å°æ¯”ç‡å¤§äº1.5å€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸è€ƒè™‘åƒXå½¢ä¸¤ä¸ªç‰©ä½“è¿™æ ·çš„å°‘æ•°æƒ…å†µã€‚æ€»ä¹‹ï¼Œ<strong>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå›¾åƒä¸­çš„ä¸¤ä¸ªå®ä¾‹è¦ä¹ˆå…·æœ‰ä¸åŒçš„ä¸­å¿ƒä½ç½®ï¼Œè¦ä¹ˆå…·æœ‰ä¸åŒçš„å¯¹è±¡å¤§å°</strong>ã€‚</p><p>äºæ˜¯ä½œè€…æå‡ºé€šè¿‡ç‰©ä½“åœ¨å›¾ç‰‡ä¸­çš„<strong>ä½ç½®</strong>å’Œ<strong>å½¢çŠ¶</strong>æ¥è¿›è¡Œå®ä¾‹çš„åŒºåˆ†ã€‚åŒä¸€å¼ å›¾ç‰‡ä¸­ï¼Œä½ç½®å’Œå½¢çŠ¶å®Œå…¨ç›¸åŒï¼Œå°±æ˜¯åŒä¸€ä¸ªå®ä¾‹ï¼Œç”±äºå½¢çŠ¶æœ‰å¾ˆå¤šæ–¹é¢ï¼Œæ–‡ç« ä¸­æœ´ç´ åœ°ä½¿ç”¨å°ºå¯¸æè¿°å½¢çŠ¶ã€‚</p><p>è¯¥æ–¹æ³•ä¸ Mask R-CNN å®ç°äº†åŒç­‰å‡†ç¡®åº¦ï¼Œå¹¶ä¸”åœ¨å‡†ç¡®åº¦ä¸Šä¼˜äºæœ€è¿‘çš„å•æ¬¡å®ä¾‹åˆ†å‰²å™¨ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-branch">multi-branch</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about SOLO - Segmenting Objects by Locations" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bolya%2C+D" target="_blank" rel="noopener noreferrer">Daniel Bolya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+C" target="_blank" rel="noopener noreferrer">Chong Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao%2C+F" target="_blank" rel="noopener noreferrer">Fanyi Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+Y+J" target="_blank" rel="noopener noreferrer">Yong Jae Lee</a></p><blockquote><p>We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn&#x27;t depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.</p></blockquote><p>YOLACTæ˜¯You Only Look At CoefficienTs çš„ç®€å†™ï¼Œå…¶ä¸­ coefficients æ˜¯è¿™ä¸ªæ¨¡å‹çš„è¾“å‡ºä¹‹ä¸€ï¼Œè¿™ä¸ªå‘½åé£æ ¼åº”æ˜¯è‡´æ•¬äº†å¦ä¸€ç›®æ ‡æ£€æµ‹æ¨¡å‹ YOLOã€‚</p><p><img alt="image-20210818180207356" src="/assets/images/image-20210818180207356-905e78fcdbb7ba866da030d933f9b7dc.png"></p><p>ä¸Šå›¾ï¼šYOLACTçš„ç½‘ç»œç»“æ„å›¾ã€‚<strong>YOLACTçš„ç›®æ ‡æ˜¯å°†æ©æ¨¡åˆ†æ”¯æ·»åŠ åˆ°ç°æœ‰çš„ä¸€é˜¶æ®µï¼ˆone-stageï¼‰ç›®æ ‡æ£€æµ‹æ¨¡å‹</strong>ã€‚æˆ‘ä¸ªäººè§‰å¾—è¿™æ˜¯å¤¹åœ¨ä¸€é˜¶æ®µå’ŒäºŒé˜¶æ®µä¸­é—´çš„äº§ç‰©ã€‚å°†å…¶åˆ†ä¸ºä¸€é˜¶æ®µçš„ä¾æ®æ˜¯å…¶å®ç°â€œå°†æ©æ¨¡åˆ†æ”¯æ·»åŠ åˆ°ç°æœ‰çš„ä¸€é˜¶æ®µç›®æ ‡æ£€æµ‹æ¨¡å‹â€çš„æ–¹å¼ä¸Mask R-CNNå¯¹ Faster-CNN æ“ä½œç›¸åŒï¼Œä½†æ²¡æœ‰è¯¸å¦‚feature repoolingå’ŒROI alignç­‰æ˜ç¡®çš„ç›®æ ‡å®šä½æ­¥éª¤ã€‚ä¹Ÿå°±æ˜¯ï¼Œ<code>å®šä½-åˆ†ç±»-åˆ†å‰²</code>çš„æ“ä½œè¢«å˜æˆäº†<code>åˆ†å‰²-å‰ªè£</code>ã€‚</p><p>æ ¹æ®è¯„ä¼°ï¼Œå½“YOLACT å¤„ç†<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>550</mn><mo>Ã—</mo><mn>550</mn></mrow><annotation encoding="application/x-tex">550\times 550</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">550</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">550</span></span></span></span></span>â€‹â€‹â€‹å¤§å°çš„å›¾ç‰‡æ—¶ï¼Œå…¶é€Ÿåº¦è¾¾åˆ°äº† 33FPSï¼Œè€Œäº’è”ç½‘ä¸Šå¤šæ•°è§†é¢‘ä¸€èˆ¬æ˜¯ 30FPS çš„ï¼Œè¿™ä¹Ÿå°±æ˜¯å®æ—¶çš„å«ä¹‰äº†ã€‚è¿™æ˜¯å•é˜¶æ®µæ¯”è¾ƒæ—©çš„ä¸€ä»½å·¥ä½œï¼Œè™½ç„¶è¿™ä¸ªé€Ÿåº¦ä¸å¿«ä½†ä¹Ÿè¿˜è¡Œäº†ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-branch">multi-branch</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about YOLACT - Real-time Instance Segmentation" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Q" target="_blank" rel="noopener noreferrer">Qiang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y" target="_blank" rel="noopener noreferrer">Yingming Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+T" target="_blank" rel="noopener noreferrer">Tong Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+X" target="_blank" rel="noopener noreferrer">Xiangyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+J" target="_blank" rel="noopener noreferrer">Jian Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" target="_blank" rel="noopener noreferrer">Jian Sun</a></p><blockquote><p>This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - {\em utilizing only one-level feature for detection}. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5Ã— faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7Ã— less training epochs. With an image size of 608Ã—608, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4. Code is available at <a href="https://github.com/megvii-model/YOLOF" target="_blank" rel="noopener noreferrer">this https URL</a>.</p></blockquote><p>æœ¬æ–‡ç®€ç§°YOLOFã€‚æˆªè‡³åˆ°æœ¬æ–‡å†™ä½œæ—¶ï¼ŒäºŒé˜¶æ®µå’Œå•é˜¶æ®µç›®æ ‡æ£€æµ‹çš„SOTAæ–¹æ³•ä¸­å¹¿æ³›ä½¿ç”¨äº†å¤šå°ºåº¦ç‰¹å¾èåˆçš„æ–¹æ³•ã€‚FPNæ–¹æ³•å‡ ä¹å·²ç»ç§°ä¸ºäº†ç½‘ç»œä¸­ç†æ‰€åº”å½“çš„ä¸€ä¸ªç»„ä»¶ã€‚</p><p>æœ¬æ–‡ä¸­ä½œè€…é‡æ–°å›é¡¾äº†FPNæ¨¡å—ï¼Œå¹¶æŒ‡å‡ºFPNçš„ä¸¤ä¸ªä¼˜åŠ¿åˆ†åˆ«æ˜¯å…¶åˆ†æ²»ï¼ˆdivide-and-conquerï¼‰çš„è§£å†³æ–¹æ¡ˆã€ä»¥åŠå¤šå°ºåº¦ç‰¹å¾èåˆã€‚æœ¬æ–‡åœ¨å•é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨ä¸Šç ”ç©¶äº†FPNçš„è¿™ä¸¤ä¸ªä¼˜åŠ¿ï¼Œå¹¶åœ¨RetinaNetä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†ä¸Šè¿°ä¸¤ä¸ªä¼˜åŠ¿è§£è€¦ï¼Œåˆ†åˆ«ç ”ç©¶å…¶å‘æŒ¥çš„ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºï¼ŒFPNåœ¨å¤šå°ºåº¦ç‰¹å¾èåˆä¸Šå‘æŒ¥çš„ä½œç”¨å¯èƒ½æ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¤§ã€‚</p><p>æœ€åï¼Œä½œè€…æå‡ºYOLOFï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ä½¿ç”¨FPNçš„ç›®æ ‡æ£€æµ‹ç½‘ç»œã€‚å…¶ä¸»è¦åˆ›æ–°æ˜¯ï¼š</p><ol><li>Dilated Encoder</li><li>Uniform Matching</li></ol><p>è¯¥ç½‘ç»œåœ¨è¾¾åˆ°RetinaNetå¯¹ç­‰ç²¾åº¦çš„æƒ…å†µä¸‹é€Ÿåº¦æå‡äº†2.5å€ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fpn">fpn</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about You Only Look One-level Feature" href="/blog/[38]You-Only-Look-One-level-Feature"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai%2C+J" target="_blank" rel="noopener noreferrer">Jifeng Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" target="_blank" rel="noopener noreferrer">Kaiming He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+Y" target="_blank" rel="noopener noreferrer">Yi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren%2C+S" target="_blank" rel="noopener noreferrer">Shaoqing Ren</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" target="_blank" rel="noopener noreferrer">Jian Sun</a></p><blockquote><p>Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.</p></blockquote><p>è¿™ç¯‡å·¥ä½œåˆåInstanceFCNã€‚å®ä¾‹åˆ†å‰²æ–¹é¢ï¼Œç”±äºç½‘ç»œéš¾ä»¥åŒæ—¶è¿›è¡Œåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ï¼Œå› æ­¤é¦–å…ˆæµè¡Œçš„æ˜¯äºŒé˜¶æ®µå®ä¾‹åˆ†å‰²ç½‘ç»œï¼Œé¦–å…ˆå¯¹è¾“å…¥æ‰¾åˆ°å®ä¾‹çš„proposalï¼Œç„¶ååœ¨å…¶ä¸­è¿›è¡Œå¯†é›†é¢„æµ‹ï¼ˆä¹Ÿå°±æ˜¯å…ˆæ¡†æ¡†å†åˆ†å‰²ï¼‰ã€‚æœ¬æ–‡ä»åç§°ä¸Šçœ‹ä¸æ˜¯ä¸€ç¯‡è®²å®ä¾‹åˆ†å‰²çš„æ–‡ç« ï¼Œæ˜¯è®²å¦‚ä½•é€šè¿‡FCNè·å¾—å®ä¾‹çº§åˆ«çš„åˆ†å‰²maskçš„çš„ã€‚</p><p>åœ¨é˜…è¯»ä¹‹å‰æˆ‘æƒ³æé†’ä¸€ä¸‹ï¼Œè¿™ç¯‡å·¥ä½œçš„æ•ˆæœæ˜¯æ¯”è¾ƒå·®çš„ï¼Œæ¯•ç«Ÿæ˜¯æ—©æœŸå·¥ä½œã€‚ä¸è¿‡è¿™ç¯‡å·¥ä½œå…·æœ‰ä¸é”™çš„å¯å‘æ„ä¹‰ï¼Œå€¼å¾—è¯»ä¸€è¯»ã€‚åé¢çš„ä¸€ç¯‡å·¥ä½œFCISï¼ˆFully Convolutional Instance-aware Semantic Segmentationï¼‰ä¸­å°±å€Ÿé‰´äº†æœ¬æ–‡ä¸­æå‡ºçš„instance-sensitive score mapsï¼ˆè¯·ä¸è¦å¼„æ··æœ¬ç¯‡å·¥ä½œå’ŒFCISï¼‰ã€‚æœ¬æ–‡çš„ä¸€å¤§è´¡çŒ®å°±æ˜¯æå‡ºä½¿ç”¨instance-sensitive score mapsåŒºåˆ†ä¸åŒä¸ªä½“ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fcn">fcn</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/instance-segmentation">instance-segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Instance-sensitive Fully Convolutional Networks" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/page/4"><div class="pagination-nav__label">Â«<!-- --> <!-- -->Newer Entries</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/page/6"><div class="pagination-nav__label">Older Entries<!-- --> <!-- -->Â»</div></a></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>