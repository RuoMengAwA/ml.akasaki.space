<!doctype html>
<html lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿ Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">Blog | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" property="og:title" content="Blog | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="description" content="Blog"><meta data-react-helmet="true" property="og:description" content="Blog"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//blog/page/3"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_posts_list"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//blog/page/3"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/page/3" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//blog/page/3" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2aa28e4b.css">
<link rel="preload" href="/assets/js/runtime~main.70701027.js" as="script">
<link rel="preload" href="/assets/js/main.e4cdc564.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">é­”æ³•éƒ¨æ—¥å¿—</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">All posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[00]unlimited-paper-works">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust</a>ã€‚<a aria-hidden="true" class="hash-link" href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust" title="Direct link to heading">â€‹</a></h3><p>åŸè®ºæ–‡<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Networks for Object Detection</a>ã€‚</p><p>è¿™ç¯‡è®ºæ–‡å°±æ˜¯å¤§å®¶ç†ŸçŸ¥çš„FPNäº†ã€‚FPNæ˜¯<strong>æ¯”è¾ƒæ—©æœŸçš„ä¸€ä»½å·¥ä½œ</strong>ï¼ˆè¯·æ³¨æ„ï¼Œè¿™ç¯‡è®ºæ–‡åªæ˜¯å¤šå°ºåº¦ç‰¹å¾èåˆçš„ä¸€ç§æ–¹å¼ã€‚ä¸è¿‡è¿™ç¯‡è®ºæ–‡æå‡ºçš„æ¯”è¾ƒæ—©ï¼ˆCVPR2017ï¼‰ï¼Œåœ¨å½“æ—¶çœ‹æ¥æ˜¯éå¸¸å…ˆè¿›çš„ï¼‰ï¼Œåœ¨å½“æ—¶å…·æœ‰å¾ˆå¤šäº®ç‚¹ï¼šFPNä¸»è¦è§£å†³çš„æ˜¯ç‰©ä½“æ£€æµ‹ä¸­çš„å¤šå°ºåº¦é—®é¢˜ï¼Œé€šè¿‡ç®€å•çš„ç½‘ç»œè¿æ¥æ”¹å˜ï¼Œåœ¨åŸºæœ¬ä¸å¢åŠ åŸæœ‰æ¨¡å‹è®¡ç®—é‡æƒ…å†µä¸‹ï¼Œå¤§å¹…åº¦æå‡äº†å°ç‰©ä½“æ£€æµ‹çš„æ€§èƒ½ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstractæ‘˜è¦">Abstractï¼ˆæ‘˜è¦ï¼‰<a aria-hidden="true" class="hash-link" href="#abstractæ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></blockquote><p>è¿™ç¯‡è®ºæ–‡å¯¹ä»¥åçš„è®¸å¤šç½‘ç»œè®¾è®¡äº§ç”Ÿäº†è¾ƒå¤§çš„å½±å“ï¼Œæ¨èä½ é˜…è¯»<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">åŸæ–‡</a>ã€‚è¿™é‡Œåªæ˜¯å¯¹è¿™ç¯‡è®ºæ–‡çš„ç²—æµ…é˜…è¯»ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/fpn">FPN</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Feature Pyramid Networks for Object Detection" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡å…³äºç»¼è¿°è®ºæ–‡çš„è§£è¯»ã€‚<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆA Review on Deep Learning Techniques Applied to Semantic Segmentationï¼‰</a></p><p>æ‘˜è¦ï¼š</p><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>æˆ‘çœ‹äº†è¿™ç¯‡ç»¼è¿°å—ç›ŠåŒªæµ…ï¼Œå¦‚æœæœ‰æ—¶é—´çš„è¯è¯·é˜…è¯»<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">åŸä½œ</a>ã€‚æœ¬æ–‡åªæ˜¯å¯¹åŸä½œé˜…è¯»çš„ç²—æµ…ç¬”è®°ã€‚</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/survey">survey</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about A Review on Deep Learning Techniques Applied to Semantic Segmentation" href="/blog/[10]Overview-Of-Semantic-Segmentation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§è½»é‡çº§ä¸»å¹²ç½‘ç»œçš„è®ºæ–‡ã€‚<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆMobileNetV2: Inverted Residuals and Linear Bottlenecksï¼‰</a>ã€‚</p><ul><li>æœ¬æ–‡ä¸»è¦é’ˆå¯¹è½»é‡ç‰¹å¾æå–ç½‘ç»œä¸­ç»“æ„ä¸Šçš„ä¸‰ä¸ªä¿®æ”¹æé«˜äº†ç½‘ç»œæ€§èƒ½ã€‚</li><li>æœ¬æ–‡æ€»æ€è·¯ï¼šä½¿ç”¨ä½ç»´åº¦çš„å¼ é‡å¾—åˆ°è¶³å¤Ÿå¤šçš„ç‰¹å¾</li></ul><p>æ‘˜è¦:</p><blockquote><p>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and bench- marks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottle- neck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demon- strate that this improves performance and provide an in- tuition that led to this design. Finally, our approach allows decoupling of the in- put/output domains from the expressiveness of the trans- formation, which provides a convenient framework for further analysis. We measure our performance on ImageNet classification, COCO object detection <!-- -->[2]<!-- -->, VOC image segmentation <!-- -->[3]<!-- -->. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MobileNetV2 - Inverted Residuals and Linear Bottlenecks" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§å¿«é€Ÿè¯­ä¹‰åˆ†å‰²çš„è®ºæ–‡ã€‚è®ºæ–‡å:<a href="https://arxiv.org/abs/1902.04502" target="_blank" rel="noopener noreferrer">Fast-SCNN: Fast Semantic Segmentation Network</a></p><ul><li>ä¸»è¦æ˜¯é‡‡ç”¨åŒæµæ¨¡å‹çš„æ¶æ„è®¾è®¡è¿™ä¸ªç½‘ç»œ</li><li>æœ¬æ–‡æ€»æ€è·¯ï¼šå‡å°‘å†—ä½™çš„å·ç§¯è¿‡ç¨‹ï¼Œä»è€Œæé«˜é€Ÿåº¦</li></ul><p>æ‘˜è¦ï¼š</p><blockquote><p>The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024 Ã— 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our â€˜learning to downsampleâ€™ module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Fast-SCNN - Fast Semantic Segmentation Network" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>è¿™æ˜¯ä¸€ç¯‡è®²è§£ä¸€ç§è½»é‡çº§ä¸»å¹²ç½‘ç»œçš„è®ºæ–‡ã€‚<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener noreferrer">åŸè®ºæ–‡ï¼ˆMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applicationsï¼‰</a>ã€‚</p><ul><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åº”ç”¨äºç§»åŠ¨æˆ–è€…åµŒå…¥å¼è®¾å¤‡çš„é«˜æ•ˆç¥ç»ç½‘ç»œ</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ“ä½œæ•°è¾ƒå°çš„å·ç§¯æ¨¡å—æ·±åº¦å¯åˆ†ç¦»å·ç§¯(Depthwise Separable Convolutionï¼Œä»¥ä¸‹ç§°DSC)</li></ul><p>æ‘˜è¦:</p><blockquote><p>We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/detection">detection</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/backbone">backbone</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/light-weight">light-weight</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/1909.11519" target="_blank" rel="noopener noreferrer">Gated Channel Transformation for Visual Recognition</a></p><p>ä½œè€…ï¼šZongxin Yang, Linchao Zhu, Y u Wu, and Yi Yang</p><p>Codeï¼š<a href="https://github.com/z-x-yang/GCT" target="_blank" rel="noopener noreferrer">https://github.com/z-x-yang/GCT</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><ul><li>GCTæ¨¡å—æ˜¯ä¸€ä¸ªæ™®éé€‚ç”¨çš„é—¨æ§è½¬æ¢å•å…ƒï¼Œå¯ä¸ç½‘ç»œæƒé‡ä¸€èµ·ä¼˜åŒ–ã€‚</li><li>ä¸åŒäºSEneté€šè¿‡å…¨è¿æ¥çš„éšå¼å­¦ä¹ ï¼Œå…¶ä½¿ç”¨å¯è§£é‡Šçš„å˜é‡æ˜¾å¼åœ°å»ºæ¨¡é€šé“é—´çš„å…³ç³»ï¼Œå†³å®šæ˜¯ç«äº‰æˆ–æ˜¯åˆä½œã€‚</li></ul><p><strong>å…³é”®è¯ï¼šå¯è§£é‡Šæ€§ã€æ˜¾å¼å…³ç³»ã€é—¨æ§</strong></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä»‹ç»">ä»‹ç»<a aria-hidden="true" class="hash-link" href="#ä»‹ç»" title="Direct link to heading">â€‹</a></h2><ul><li>å•ä¸ªå·ç§¯å±‚åªå¯¹Feature Mapä¸­æ¯ä¸ªç©ºé—´ä½ç½®çš„ä¸´è¿‘å±€éƒ¨ä¸Šä¸‹æ–‡è¿›è¡Œæ“ä½œï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å±€éƒ¨æ­§ä¹‰ã€‚é€šå¸¸æœ‰ä¸¤ç§æ–¹æ³•è§£å†³è¿™ç§é—®é¢˜ï¼šä¸€æ˜¯å¢åŠ ç½‘ç»œçš„æ·±åº¦ï¼Œå¦‚VGGï¼ŒResnetï¼ŒäºŒæ˜¯å¢åŠ ç½‘ç»œçš„å®½åº¦æ¥è·å¾—æ›´å¤šçš„å…¨å±€ä¿¡æ¯ï¼Œå¦‚GEnetå¤§é‡ä½¿ç”¨é¢†åŸŸåµŒå…¥ï¼ŒSEneté€šè¿‡å…¨å±€åµŒå…¥ä¿¡æ¯æ¥å»ºæ¨¡é€šé“å…³ç³»ã€‚</li><li>ç„¶è€ŒSEnetä¸­ä½¿ç”¨fcå±‚ä¼šå‡ºç°ä¸¤ä¸ªé—®é¢˜ï¼š<ol><li>ç”±äºä½¿ç”¨äº†fcå±‚ï¼Œå‡ºäºèŠ‚çœå‚æ•°çš„è€ƒè™‘ï¼Œæ— æ³•åœ¨æ‰€æœ‰å±‚ä¸Šä½¿ç”¨</li><li>fcå±‚çš„å‚æ•°è¾ƒä¸ºå¤æ‚ï¼Œéš¾ä»¥åˆ†æä¸åŒé€šé“é—´çš„å…³è”æ€§ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ç§<strong>éšå¼</strong>å­¦ä¹ </li><li>æ”¾åœ¨æŸäº›å±‚ä¹‹åä¼šå‡ºç°é—®é¢˜</li></ol></li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attention-mechanism">attention-mechanism</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Gated Channel Transformation for Visual Recognition" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/1807.06521" target="_blank" rel="noopener noreferrer">CBAM: Convolutional Block Attention Module</a></p><p>ä½œè€…ï¼šSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweonï¼ŒKorea Advanced Institute of Science and Technology, Daejeon, Korea</p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="æ‘˜è¦">æ‘˜è¦<a aria-hidden="true" class="hash-link" href="#æ‘˜è¦" title="Direct link to heading">â€‹</a></h2><ul><li>CBAMï¼ˆConvolutional Block Attention Moudule)æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„<a href="https://www.cnblogs.com/samshare/p/11801806.html" target="_blank" rel="noopener noreferrer">å‰é¦ˆ</a>å·ç§¯ç¥ç»ç½‘ç»œæ³¨æ„åŠ›æ¨¡å—ã€‚ </li><li>è¯¥æ¨¡å—ä¸ºæ··åˆåŸŸæ³¨æ„åŠ›æœºåˆ¶ï¼ˆï¼‰ä»é€šé“å’Œç©ºé—´ä¸¤ä¸ªæ–¹é¢ä¾æ¬¡æ¨æ–­attention mapã€‚</li><li>CBAMæ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€šç”¨æ¨¡å—ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•CNNä¸­ã€‚</li></ul><p><strong>å…³é”®è¯:ç‰©ä½“è¯†åˆ«ï¼Œæ³¨æ„æœºåˆ¶ï¼Œé—¨æ§å·ç§¯</strong></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä»‹ç»">ä»‹ç»<a aria-hidden="true" class="hash-link" href="#ä»‹ç»" title="Direct link to heading">â€‹</a></h2><ul><li>å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)åŸºäºå…¶ä¸°å¯Œçš„è¡¨è¾¾èƒ½åŠ›æ˜¾è‘—æé«˜äº†è§†è§‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œç›®å‰çš„ä¸»è¦å…³æ³¨ç½‘ç»œçš„ä¸‰ä¸ªé‡è¦å› ç´ ï¼š<strong>æ·±åº¦ï¼Œå®½åº¦å’ŒåŸºæ•°</strong>ï¼ˆCardinalityï¼‰ã€‚</li><li>ä»LeNetåˆ°æ®‹å·®ç½‘ç»œï¼Œç½‘ç»œå˜çš„æ›´åŠ æ·±å…¥ï¼Œè¡¨è¾¾å½¢å¼æ›´åŠ ä¸°å¯Œï¼›GoogLeNetè¡¨æ˜å®½åº¦æ˜¯æé«˜æ¨¡å‹æ€§èƒ½çš„å¦ä¸€ä¸ªé‡è¦å› ç´ ï¼›Xceptionå’ŒResNextåˆ™é€šè¿‡å¢åŠ ç½‘ç»œçš„<strong>åŸºæ•°</strong>ï¼Œåœ¨èŠ‚çœå‚æ•°çš„åŒæ—¶ï¼Œæ¥è·å¾—æ¯”æ·±åº¦ã€å®½åº¦æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼ˆå¼•ç”¨äºResNextè®ºæ–‡ï¼‰ã€‚</li><li>é™¤äº†è¿™äº›å› ç´ ä¹‹å¤–ï¼Œæœ¬æ–‡è€ƒå¯Ÿäº†ä¸ç½‘ç»œç»“æ„è®¾è®¡ä¸åŒçš„æ–¹é¢â€”â€”æ³¨æ„åŠ›ã€‚</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attention-mechanism">attention-mechanism</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Convolutional Block Attention Module" href="/blog/[16]Convolutional-Block-Attention-Module"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/2103.16562" target="_blank" rel="noopener noreferrer">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></p><p>ä½œè€…ï¼šBowen Chengï¼ŒRoss Girshickï¼ŒPiotr DollÃ¡rï¼ŒAlexander C. Bergï¼ŒAlexander Kirillov</p><p>Codeï¼š<a href="https://github.com/bowenc0221/boundary-iou-api" target="_blank" rel="noopener noreferrer">https://github.com/bowenc0221/boundary-iou-api</a></p></blockquote><p>å†™åœ¨å‰é¢ï¼š</p><p>â€‹	<strong>æ­£å¦‚å®ƒçš„åå­—ï¼ŒBoundary IoUå°±æ˜¯è¾¹ç•Œè½®å»“ä¹‹é—´çš„IoUã€‚</strong></p><p>â€‹	é‡ç‚¹ä¸º3.4èŠ‚ã€5.1èŠ‚ï¼Œå…¶ä»–åŸºæœ¬éƒ½æ˜¯å¯¹æ¯”å®éªŒã€‚</p><header><h1>æ‘˜è¦</h1></header><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè¾¹ç•Œè´¨é‡çš„åˆ†å‰²è¯„ä»·æ–¹æ³•â€”â€”Boundary IoUï¼›</li><li>Boundary IoUå¯¹å¤§å¯¹è±¡çš„è¾¹ç•Œè¯¯å·®æ¯”æ ‡å‡†æ©ç IoUæµ‹é‡æ˜æ˜¾æ›´æ•æ„Ÿï¼Œå¹¶ä¸”ä¸ä¼šè¿‡åˆ†æƒ©ç½šè¾ƒå°å¯¹è±¡çš„è¯¯å·®ï¼›</li><li>æ¯”å…¶ä»–æ–¹æ³•æ›´é€‚åˆä½œä¸ºè¯„ä»·åˆ†å‰²çš„æŒ‡æ ‡ã€‚</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/loss-function">loss-function</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/iou">iou</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Boundary IoU - Improving Object-Centric Image Segmentation Evaluation" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/AsTheStarsFalll.png" alt="AsTheStarsFall"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AsTheStarsFalll" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">AsTheStarsFall</span></a></div><small class="avatar__subtitle" itemprop="description">None</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>è®ºæ–‡åç§°ï¼š<a href="https://arxiv.org/abs/2103.06255" target="_blank" rel="noopener noreferrer"><em>Involution: Inverting the Inherence of Convolution for Visual Recognition</em></a></p><p>ä½œè€…ï¼šDuo Liï¼Œ Jie Huï¼Œ Changhu Wangï¼Œ Xiangtai Liï¼Œ Qi Sheï¼Œ Lei Zhuï¼Œ Tong Zhangï¼Œ Qifeng Chenï¼Œ The Hong Kong University of Science and Technologyï¼Œ ByteDance AI Labï¼Œ Peking Universityï¼Œ Beijing University of Posts and Telecommunications</p></blockquote><header><h1>Convolution</h1></header><ol><li><a href="https://arxiv.org/abs/1805.12177" target="_blank" rel="noopener noreferrer">ç©ºé—´æ— å…³æ€§(spatial agnostic)</a>ï¼šsame kernel for different position<ul><li>ä¼˜ç‚¹ï¼šå‚æ•°å…±äº«ï¼Œå¹³ç§»ç­‰å˜</li><li>ç¼ºç‚¹ï¼šä¸èƒ½çµæ´»æ”¹å˜å‚æ•°ï¼Œå·ç§¯æ ¸å°ºå¯¸ä¸èƒ½è¿‡å¤§ï¼Œåªèƒ½é€šè¿‡å †å æ¥æ‰©å¤§æ„Ÿå—é‡ã€æ•æ‰é•¿è·ç¦»å…³ç³»</li></ul></li><li>é€šé“ç‰¹å¼‚æ€§(channel specific)ï¼šdifferent kernels for different channels<ul><li>ä¼˜ç‚¹ï¼šå……åˆ†æå–ä¸åŒé€šé“ä¸Šçš„ä¿¡æ¯</li><li>ç¼ºç‚¹ï¼šæœ‰å†—ä½™</li></ul></li></ol><p>Convolution kernel å°ºå¯¸ä¸º B,C_out,C_in,K,K</p><header><h1>Involution</h1></header><p>ä¸convolutionä¸åŒï¼Œinvolutionæ‹¥æœ‰<strong>å®Œå…¨ç›¸å</strong>çš„æ€§è´¨ï¼š</p><ol><li>ç©ºé—´ç‰¹å¼‚æ€§ï¼škernel privatized for different position</li><li>é€šé“ä¸å˜æ€§ï¼škernel shared across different channels</li></ol><p>involution kernel çš„å°ºå¯¸ä¸ºB,G,KK,H,W.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/attention-mechanism">attention-mechanism</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/non-convolution">non-convolution</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Involution - Inverting the Inherence of Convolution for Visual Recognition" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2022-11-05T07:47:19.660Z" itemprop="datePublished">2022å¹´11æœˆ5æ—¥</time> Â· <!-- -->One min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | é”®åœˆèººå°¸ç –å®¶</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="image-20210601121147760" src="/assets/images/image-20210601121147760-2609d0f40692dce369b8041f749ff63b.png"></p><blockquote><p>â€œæˆ‘ä»¬å¸Œæœ›é¢„æµ‹åˆ†å‰²å›¾çš„è¾¹ç•ŒåŒºåŸŸæ›´åŠ å‡†ç¡®ï¼Œæˆ‘ä»¬å°±ä¸åº”è¯¥ä½¿ç”¨å‡åŒ€é‡‡æ ·ï¼Œè€Œåº”è¯¥æ›´åŠ å€¾å‘äºå›¾åƒè¾¹ç•ŒåŒºåŸŸã€‚â€</p></blockquote><p>è¿™æ˜¯ä¸€ç¯‡ç”¨äºæ”¹å–„å›¾åƒåˆ†å‰²é—®é¢˜ä¸­è¾¹ç¼˜åˆ†å‰²æ•ˆæœçš„æ–¹æ³•çš„è®ºæ–‡çš„é˜…è¯»ç¬”è®°ã€‚è¯¥æ–¹æ³•â€œå°†åˆ†å‰²é—®é¢˜çœ‹ä½œæ¸²æŸ“é—®é¢˜â€ï¼Œè¾¾åˆ°äº†è¾ƒå¥½çš„æ•ˆæœã€‚è®ºæ–‡åŸæ–‡ï¼š<a href="https://arxiv.org/abs/1912.08193" target="_blank" rel="noopener noreferrer">PointRend: Image Segmentation as Rendering</a>ã€‚åœ¨é˜…è¯»è¿™ç¯‡ç¬”è®°ä¹‹å‰ï¼Œè¯·ç¡®ä¿å…ˆäº†è§£å›¾åƒåˆ†å‰²æŠ€æœ¯ã€‚å¯¹åˆ†å‰²çš„æŠ€æœ¯è¿›è¡Œç®€è¦çš„äº†è§£ï¼Œå¯ä»¥å‚è€ƒ<a href="/blog/page/[10]Overview-Of-Semantic-Segmentation">å¦ä¸€ç¯‡ç¬”è®°</a>ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstractæ‘˜è¦">Abstractï¼ˆæ‘˜è¦ï¼‰<a aria-hidden="true" class="hash-link" href="#abstractæ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend&#x27;s efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend" target="_blank" rel="noopener noreferrer">this https URL</a>.</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/blog/tags/refinement">refinement</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about PointRend - Image Segmentation as Rendering" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/page/2"><div class="pagination-nav__label">Â«<!-- --> <!-- -->Newer Entries</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/page/4"><div class="pagination-nav__label">Older Entries<!-- --> <!-- -->Â»</div></a></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.70701027.js"></script>
<script src="/assets/js/main.e4cdc564.js"></script>
</body>
</html>